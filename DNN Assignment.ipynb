{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3137787e-5499-4fb6-8395-2f128a8a53e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\sajal\\anaconda3\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\sajal\\anaconda3\\lib\\site-packages (from ucimlrepo) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\sajal\\anaconda3\\lib\\site-packages (from ucimlrepo) (2025.4.26)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sajal\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sajal\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sajal\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sajal\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sajal\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f1e09f5d-bbe2-4fd2-b0ba-eaa117442042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2225b4a-9858-48b7-bfb7-287aeb94612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 17, 'name': 'Breast Cancer Wisconsin (Diagnostic)', 'repository_url': 'https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic', 'data_url': 'https://archive.ics.uci.edu/static/public/17/data.csv', 'abstract': 'Diagnostic Wisconsin Breast Cancer Database.', 'area': 'Health and Medicine', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 569, 'num_features': 30, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['Diagnosis'], 'index_col': ['ID'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1993, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C5DW2B', 'creators': ['William Wolberg', 'Olvi Mangasarian', 'Nick Street', 'W. Street'], 'intro_paper': {'ID': 230, 'type': 'NATIVE', 'title': 'Nuclear feature extraction for breast tumor diagnosis', 'authors': 'W. Street, W. Wolberg, O. Mangasarian', 'venue': 'Electronic imaging', 'year': 1993, 'journal': None, 'DOI': '10.1117/12.148698', 'URL': 'https://www.semanticscholar.org/paper/53f0fbb425bc14468eb3bf96b2e1d41ba8087f36', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image. A few of the images can be found at http://www.cs.wisc.edu/~street/images/\\r\\n\\r\\nSeparating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree.  Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.\\r\\n\\r\\nThe actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\\r\\n\\r\\nThis database is also available through the UW CS ftp server:\\r\\nftp ftp.cs.wisc.edu\\r\\ncd math-prog/cpo-dataset/machine-learn/WDBC/', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '1) ID number\\r\\n2) Diagnosis (M = malignant, B = benign)\\r\\n3-32)\\r\\n\\r\\nTen real-valued features are computed for each cell nucleus:\\r\\n\\r\\n\\ta) radius (mean of distances from center to points on the perimeter)\\r\\n\\tb) texture (standard deviation of gray-scale values)\\r\\n\\tc) perimeter\\r\\n\\td) area\\r\\n\\te) smoothness (local variation in radius lengths)\\r\\n\\tf) compactness (perimeter^2 / area - 1.0)\\r\\n\\tg) concavity (severity of concave portions of the contour)\\r\\n\\th) concave points (number of concave portions of the contour)\\r\\n\\ti) symmetry \\r\\n\\tj) fractal dimension (\"coastline approximation\" - 1)', 'citation': None}}\n",
      "                  name     role         type demographic description units  \\\n",
      "0                   ID       ID  Categorical        None        None  None   \n",
      "1            Diagnosis   Target  Categorical        None        None  None   \n",
      "2              radius1  Feature   Continuous        None        None  None   \n",
      "3             texture1  Feature   Continuous        None        None  None   \n",
      "4           perimeter1  Feature   Continuous        None        None  None   \n",
      "5                area1  Feature   Continuous        None        None  None   \n",
      "6          smoothness1  Feature   Continuous        None        None  None   \n",
      "7         compactness1  Feature   Continuous        None        None  None   \n",
      "8           concavity1  Feature   Continuous        None        None  None   \n",
      "9      concave_points1  Feature   Continuous        None        None  None   \n",
      "10           symmetry1  Feature   Continuous        None        None  None   \n",
      "11  fractal_dimension1  Feature   Continuous        None        None  None   \n",
      "12             radius2  Feature   Continuous        None        None  None   \n",
      "13            texture2  Feature   Continuous        None        None  None   \n",
      "14          perimeter2  Feature   Continuous        None        None  None   \n",
      "15               area2  Feature   Continuous        None        None  None   \n",
      "16         smoothness2  Feature   Continuous        None        None  None   \n",
      "17        compactness2  Feature   Continuous        None        None  None   \n",
      "18          concavity2  Feature   Continuous        None        None  None   \n",
      "19     concave_points2  Feature   Continuous        None        None  None   \n",
      "20           symmetry2  Feature   Continuous        None        None  None   \n",
      "21  fractal_dimension2  Feature   Continuous        None        None  None   \n",
      "22             radius3  Feature   Continuous        None        None  None   \n",
      "23            texture3  Feature   Continuous        None        None  None   \n",
      "24          perimeter3  Feature   Continuous        None        None  None   \n",
      "25               area3  Feature   Continuous        None        None  None   \n",
      "26         smoothness3  Feature   Continuous        None        None  None   \n",
      "27        compactness3  Feature   Continuous        None        None  None   \n",
      "28          concavity3  Feature   Continuous        None        None  None   \n",
      "29     concave_points3  Feature   Continuous        None        None  None   \n",
      "30           symmetry3  Feature   Continuous        None        None  None   \n",
      "31  fractal_dimension3  Feature   Continuous        None        None  None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n",
      "13             no  \n",
      "14             no  \n",
      "15             no  \n",
      "16             no  \n",
      "17             no  \n",
      "18             no  \n",
      "19             no  \n",
      "20             no  \n",
      "21             no  \n",
      "22             no  \n",
      "23             no  \n",
      "24             no  \n",
      "25             no  \n",
      "26             no  \n",
      "27             no  \n",
      "28             no  \n",
      "29             no  \n",
      "30             no  \n",
      "31             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(breast_cancer_wisconsin_diagnostic.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(breast_cancer_wisconsin_diagnostic.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f00773b-6de5-4eb1-8dc5-4fb58de36831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63f8ee9-9b8e-4ce8-a9a0-2aa55f38a47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>fractal_dimension1</th>\n",
       "      <th>...</th>\n",
       "      <th>radius3</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>15.47</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>17.06</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>15.49</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>15.09</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius1  texture1  perimeter1   area1  smoothness1  compactness1  \\\n",
       "0    17.99     10.38      122.80  1001.0      0.11840       0.27760   \n",
       "1    20.57     17.77      132.90  1326.0      0.08474       0.07864   \n",
       "2    19.69     21.25      130.00  1203.0      0.10960       0.15990   \n",
       "3    11.42     20.38       77.58   386.1      0.14250       0.28390   \n",
       "4    20.29     14.34      135.10  1297.0      0.10030       0.13280   \n",
       "5    12.45     15.70       82.57   477.1      0.12780       0.17000   \n",
       "6    18.25     19.98      119.60  1040.0      0.09463       0.10900   \n",
       "7    13.71     20.83       90.20   577.9      0.11890       0.16450   \n",
       "8    13.00     21.82       87.50   519.8      0.12730       0.19320   \n",
       "9    12.46     24.04       83.97   475.9      0.11860       0.23960   \n",
       "\n",
       "   concavity1  concave_points1  symmetry1  fractal_dimension1  ...  radius3  \\\n",
       "0     0.30010          0.14710     0.2419             0.07871  ...    25.38   \n",
       "1     0.08690          0.07017     0.1812             0.05667  ...    24.99   \n",
       "2     0.19740          0.12790     0.2069             0.05999  ...    23.57   \n",
       "3     0.24140          0.10520     0.2597             0.09744  ...    14.91   \n",
       "4     0.19800          0.10430     0.1809             0.05883  ...    22.54   \n",
       "5     0.15780          0.08089     0.2087             0.07613  ...    15.47   \n",
       "6     0.11270          0.07400     0.1794             0.05742  ...    22.88   \n",
       "7     0.09366          0.05985     0.2196             0.07451  ...    17.06   \n",
       "8     0.18590          0.09353     0.2350             0.07389  ...    15.49   \n",
       "9     0.22730          0.08543     0.2030             0.08243  ...    15.09   \n",
       "\n",
       "   texture3  perimeter3   area3  smoothness3  compactness3  concavity3  \\\n",
       "0     17.33      184.60  2019.0       0.1622        0.6656      0.7119   \n",
       "1     23.41      158.80  1956.0       0.1238        0.1866      0.2416   \n",
       "2     25.53      152.50  1709.0       0.1444        0.4245      0.4504   \n",
       "3     26.50       98.87   567.7       0.2098        0.8663      0.6869   \n",
       "4     16.67      152.20  1575.0       0.1374        0.2050      0.4000   \n",
       "5     23.75      103.40   741.6       0.1791        0.5249      0.5355   \n",
       "6     27.66      153.20  1606.0       0.1442        0.2576      0.3784   \n",
       "7     28.14      110.60   897.0       0.1654        0.3682      0.2678   \n",
       "8     30.73      106.20   739.3       0.1703        0.5401      0.5390   \n",
       "9     40.68       97.65   711.4       0.1853        1.0580      1.1050   \n",
       "\n",
       "   concave_points3  symmetry3  fractal_dimension3  \n",
       "0           0.2654     0.4601             0.11890  \n",
       "1           0.1860     0.2750             0.08902  \n",
       "2           0.2430     0.3613             0.08758  \n",
       "3           0.2575     0.6638             0.17300  \n",
       "4           0.1625     0.2364             0.07678  \n",
       "5           0.1741     0.3985             0.12440  \n",
       "6           0.1932     0.3063             0.08368  \n",
       "7           0.1556     0.3196             0.11510  \n",
       "8           0.2060     0.4378             0.10720  \n",
       "9           0.2210     0.4366             0.20750  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd51100-91fe-4188-8e8c-b0a0f89ecbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Diagnosis\n",
       "0         M\n",
       "1         M\n",
       "2         M\n",
       "3         M\n",
       "4         M\n",
       "5         M\n",
       "6         M\n",
       "7         M\n",
       "8         M\n",
       "9         M"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be40abd0-a950-40fa-a645-576315ff42d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave_points1</th>\n",
       "      <th>symmetry1</th>\n",
       "      <th>fractal_dimension1</th>\n",
       "      <th>...</th>\n",
       "      <th>radius3</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave_points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal_dimension3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          radius1    texture1  perimeter1        area1  smoothness1  \\\n",
       "count  569.000000  569.000000  569.000000   569.000000   569.000000   \n",
       "mean    14.127292   19.289649   91.969033   654.889104     0.096360   \n",
       "std      3.524049    4.301036   24.298981   351.914129     0.014064   \n",
       "min      6.981000    9.710000   43.790000   143.500000     0.052630   \n",
       "25%     11.700000   16.170000   75.170000   420.300000     0.086370   \n",
       "50%     13.370000   18.840000   86.240000   551.100000     0.095870   \n",
       "75%     15.780000   21.800000  104.100000   782.700000     0.105300   \n",
       "max     28.110000   39.280000  188.500000  2501.000000     0.163400   \n",
       "\n",
       "       compactness1  concavity1  concave_points1   symmetry1  \\\n",
       "count    569.000000  569.000000       569.000000  569.000000   \n",
       "mean       0.104341    0.088799         0.048919    0.181162   \n",
       "std        0.052813    0.079720         0.038803    0.027414   \n",
       "min        0.019380    0.000000         0.000000    0.106000   \n",
       "25%        0.064920    0.029560         0.020310    0.161900   \n",
       "50%        0.092630    0.061540         0.033500    0.179200   \n",
       "75%        0.130400    0.130700         0.074000    0.195700   \n",
       "max        0.345400    0.426800         0.201200    0.304000   \n",
       "\n",
       "       fractal_dimension1  ...     radius3    texture3  perimeter3  \\\n",
       "count          569.000000  ...  569.000000  569.000000  569.000000   \n",
       "mean             0.062798  ...   16.269190   25.677223  107.261213   \n",
       "std              0.007060  ...    4.833242    6.146258   33.602542   \n",
       "min              0.049960  ...    7.930000   12.020000   50.410000   \n",
       "25%              0.057700  ...   13.010000   21.080000   84.110000   \n",
       "50%              0.061540  ...   14.970000   25.410000   97.660000   \n",
       "75%              0.066120  ...   18.790000   29.720000  125.400000   \n",
       "max              0.097440  ...   36.040000   49.540000  251.200000   \n",
       "\n",
       "             area3  smoothness3  compactness3  concavity3  concave_points3  \\\n",
       "count   569.000000   569.000000    569.000000  569.000000       569.000000   \n",
       "mean    880.583128     0.132369      0.254265    0.272188         0.114606   \n",
       "std     569.356993     0.022832      0.157336    0.208624         0.065732   \n",
       "min     185.200000     0.071170      0.027290    0.000000         0.000000   \n",
       "25%     515.300000     0.116600      0.147200    0.114500         0.064930   \n",
       "50%     686.500000     0.131300      0.211900    0.226700         0.099930   \n",
       "75%    1084.000000     0.146000      0.339100    0.382900         0.161400   \n",
       "max    4254.000000     0.222600      1.058000    1.252000         0.291000   \n",
       "\n",
       "        symmetry3  fractal_dimension3  \n",
       "count  569.000000          569.000000  \n",
       "mean     0.290076            0.083946  \n",
       "std      0.061867            0.018061  \n",
       "min      0.156500            0.055040  \n",
       "25%      0.250400            0.071460  \n",
       "50%      0.282200            0.080040  \n",
       "75%      0.317900            0.092080  \n",
       "max      0.663800            0.207500  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a1c9ff1-ab8d-4c94-89e2-797d3e835e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in feature:: radius1               0\n",
      "texture1              0\n",
      "perimeter1            0\n",
      "area1                 0\n",
      "smoothness1           0\n",
      "compactness1          0\n",
      "concavity1            0\n",
      "concave_points1       0\n",
      "symmetry1             0\n",
      "fractal_dimension1    0\n",
      "radius2               0\n",
      "texture2              0\n",
      "perimeter2            0\n",
      "area2                 0\n",
      "smoothness2           0\n",
      "compactness2          0\n",
      "concavity2            0\n",
      "concave_points2       0\n",
      "symmetry2             0\n",
      "fractal_dimension2    0\n",
      "radius3               0\n",
      "texture3              0\n",
      "perimeter3            0\n",
      "area3                 0\n",
      "smoothness3           0\n",
      "compactness3          0\n",
      "concavity3            0\n",
      "concave_points3       0\n",
      "symmetry3             0\n",
      "fractal_dimension3    0\n",
      "dtype: int64\n",
      "Null values in target:: Diagnosis    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Null values in feature::',X.isnull().sum())\n",
    "print('Null values in target::',y.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592ddbfb-6284-4100-94d5-b0baaf94236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malignant → 0, Benign → 1\n",
    "y = y['Diagnosis'].map({'M': 0, 'B': 1})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e41f595-0f2a-4c39-b052-7fdbb4c51900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80-20, stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54acebb-a653-4c2a-8f8a-09476a8f4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = Standardcaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70a3016b-928c-4c65-843f-509039a558a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.loss_history = []\n",
    "        self.weights = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        # Clip z to avoid overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _compute_loss(self, y_true, y_pred):\n",
    "        # Binary Cross-Entropy Loss (avoid log(0))\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Initialize weights (including bias if not added externally)\n",
    "        self.weights = np.zeros(n_features)\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            # Forward pass\n",
    "            z = np.dot(X, self.weights)\n",
    "            y_pred = self._sigmoid(z)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self._compute_loss(y, y_pred)\n",
    "            print(f\"Iteration {i+1}, Loss: {loss:.6f}\")\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Gradient computation\n",
    "            gradient = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            \n",
    "            # Update weights\n",
    "            self.weights -= self.lr * gradient\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = np.dot(X, self.weights)\n",
    "        return self._sigmoid(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)  # This returns sigmoid outputs\n",
    "        return (proba >= 0.5).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bc18215-28d3-4770-8224-7d17f0d946a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias term (for Logistic Regression baseline)\n",
    "X_train_bias = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test_bias = np.c_[np.ones(X_test.shape[0]), X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d01d8ed-af84-4a8c-8cc9-c6a36fc3f384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss: 0.693147\n",
      "Iteration 2, Loss: 0.520303\n",
      "Iteration 3, Loss: 0.432024\n",
      "Iteration 4, Loss: 0.378063\n",
      "Iteration 5, Loss: 0.340960\n",
      "Iteration 6, Loss: 0.313498\n",
      "Iteration 7, Loss: 0.292132\n",
      "Iteration 8, Loss: 0.274900\n",
      "Iteration 9, Loss: 0.260620\n",
      "Iteration 10, Loss: 0.248533\n",
      "Iteration 11, Loss: 0.238129\n",
      "Iteration 12, Loss: 0.229049\n",
      "Iteration 13, Loss: 0.221034\n",
      "Iteration 14, Loss: 0.213891\n",
      "Iteration 15, Loss: 0.207473\n",
      "Iteration 16, Loss: 0.201665\n",
      "Iteration 17, Loss: 0.196378\n",
      "Iteration 18, Loss: 0.191538\n",
      "Iteration 19, Loss: 0.187088\n",
      "Iteration 20, Loss: 0.182978\n",
      "Iteration 21, Loss: 0.179168\n",
      "Iteration 22, Loss: 0.175624\n",
      "Iteration 23, Loss: 0.172318\n",
      "Iteration 24, Loss: 0.169224\n",
      "Iteration 25, Loss: 0.166322\n",
      "Iteration 26, Loss: 0.163593\n",
      "Iteration 27, Loss: 0.161020\n",
      "Iteration 28, Loss: 0.158591\n",
      "Iteration 29, Loss: 0.156293\n",
      "Iteration 30, Loss: 0.154114\n",
      "Iteration 31, Loss: 0.152045\n",
      "Iteration 32, Loss: 0.150077\n",
      "Iteration 33, Loss: 0.148203\n",
      "Iteration 34, Loss: 0.146416\n",
      "Iteration 35, Loss: 0.144709\n",
      "Iteration 36, Loss: 0.143076\n",
      "Iteration 37, Loss: 0.141514\n",
      "Iteration 38, Loss: 0.140015\n",
      "Iteration 39, Loss: 0.138578\n",
      "Iteration 40, Loss: 0.137197\n",
      "Iteration 41, Loss: 0.135870\n",
      "Iteration 42, Loss: 0.134592\n",
      "Iteration 43, Loss: 0.133362\n",
      "Iteration 44, Loss: 0.132175\n",
      "Iteration 45, Loss: 0.131030\n",
      "Iteration 46, Loss: 0.129925\n",
      "Iteration 47, Loss: 0.128857\n",
      "Iteration 48, Loss: 0.127824\n",
      "Iteration 49, Loss: 0.126824\n",
      "Iteration 50, Loss: 0.125856\n",
      "Iteration 51, Loss: 0.124918\n",
      "Iteration 52, Loss: 0.124009\n",
      "Iteration 53, Loss: 0.123126\n",
      "Iteration 54, Loss: 0.122270\n",
      "Iteration 55, Loss: 0.121438\n",
      "Iteration 56, Loss: 0.120629\n",
      "Iteration 57, Loss: 0.119843\n",
      "Iteration 58, Loss: 0.119079\n",
      "Iteration 59, Loss: 0.118335\n",
      "Iteration 60, Loss: 0.117610\n",
      "Iteration 61, Loss: 0.116904\n",
      "Iteration 62, Loss: 0.116216\n",
      "Iteration 63, Loss: 0.115546\n",
      "Iteration 64, Loss: 0.114892\n",
      "Iteration 65, Loss: 0.114254\n",
      "Iteration 66, Loss: 0.113631\n",
      "Iteration 67, Loss: 0.113022\n",
      "Iteration 68, Loss: 0.112428\n",
      "Iteration 69, Loss: 0.111848\n",
      "Iteration 70, Loss: 0.111280\n",
      "Iteration 71, Loss: 0.110725\n",
      "Iteration 72, Loss: 0.110182\n",
      "Iteration 73, Loss: 0.109651\n",
      "Iteration 74, Loss: 0.109132\n",
      "Iteration 75, Loss: 0.108623\n",
      "Iteration 76, Loss: 0.108124\n",
      "Iteration 77, Loss: 0.107636\n",
      "Iteration 78, Loss: 0.107158\n",
      "Iteration 79, Loss: 0.106689\n",
      "Iteration 80, Loss: 0.106230\n",
      "Iteration 81, Loss: 0.105779\n",
      "Iteration 82, Loss: 0.105337\n",
      "Iteration 83, Loss: 0.104903\n",
      "Iteration 84, Loss: 0.104478\n",
      "Iteration 85, Loss: 0.104060\n",
      "Iteration 86, Loss: 0.103650\n",
      "Iteration 87, Loss: 0.103248\n",
      "Iteration 88, Loss: 0.102852\n",
      "Iteration 89, Loss: 0.102464\n",
      "Iteration 90, Loss: 0.102082\n",
      "Iteration 91, Loss: 0.101707\n",
      "Iteration 92, Loss: 0.101338\n",
      "Iteration 93, Loss: 0.100976\n",
      "Iteration 94, Loss: 0.100620\n",
      "Iteration 95, Loss: 0.100269\n",
      "Iteration 96, Loss: 0.099924\n",
      "Iteration 97, Loss: 0.099585\n",
      "Iteration 98, Loss: 0.099251\n",
      "Iteration 99, Loss: 0.098923\n",
      "Iteration 100, Loss: 0.098599\n",
      "Iteration 101, Loss: 0.098281\n",
      "Iteration 102, Loss: 0.097968\n",
      "Iteration 103, Loss: 0.097659\n",
      "Iteration 104, Loss: 0.097355\n",
      "Iteration 105, Loss: 0.097055\n",
      "Iteration 106, Loss: 0.096760\n",
      "Iteration 107, Loss: 0.096470\n",
      "Iteration 108, Loss: 0.096183\n",
      "Iteration 109, Loss: 0.095901\n",
      "Iteration 110, Loss: 0.095622\n",
      "Iteration 111, Loss: 0.095348\n",
      "Iteration 112, Loss: 0.095077\n",
      "Iteration 113, Loss: 0.094810\n",
      "Iteration 114, Loss: 0.094547\n",
      "Iteration 115, Loss: 0.094288\n",
      "Iteration 116, Loss: 0.094032\n",
      "Iteration 117, Loss: 0.093779\n",
      "Iteration 118, Loss: 0.093530\n",
      "Iteration 119, Loss: 0.093284\n",
      "Iteration 120, Loss: 0.093041\n",
      "Iteration 121, Loss: 0.092801\n",
      "Iteration 122, Loss: 0.092564\n",
      "Iteration 123, Loss: 0.092331\n",
      "Iteration 124, Loss: 0.092100\n",
      "Iteration 125, Loss: 0.091872\n",
      "Iteration 126, Loss: 0.091647\n",
      "Iteration 127, Loss: 0.091425\n",
      "Iteration 128, Loss: 0.091206\n",
      "Iteration 129, Loss: 0.090989\n",
      "Iteration 130, Loss: 0.090775\n",
      "Iteration 131, Loss: 0.090563\n",
      "Iteration 132, Loss: 0.090354\n",
      "Iteration 133, Loss: 0.090147\n",
      "Iteration 134, Loss: 0.089943\n",
      "Iteration 135, Loss: 0.089741\n",
      "Iteration 136, Loss: 0.089542\n",
      "Iteration 137, Loss: 0.089344\n",
      "Iteration 138, Loss: 0.089149\n",
      "Iteration 139, Loss: 0.088956\n",
      "Iteration 140, Loss: 0.088766\n",
      "Iteration 141, Loss: 0.088577\n",
      "Iteration 142, Loss: 0.088391\n",
      "Iteration 143, Loss: 0.088206\n",
      "Iteration 144, Loss: 0.088024\n",
      "Iteration 145, Loss: 0.087843\n",
      "Iteration 146, Loss: 0.087665\n",
      "Iteration 147, Loss: 0.087488\n",
      "Iteration 148, Loss: 0.087313\n",
      "Iteration 149, Loss: 0.087140\n",
      "Iteration 150, Loss: 0.086969\n",
      "Iteration 151, Loss: 0.086800\n",
      "Iteration 152, Loss: 0.086632\n",
      "Iteration 153, Loss: 0.086466\n",
      "Iteration 154, Loss: 0.086302\n",
      "Iteration 155, Loss: 0.086139\n",
      "Iteration 156, Loss: 0.085978\n",
      "Iteration 157, Loss: 0.085819\n",
      "Iteration 158, Loss: 0.085661\n",
      "Iteration 159, Loss: 0.085505\n",
      "Iteration 160, Loss: 0.085350\n",
      "Iteration 161, Loss: 0.085197\n",
      "Iteration 162, Loss: 0.085045\n",
      "Iteration 163, Loss: 0.084895\n",
      "Iteration 164, Loss: 0.084746\n",
      "Iteration 165, Loss: 0.084598\n",
      "Iteration 166, Loss: 0.084452\n",
      "Iteration 167, Loss: 0.084308\n",
      "Iteration 168, Loss: 0.084164\n",
      "Iteration 169, Loss: 0.084022\n",
      "Iteration 170, Loss: 0.083882\n",
      "Iteration 171, Loss: 0.083742\n",
      "Iteration 172, Loss: 0.083604\n",
      "Iteration 173, Loss: 0.083467\n",
      "Iteration 174, Loss: 0.083331\n",
      "Iteration 175, Loss: 0.083197\n",
      "Iteration 176, Loss: 0.083064\n",
      "Iteration 177, Loss: 0.082931\n",
      "Iteration 178, Loss: 0.082800\n",
      "Iteration 179, Loss: 0.082671\n",
      "Iteration 180, Loss: 0.082542\n",
      "Iteration 181, Loss: 0.082414\n",
      "Iteration 182, Loss: 0.082288\n",
      "Iteration 183, Loss: 0.082162\n",
      "Iteration 184, Loss: 0.082038\n",
      "Iteration 185, Loss: 0.081915\n",
      "Iteration 186, Loss: 0.081792\n",
      "Iteration 187, Loss: 0.081671\n",
      "Iteration 188, Loss: 0.081551\n",
      "Iteration 189, Loss: 0.081432\n",
      "Iteration 190, Loss: 0.081313\n",
      "Iteration 191, Loss: 0.081196\n",
      "Iteration 192, Loss: 0.081080\n",
      "Iteration 193, Loss: 0.080964\n",
      "Iteration 194, Loss: 0.080850\n",
      "Iteration 195, Loss: 0.080736\n",
      "Iteration 196, Loss: 0.080624\n",
      "Iteration 197, Loss: 0.080512\n",
      "Iteration 198, Loss: 0.080401\n",
      "Iteration 199, Loss: 0.080291\n",
      "Iteration 200, Loss: 0.080182\n",
      "Iteration 201, Loss: 0.080073\n",
      "Iteration 202, Loss: 0.079966\n",
      "Iteration 203, Loss: 0.079859\n",
      "Iteration 204, Loss: 0.079753\n",
      "Iteration 205, Loss: 0.079648\n",
      "Iteration 206, Loss: 0.079544\n",
      "Iteration 207, Loss: 0.079440\n",
      "Iteration 208, Loss: 0.079338\n",
      "Iteration 209, Loss: 0.079236\n",
      "Iteration 210, Loss: 0.079135\n",
      "Iteration 211, Loss: 0.079034\n",
      "Iteration 212, Loss: 0.078935\n",
      "Iteration 213, Loss: 0.078836\n",
      "Iteration 214, Loss: 0.078737\n",
      "Iteration 215, Loss: 0.078640\n",
      "Iteration 216, Loss: 0.078543\n",
      "Iteration 217, Loss: 0.078447\n",
      "Iteration 218, Loss: 0.078351\n",
      "Iteration 219, Loss: 0.078257\n",
      "Iteration 220, Loss: 0.078162\n",
      "Iteration 221, Loss: 0.078069\n",
      "Iteration 222, Loss: 0.077976\n",
      "Iteration 223, Loss: 0.077884\n",
      "Iteration 224, Loss: 0.077793\n",
      "Iteration 225, Loss: 0.077702\n",
      "Iteration 226, Loss: 0.077611\n",
      "Iteration 227, Loss: 0.077522\n",
      "Iteration 228, Loss: 0.077433\n",
      "Iteration 229, Loss: 0.077344\n",
      "Iteration 230, Loss: 0.077256\n",
      "Iteration 231, Loss: 0.077169\n",
      "Iteration 232, Loss: 0.077083\n",
      "Iteration 233, Loss: 0.076997\n",
      "Iteration 234, Loss: 0.076911\n",
      "Iteration 235, Loss: 0.076826\n",
      "Iteration 236, Loss: 0.076742\n",
      "Iteration 237, Loss: 0.076658\n",
      "Iteration 238, Loss: 0.076575\n",
      "Iteration 239, Loss: 0.076492\n",
      "Iteration 240, Loss: 0.076410\n",
      "Iteration 241, Loss: 0.076328\n",
      "Iteration 242, Loss: 0.076247\n",
      "Iteration 243, Loss: 0.076166\n",
      "Iteration 244, Loss: 0.076086\n",
      "Iteration 245, Loss: 0.076006\n",
      "Iteration 246, Loss: 0.075927\n",
      "Iteration 247, Loss: 0.075849\n",
      "Iteration 248, Loss: 0.075771\n",
      "Iteration 249, Loss: 0.075693\n",
      "Iteration 250, Loss: 0.075616\n",
      "Iteration 251, Loss: 0.075539\n",
      "Iteration 252, Loss: 0.075463\n",
      "Iteration 253, Loss: 0.075387\n",
      "Iteration 254, Loss: 0.075312\n",
      "Iteration 255, Loss: 0.075237\n",
      "Iteration 256, Loss: 0.075163\n",
      "Iteration 257, Loss: 0.075089\n",
      "Iteration 258, Loss: 0.075015\n",
      "Iteration 259, Loss: 0.074942\n",
      "Iteration 260, Loss: 0.074870\n",
      "Iteration 261, Loss: 0.074797\n",
      "Iteration 262, Loss: 0.074726\n",
      "Iteration 263, Loss: 0.074654\n",
      "Iteration 264, Loss: 0.074583\n",
      "Iteration 265, Loss: 0.074513\n",
      "Iteration 266, Loss: 0.074443\n",
      "Iteration 267, Loss: 0.074373\n",
      "Iteration 268, Loss: 0.074304\n",
      "Iteration 269, Loss: 0.074235\n",
      "Iteration 270, Loss: 0.074167\n",
      "Iteration 271, Loss: 0.074099\n",
      "Iteration 272, Loss: 0.074031\n",
      "Iteration 273, Loss: 0.073963\n",
      "Iteration 274, Loss: 0.073897\n",
      "Iteration 275, Loss: 0.073830\n",
      "Iteration 276, Loss: 0.073764\n",
      "Iteration 277, Loss: 0.073698\n",
      "Iteration 278, Loss: 0.073632\n",
      "Iteration 279, Loss: 0.073567\n",
      "Iteration 280, Loss: 0.073503\n",
      "Iteration 281, Loss: 0.073438\n",
      "Iteration 282, Loss: 0.073374\n",
      "Iteration 283, Loss: 0.073311\n",
      "Iteration 284, Loss: 0.073247\n",
      "Iteration 285, Loss: 0.073184\n",
      "Iteration 286, Loss: 0.073122\n",
      "Iteration 287, Loss: 0.073059\n",
      "Iteration 288, Loss: 0.072997\n",
      "Iteration 289, Loss: 0.072936\n",
      "Iteration 290, Loss: 0.072874\n",
      "Iteration 291, Loss: 0.072813\n",
      "Iteration 292, Loss: 0.072753\n",
      "Iteration 293, Loss: 0.072692\n",
      "Iteration 294, Loss: 0.072632\n",
      "Iteration 295, Loss: 0.072573\n",
      "Iteration 296, Loss: 0.072513\n",
      "Iteration 297, Loss: 0.072454\n",
      "Iteration 298, Loss: 0.072395\n",
      "Iteration 299, Loss: 0.072337\n",
      "Iteration 300, Loss: 0.072278\n",
      "Iteration 301, Loss: 0.072221\n",
      "Iteration 302, Loss: 0.072163\n",
      "Iteration 303, Loss: 0.072106\n",
      "Iteration 304, Loss: 0.072049\n",
      "Iteration 305, Loss: 0.071992\n",
      "Iteration 306, Loss: 0.071935\n",
      "Iteration 307, Loss: 0.071879\n",
      "Iteration 308, Loss: 0.071823\n",
      "Iteration 309, Loss: 0.071768\n",
      "Iteration 310, Loss: 0.071712\n",
      "Iteration 311, Loss: 0.071657\n",
      "Iteration 312, Loss: 0.071603\n",
      "Iteration 313, Loss: 0.071548\n",
      "Iteration 314, Loss: 0.071494\n",
      "Iteration 315, Loss: 0.071440\n",
      "Iteration 316, Loss: 0.071386\n",
      "Iteration 317, Loss: 0.071333\n",
      "Iteration 318, Loss: 0.071279\n",
      "Iteration 319, Loss: 0.071226\n",
      "Iteration 320, Loss: 0.071174\n",
      "Iteration 321, Loss: 0.071121\n",
      "Iteration 322, Loss: 0.071069\n",
      "Iteration 323, Loss: 0.071017\n",
      "Iteration 324, Loss: 0.070965\n",
      "Iteration 325, Loss: 0.070914\n",
      "Iteration 326, Loss: 0.070863\n",
      "Iteration 327, Loss: 0.070812\n",
      "Iteration 328, Loss: 0.070761\n",
      "Iteration 329, Loss: 0.070711\n",
      "Iteration 330, Loss: 0.070660\n",
      "Iteration 331, Loss: 0.070610\n",
      "Iteration 332, Loss: 0.070561\n",
      "Iteration 333, Loss: 0.070511\n",
      "Iteration 334, Loss: 0.070462\n",
      "Iteration 335, Loss: 0.070412\n",
      "Iteration 336, Loss: 0.070364\n",
      "Iteration 337, Loss: 0.070315\n",
      "Iteration 338, Loss: 0.070266\n",
      "Iteration 339, Loss: 0.070218\n",
      "Iteration 340, Loss: 0.070170\n",
      "Iteration 341, Loss: 0.070122\n",
      "Iteration 342, Loss: 0.070075\n",
      "Iteration 343, Loss: 0.070027\n",
      "Iteration 344, Loss: 0.069980\n",
      "Iteration 345, Loss: 0.069933\n",
      "Iteration 346, Loss: 0.069887\n",
      "Iteration 347, Loss: 0.069840\n",
      "Iteration 348, Loss: 0.069794\n",
      "Iteration 349, Loss: 0.069748\n",
      "Iteration 350, Loss: 0.069702\n",
      "Iteration 351, Loss: 0.069656\n",
      "Iteration 352, Loss: 0.069610\n",
      "Iteration 353, Loss: 0.069565\n",
      "Iteration 354, Loss: 0.069520\n",
      "Iteration 355, Loss: 0.069475\n",
      "Iteration 356, Loss: 0.069430\n",
      "Iteration 357, Loss: 0.069386\n",
      "Iteration 358, Loss: 0.069341\n",
      "Iteration 359, Loss: 0.069297\n",
      "Iteration 360, Loss: 0.069253\n",
      "Iteration 361, Loss: 0.069209\n",
      "Iteration 362, Loss: 0.069166\n",
      "Iteration 363, Loss: 0.069122\n",
      "Iteration 364, Loss: 0.069079\n",
      "Iteration 365, Loss: 0.069036\n",
      "Iteration 366, Loss: 0.068993\n",
      "Iteration 367, Loss: 0.068950\n",
      "Iteration 368, Loss: 0.068908\n",
      "Iteration 369, Loss: 0.068865\n",
      "Iteration 370, Loss: 0.068823\n",
      "Iteration 371, Loss: 0.068781\n",
      "Iteration 372, Loss: 0.068739\n",
      "Iteration 373, Loss: 0.068698\n",
      "Iteration 374, Loss: 0.068656\n",
      "Iteration 375, Loss: 0.068615\n",
      "Iteration 376, Loss: 0.068574\n",
      "Iteration 377, Loss: 0.068533\n",
      "Iteration 378, Loss: 0.068492\n",
      "Iteration 379, Loss: 0.068451\n",
      "Iteration 380, Loss: 0.068411\n",
      "Iteration 381, Loss: 0.068370\n",
      "Iteration 382, Loss: 0.068330\n",
      "Iteration 383, Loss: 0.068290\n",
      "Iteration 384, Loss: 0.068250\n",
      "Iteration 385, Loss: 0.068210\n",
      "Iteration 386, Loss: 0.068171\n",
      "Iteration 387, Loss: 0.068132\n",
      "Iteration 388, Loss: 0.068092\n",
      "Iteration 389, Loss: 0.068053\n",
      "Iteration 390, Loss: 0.068014\n",
      "Iteration 391, Loss: 0.067975\n",
      "Iteration 392, Loss: 0.067937\n",
      "Iteration 393, Loss: 0.067898\n",
      "Iteration 394, Loss: 0.067860\n",
      "Iteration 395, Loss: 0.067822\n",
      "Iteration 396, Loss: 0.067784\n",
      "Iteration 397, Loss: 0.067746\n",
      "Iteration 398, Loss: 0.067708\n",
      "Iteration 399, Loss: 0.067671\n",
      "Iteration 400, Loss: 0.067633\n",
      "Iteration 401, Loss: 0.067596\n",
      "Iteration 402, Loss: 0.067559\n",
      "Iteration 403, Loss: 0.067521\n",
      "Iteration 404, Loss: 0.067485\n",
      "Iteration 405, Loss: 0.067448\n",
      "Iteration 406, Loss: 0.067411\n",
      "Iteration 407, Loss: 0.067375\n",
      "Iteration 408, Loss: 0.067338\n",
      "Iteration 409, Loss: 0.067302\n",
      "Iteration 410, Loss: 0.067266\n",
      "Iteration 411, Loss: 0.067230\n",
      "Iteration 412, Loss: 0.067194\n",
      "Iteration 413, Loss: 0.067159\n",
      "Iteration 414, Loss: 0.067123\n",
      "Iteration 415, Loss: 0.067088\n",
      "Iteration 416, Loss: 0.067052\n",
      "Iteration 417, Loss: 0.067017\n",
      "Iteration 418, Loss: 0.066982\n",
      "Iteration 419, Loss: 0.066947\n",
      "Iteration 420, Loss: 0.066912\n",
      "Iteration 421, Loss: 0.066878\n",
      "Iteration 422, Loss: 0.066843\n",
      "Iteration 423, Loss: 0.066809\n",
      "Iteration 424, Loss: 0.066774\n",
      "Iteration 425, Loss: 0.066740\n",
      "Iteration 426, Loss: 0.066706\n",
      "Iteration 427, Loss: 0.066672\n",
      "Iteration 428, Loss: 0.066638\n",
      "Iteration 429, Loss: 0.066605\n",
      "Iteration 430, Loss: 0.066571\n",
      "Iteration 431, Loss: 0.066538\n",
      "Iteration 432, Loss: 0.066504\n",
      "Iteration 433, Loss: 0.066471\n",
      "Iteration 434, Loss: 0.066438\n",
      "Iteration 435, Loss: 0.066405\n",
      "Iteration 436, Loss: 0.066372\n",
      "Iteration 437, Loss: 0.066339\n",
      "Iteration 438, Loss: 0.066307\n",
      "Iteration 439, Loss: 0.066274\n",
      "Iteration 440, Loss: 0.066242\n",
      "Iteration 441, Loss: 0.066209\n",
      "Iteration 442, Loss: 0.066177\n",
      "Iteration 443, Loss: 0.066145\n",
      "Iteration 444, Loss: 0.066113\n",
      "Iteration 445, Loss: 0.066081\n",
      "Iteration 446, Loss: 0.066049\n",
      "Iteration 447, Loss: 0.066018\n",
      "Iteration 448, Loss: 0.065986\n",
      "Iteration 449, Loss: 0.065955\n",
      "Iteration 450, Loss: 0.065923\n",
      "Iteration 451, Loss: 0.065892\n",
      "Iteration 452, Loss: 0.065861\n",
      "Iteration 453, Loss: 0.065830\n",
      "Iteration 454, Loss: 0.065799\n",
      "Iteration 455, Loss: 0.065768\n",
      "Iteration 456, Loss: 0.065738\n",
      "Iteration 457, Loss: 0.065707\n",
      "Iteration 458, Loss: 0.065676\n",
      "Iteration 459, Loss: 0.065646\n",
      "Iteration 460, Loss: 0.065616\n",
      "Iteration 461, Loss: 0.065585\n",
      "Iteration 462, Loss: 0.065555\n",
      "Iteration 463, Loss: 0.065525\n",
      "Iteration 464, Loss: 0.065495\n",
      "Iteration 465, Loss: 0.065466\n",
      "Iteration 466, Loss: 0.065436\n",
      "Iteration 467, Loss: 0.065406\n",
      "Iteration 468, Loss: 0.065377\n",
      "Iteration 469, Loss: 0.065347\n",
      "Iteration 470, Loss: 0.065318\n",
      "Iteration 471, Loss: 0.065289\n",
      "Iteration 472, Loss: 0.065259\n",
      "Iteration 473, Loss: 0.065230\n",
      "Iteration 474, Loss: 0.065201\n",
      "Iteration 475, Loss: 0.065172\n",
      "Iteration 476, Loss: 0.065144\n",
      "Iteration 477, Loss: 0.065115\n",
      "Iteration 478, Loss: 0.065086\n",
      "Iteration 479, Loss: 0.065058\n",
      "Iteration 480, Loss: 0.065029\n",
      "Iteration 481, Loss: 0.065001\n",
      "Iteration 482, Loss: 0.064973\n",
      "Iteration 483, Loss: 0.064945\n",
      "Iteration 484, Loss: 0.064916\n",
      "Iteration 485, Loss: 0.064888\n",
      "Iteration 486, Loss: 0.064861\n",
      "Iteration 487, Loss: 0.064833\n",
      "Iteration 488, Loss: 0.064805\n",
      "Iteration 489, Loss: 0.064777\n",
      "Iteration 490, Loss: 0.064750\n",
      "Iteration 491, Loss: 0.064722\n",
      "Iteration 492, Loss: 0.064695\n",
      "Iteration 493, Loss: 0.064668\n",
      "Iteration 494, Loss: 0.064640\n",
      "Iteration 495, Loss: 0.064613\n",
      "Iteration 496, Loss: 0.064586\n",
      "Iteration 497, Loss: 0.064559\n",
      "Iteration 498, Loss: 0.064532\n",
      "Iteration 499, Loss: 0.064505\n",
      "Iteration 500, Loss: 0.064479\n",
      "Iteration 501, Loss: 0.064452\n",
      "Iteration 502, Loss: 0.064425\n",
      "Iteration 503, Loss: 0.064399\n",
      "Iteration 504, Loss: 0.064372\n",
      "Iteration 505, Loss: 0.064346\n",
      "Iteration 506, Loss: 0.064320\n",
      "Iteration 507, Loss: 0.064294\n",
      "Iteration 508, Loss: 0.064267\n",
      "Iteration 509, Loss: 0.064241\n",
      "Iteration 510, Loss: 0.064215\n",
      "Iteration 511, Loss: 0.064190\n",
      "Iteration 512, Loss: 0.064164\n",
      "Iteration 513, Loss: 0.064138\n",
      "Iteration 514, Loss: 0.064112\n",
      "Iteration 515, Loss: 0.064087\n",
      "Iteration 516, Loss: 0.064061\n",
      "Iteration 517, Loss: 0.064036\n",
      "Iteration 518, Loss: 0.064010\n",
      "Iteration 519, Loss: 0.063985\n",
      "Iteration 520, Loss: 0.063960\n",
      "Iteration 521, Loss: 0.063935\n",
      "Iteration 522, Loss: 0.063910\n",
      "Iteration 523, Loss: 0.063885\n",
      "Iteration 524, Loss: 0.063860\n",
      "Iteration 525, Loss: 0.063835\n",
      "Iteration 526, Loss: 0.063810\n",
      "Iteration 527, Loss: 0.063785\n",
      "Iteration 528, Loss: 0.063761\n",
      "Iteration 529, Loss: 0.063736\n",
      "Iteration 530, Loss: 0.063712\n",
      "Iteration 531, Loss: 0.063687\n",
      "Iteration 532, Loss: 0.063663\n",
      "Iteration 533, Loss: 0.063639\n",
      "Iteration 534, Loss: 0.063614\n",
      "Iteration 535, Loss: 0.063590\n",
      "Iteration 536, Loss: 0.063566\n",
      "Iteration 537, Loss: 0.063542\n",
      "Iteration 538, Loss: 0.063518\n",
      "Iteration 539, Loss: 0.063494\n",
      "Iteration 540, Loss: 0.063470\n",
      "Iteration 541, Loss: 0.063447\n",
      "Iteration 542, Loss: 0.063423\n",
      "Iteration 543, Loss: 0.063399\n",
      "Iteration 544, Loss: 0.063376\n",
      "Iteration 545, Loss: 0.063352\n",
      "Iteration 546, Loss: 0.063329\n",
      "Iteration 547, Loss: 0.063305\n",
      "Iteration 548, Loss: 0.063282\n",
      "Iteration 549, Loss: 0.063259\n",
      "Iteration 550, Loss: 0.063235\n",
      "Iteration 551, Loss: 0.063212\n",
      "Iteration 552, Loss: 0.063189\n",
      "Iteration 553, Loss: 0.063166\n",
      "Iteration 554, Loss: 0.063143\n",
      "Iteration 555, Loss: 0.063120\n",
      "Iteration 556, Loss: 0.063098\n",
      "Iteration 557, Loss: 0.063075\n",
      "Iteration 558, Loss: 0.063052\n",
      "Iteration 559, Loss: 0.063030\n",
      "Iteration 560, Loss: 0.063007\n",
      "Iteration 561, Loss: 0.062984\n",
      "Iteration 562, Loss: 0.062962\n",
      "Iteration 563, Loss: 0.062940\n",
      "Iteration 564, Loss: 0.062917\n",
      "Iteration 565, Loss: 0.062895\n",
      "Iteration 566, Loss: 0.062873\n",
      "Iteration 567, Loss: 0.062851\n",
      "Iteration 568, Loss: 0.062829\n",
      "Iteration 569, Loss: 0.062806\n",
      "Iteration 570, Loss: 0.062784\n",
      "Iteration 571, Loss: 0.062763\n",
      "Iteration 572, Loss: 0.062741\n",
      "Iteration 573, Loss: 0.062719\n",
      "Iteration 574, Loss: 0.062697\n",
      "Iteration 575, Loss: 0.062675\n",
      "Iteration 576, Loss: 0.062654\n",
      "Iteration 577, Loss: 0.062632\n",
      "Iteration 578, Loss: 0.062611\n",
      "Iteration 579, Loss: 0.062589\n",
      "Iteration 580, Loss: 0.062568\n",
      "Iteration 581, Loss: 0.062546\n",
      "Iteration 582, Loss: 0.062525\n",
      "Iteration 583, Loss: 0.062504\n",
      "Iteration 584, Loss: 0.062483\n",
      "Iteration 585, Loss: 0.062461\n",
      "Iteration 586, Loss: 0.062440\n",
      "Iteration 587, Loss: 0.062419\n",
      "Iteration 588, Loss: 0.062398\n",
      "Iteration 589, Loss: 0.062377\n",
      "Iteration 590, Loss: 0.062356\n",
      "Iteration 591, Loss: 0.062336\n",
      "Iteration 592, Loss: 0.062315\n",
      "Iteration 593, Loss: 0.062294\n",
      "Iteration 594, Loss: 0.062273\n",
      "Iteration 595, Loss: 0.062253\n",
      "Iteration 596, Loss: 0.062232\n",
      "Iteration 597, Loss: 0.062212\n",
      "Iteration 598, Loss: 0.062191\n",
      "Iteration 599, Loss: 0.062171\n",
      "Iteration 600, Loss: 0.062150\n",
      "Iteration 601, Loss: 0.062130\n",
      "Iteration 602, Loss: 0.062110\n",
      "Iteration 603, Loss: 0.062090\n",
      "Iteration 604, Loss: 0.062069\n",
      "Iteration 605, Loss: 0.062049\n",
      "Iteration 606, Loss: 0.062029\n",
      "Iteration 607, Loss: 0.062009\n",
      "Iteration 608, Loss: 0.061989\n",
      "Iteration 609, Loss: 0.061969\n",
      "Iteration 610, Loss: 0.061949\n",
      "Iteration 611, Loss: 0.061930\n",
      "Iteration 612, Loss: 0.061910\n",
      "Iteration 613, Loss: 0.061890\n",
      "Iteration 614, Loss: 0.061870\n",
      "Iteration 615, Loss: 0.061851\n",
      "Iteration 616, Loss: 0.061831\n",
      "Iteration 617, Loss: 0.061812\n",
      "Iteration 618, Loss: 0.061792\n",
      "Iteration 619, Loss: 0.061773\n",
      "Iteration 620, Loss: 0.061753\n",
      "Iteration 621, Loss: 0.061734\n",
      "Iteration 622, Loss: 0.061715\n",
      "Iteration 623, Loss: 0.061695\n",
      "Iteration 624, Loss: 0.061676\n",
      "Iteration 625, Loss: 0.061657\n",
      "Iteration 626, Loss: 0.061638\n",
      "Iteration 627, Loss: 0.061619\n",
      "Iteration 628, Loss: 0.061600\n",
      "Iteration 629, Loss: 0.061581\n",
      "Iteration 630, Loss: 0.061562\n",
      "Iteration 631, Loss: 0.061543\n",
      "Iteration 632, Loss: 0.061524\n",
      "Iteration 633, Loss: 0.061505\n",
      "Iteration 634, Loss: 0.061486\n",
      "Iteration 635, Loss: 0.061468\n",
      "Iteration 636, Loss: 0.061449\n",
      "Iteration 637, Loss: 0.061430\n",
      "Iteration 638, Loss: 0.061412\n",
      "Iteration 639, Loss: 0.061393\n",
      "Iteration 640, Loss: 0.061375\n",
      "Iteration 641, Loss: 0.061356\n",
      "Iteration 642, Loss: 0.061338\n",
      "Iteration 643, Loss: 0.061319\n",
      "Iteration 644, Loss: 0.061301\n",
      "Iteration 645, Loss: 0.061283\n",
      "Iteration 646, Loss: 0.061264\n",
      "Iteration 647, Loss: 0.061246\n",
      "Iteration 648, Loss: 0.061228\n",
      "Iteration 649, Loss: 0.061210\n",
      "Iteration 650, Loss: 0.061192\n",
      "Iteration 651, Loss: 0.061174\n",
      "Iteration 652, Loss: 0.061156\n",
      "Iteration 653, Loss: 0.061138\n",
      "Iteration 654, Loss: 0.061120\n",
      "Iteration 655, Loss: 0.061102\n",
      "Iteration 656, Loss: 0.061084\n",
      "Iteration 657, Loss: 0.061066\n",
      "Iteration 658, Loss: 0.061049\n",
      "Iteration 659, Loss: 0.061031\n",
      "Iteration 660, Loss: 0.061013\n",
      "Iteration 661, Loss: 0.060996\n",
      "Iteration 662, Loss: 0.060978\n",
      "Iteration 663, Loss: 0.060960\n",
      "Iteration 664, Loss: 0.060943\n",
      "Iteration 665, Loss: 0.060925\n",
      "Iteration 666, Loss: 0.060908\n",
      "Iteration 667, Loss: 0.060890\n",
      "Iteration 668, Loss: 0.060873\n",
      "Iteration 669, Loss: 0.060856\n",
      "Iteration 670, Loss: 0.060838\n",
      "Iteration 671, Loss: 0.060821\n",
      "Iteration 672, Loss: 0.060804\n",
      "Iteration 673, Loss: 0.060787\n",
      "Iteration 674, Loss: 0.060770\n",
      "Iteration 675, Loss: 0.060753\n",
      "Iteration 676, Loss: 0.060735\n",
      "Iteration 677, Loss: 0.060718\n",
      "Iteration 678, Loss: 0.060701\n",
      "Iteration 679, Loss: 0.060684\n",
      "Iteration 680, Loss: 0.060668\n",
      "Iteration 681, Loss: 0.060651\n",
      "Iteration 682, Loss: 0.060634\n",
      "Iteration 683, Loss: 0.060617\n",
      "Iteration 684, Loss: 0.060600\n",
      "Iteration 685, Loss: 0.060583\n",
      "Iteration 686, Loss: 0.060567\n",
      "Iteration 687, Loss: 0.060550\n",
      "Iteration 688, Loss: 0.060533\n",
      "Iteration 689, Loss: 0.060517\n",
      "Iteration 690, Loss: 0.060500\n",
      "Iteration 691, Loss: 0.060484\n",
      "Iteration 692, Loss: 0.060467\n",
      "Iteration 693, Loss: 0.060451\n",
      "Iteration 694, Loss: 0.060434\n",
      "Iteration 695, Loss: 0.060418\n",
      "Iteration 696, Loss: 0.060402\n",
      "Iteration 697, Loss: 0.060385\n",
      "Iteration 698, Loss: 0.060369\n",
      "Iteration 699, Loss: 0.060353\n",
      "Iteration 700, Loss: 0.060337\n",
      "Iteration 701, Loss: 0.060320\n",
      "Iteration 702, Loss: 0.060304\n",
      "Iteration 703, Loss: 0.060288\n",
      "Iteration 704, Loss: 0.060272\n",
      "Iteration 705, Loss: 0.060256\n",
      "Iteration 706, Loss: 0.060240\n",
      "Iteration 707, Loss: 0.060224\n",
      "Iteration 708, Loss: 0.060208\n",
      "Iteration 709, Loss: 0.060192\n",
      "Iteration 710, Loss: 0.060176\n",
      "Iteration 711, Loss: 0.060160\n",
      "Iteration 712, Loss: 0.060145\n",
      "Iteration 713, Loss: 0.060129\n",
      "Iteration 714, Loss: 0.060113\n",
      "Iteration 715, Loss: 0.060097\n",
      "Iteration 716, Loss: 0.060082\n",
      "Iteration 717, Loss: 0.060066\n",
      "Iteration 718, Loss: 0.060050\n",
      "Iteration 719, Loss: 0.060035\n",
      "Iteration 720, Loss: 0.060019\n",
      "Iteration 721, Loss: 0.060004\n",
      "Iteration 722, Loss: 0.059988\n",
      "Iteration 723, Loss: 0.059973\n",
      "Iteration 724, Loss: 0.059957\n",
      "Iteration 725, Loss: 0.059942\n",
      "Iteration 726, Loss: 0.059927\n",
      "Iteration 727, Loss: 0.059911\n",
      "Iteration 728, Loss: 0.059896\n",
      "Iteration 729, Loss: 0.059881\n",
      "Iteration 730, Loss: 0.059865\n",
      "Iteration 731, Loss: 0.059850\n",
      "Iteration 732, Loss: 0.059835\n",
      "Iteration 733, Loss: 0.059820\n",
      "Iteration 734, Loss: 0.059805\n",
      "Iteration 735, Loss: 0.059790\n",
      "Iteration 736, Loss: 0.059775\n",
      "Iteration 737, Loss: 0.059760\n",
      "Iteration 738, Loss: 0.059745\n",
      "Iteration 739, Loss: 0.059730\n",
      "Iteration 740, Loss: 0.059715\n",
      "Iteration 741, Loss: 0.059700\n",
      "Iteration 742, Loss: 0.059685\n",
      "Iteration 743, Loss: 0.059670\n",
      "Iteration 744, Loss: 0.059655\n",
      "Iteration 745, Loss: 0.059640\n",
      "Iteration 746, Loss: 0.059626\n",
      "Iteration 747, Loss: 0.059611\n",
      "Iteration 748, Loss: 0.059596\n",
      "Iteration 749, Loss: 0.059582\n",
      "Iteration 750, Loss: 0.059567\n",
      "Iteration 751, Loss: 0.059552\n",
      "Iteration 752, Loss: 0.059538\n",
      "Iteration 753, Loss: 0.059523\n",
      "Iteration 754, Loss: 0.059509\n",
      "Iteration 755, Loss: 0.059494\n",
      "Iteration 756, Loss: 0.059480\n",
      "Iteration 757, Loss: 0.059465\n",
      "Iteration 758, Loss: 0.059451\n",
      "Iteration 759, Loss: 0.059436\n",
      "Iteration 760, Loss: 0.059422\n",
      "Iteration 761, Loss: 0.059408\n",
      "Iteration 762, Loss: 0.059393\n",
      "Iteration 763, Loss: 0.059379\n",
      "Iteration 764, Loss: 0.059365\n",
      "Iteration 765, Loss: 0.059351\n",
      "Iteration 766, Loss: 0.059336\n",
      "Iteration 767, Loss: 0.059322\n",
      "Iteration 768, Loss: 0.059308\n",
      "Iteration 769, Loss: 0.059294\n",
      "Iteration 770, Loss: 0.059280\n",
      "Iteration 771, Loss: 0.059266\n",
      "Iteration 772, Loss: 0.059252\n",
      "Iteration 773, Loss: 0.059238\n",
      "Iteration 774, Loss: 0.059224\n",
      "Iteration 775, Loss: 0.059210\n",
      "Iteration 776, Loss: 0.059196\n",
      "Iteration 777, Loss: 0.059182\n",
      "Iteration 778, Loss: 0.059168\n",
      "Iteration 779, Loss: 0.059154\n",
      "Iteration 780, Loss: 0.059140\n",
      "Iteration 781, Loss: 0.059127\n",
      "Iteration 782, Loss: 0.059113\n",
      "Iteration 783, Loss: 0.059099\n",
      "Iteration 784, Loss: 0.059085\n",
      "Iteration 785, Loss: 0.059072\n",
      "Iteration 786, Loss: 0.059058\n",
      "Iteration 787, Loss: 0.059044\n",
      "Iteration 788, Loss: 0.059031\n",
      "Iteration 789, Loss: 0.059017\n",
      "Iteration 790, Loss: 0.059004\n",
      "Iteration 791, Loss: 0.058990\n",
      "Iteration 792, Loss: 0.058977\n",
      "Iteration 793, Loss: 0.058963\n",
      "Iteration 794, Loss: 0.058950\n",
      "Iteration 795, Loss: 0.058936\n",
      "Iteration 796, Loss: 0.058923\n",
      "Iteration 797, Loss: 0.058909\n",
      "Iteration 798, Loss: 0.058896\n",
      "Iteration 799, Loss: 0.058883\n",
      "Iteration 800, Loss: 0.058869\n",
      "Iteration 801, Loss: 0.058856\n",
      "Iteration 802, Loss: 0.058843\n",
      "Iteration 803, Loss: 0.058829\n",
      "Iteration 804, Loss: 0.058816\n",
      "Iteration 805, Loss: 0.058803\n",
      "Iteration 806, Loss: 0.058790\n",
      "Iteration 807, Loss: 0.058777\n",
      "Iteration 808, Loss: 0.058764\n",
      "Iteration 809, Loss: 0.058750\n",
      "Iteration 810, Loss: 0.058737\n",
      "Iteration 811, Loss: 0.058724\n",
      "Iteration 812, Loss: 0.058711\n",
      "Iteration 813, Loss: 0.058698\n",
      "Iteration 814, Loss: 0.058685\n",
      "Iteration 815, Loss: 0.058672\n",
      "Iteration 816, Loss: 0.058659\n",
      "Iteration 817, Loss: 0.058646\n",
      "Iteration 818, Loss: 0.058634\n",
      "Iteration 819, Loss: 0.058621\n",
      "Iteration 820, Loss: 0.058608\n",
      "Iteration 821, Loss: 0.058595\n",
      "Iteration 822, Loss: 0.058582\n",
      "Iteration 823, Loss: 0.058569\n",
      "Iteration 824, Loss: 0.058557\n",
      "Iteration 825, Loss: 0.058544\n",
      "Iteration 826, Loss: 0.058531\n",
      "Iteration 827, Loss: 0.058518\n",
      "Iteration 828, Loss: 0.058506\n",
      "Iteration 829, Loss: 0.058493\n",
      "Iteration 830, Loss: 0.058481\n",
      "Iteration 831, Loss: 0.058468\n",
      "Iteration 832, Loss: 0.058455\n",
      "Iteration 833, Loss: 0.058443\n",
      "Iteration 834, Loss: 0.058430\n",
      "Iteration 835, Loss: 0.058418\n",
      "Iteration 836, Loss: 0.058405\n",
      "Iteration 837, Loss: 0.058393\n",
      "Iteration 838, Loss: 0.058380\n",
      "Iteration 839, Loss: 0.058368\n",
      "Iteration 840, Loss: 0.058356\n",
      "Iteration 841, Loss: 0.058343\n",
      "Iteration 842, Loss: 0.058331\n",
      "Iteration 843, Loss: 0.058318\n",
      "Iteration 844, Loss: 0.058306\n",
      "Iteration 845, Loss: 0.058294\n",
      "Iteration 846, Loss: 0.058282\n",
      "Iteration 847, Loss: 0.058269\n",
      "Iteration 848, Loss: 0.058257\n",
      "Iteration 849, Loss: 0.058245\n",
      "Iteration 850, Loss: 0.058233\n",
      "Iteration 851, Loss: 0.058221\n",
      "Iteration 852, Loss: 0.058208\n",
      "Iteration 853, Loss: 0.058196\n",
      "Iteration 854, Loss: 0.058184\n",
      "Iteration 855, Loss: 0.058172\n",
      "Iteration 856, Loss: 0.058160\n",
      "Iteration 857, Loss: 0.058148\n",
      "Iteration 858, Loss: 0.058136\n",
      "Iteration 859, Loss: 0.058124\n",
      "Iteration 860, Loss: 0.058112\n",
      "Iteration 861, Loss: 0.058100\n",
      "Iteration 862, Loss: 0.058088\n",
      "Iteration 863, Loss: 0.058076\n",
      "Iteration 864, Loss: 0.058064\n",
      "Iteration 865, Loss: 0.058052\n",
      "Iteration 866, Loss: 0.058040\n",
      "Iteration 867, Loss: 0.058029\n",
      "Iteration 868, Loss: 0.058017\n",
      "Iteration 869, Loss: 0.058005\n",
      "Iteration 870, Loss: 0.057993\n",
      "Iteration 871, Loss: 0.057981\n",
      "Iteration 872, Loss: 0.057970\n",
      "Iteration 873, Loss: 0.057958\n",
      "Iteration 874, Loss: 0.057946\n",
      "Iteration 875, Loss: 0.057935\n",
      "Iteration 876, Loss: 0.057923\n",
      "Iteration 877, Loss: 0.057911\n",
      "Iteration 878, Loss: 0.057900\n",
      "Iteration 879, Loss: 0.057888\n",
      "Iteration 880, Loss: 0.057876\n",
      "Iteration 881, Loss: 0.057865\n",
      "Iteration 882, Loss: 0.057853\n",
      "Iteration 883, Loss: 0.057842\n",
      "Iteration 884, Loss: 0.057830\n",
      "Iteration 885, Loss: 0.057819\n",
      "Iteration 886, Loss: 0.057807\n",
      "Iteration 887, Loss: 0.057796\n",
      "Iteration 888, Loss: 0.057784\n",
      "Iteration 889, Loss: 0.057773\n",
      "Iteration 890, Loss: 0.057762\n",
      "Iteration 891, Loss: 0.057750\n",
      "Iteration 892, Loss: 0.057739\n",
      "Iteration 893, Loss: 0.057728\n",
      "Iteration 894, Loss: 0.057716\n",
      "Iteration 895, Loss: 0.057705\n",
      "Iteration 896, Loss: 0.057694\n",
      "Iteration 897, Loss: 0.057682\n",
      "Iteration 898, Loss: 0.057671\n",
      "Iteration 899, Loss: 0.057660\n",
      "Iteration 900, Loss: 0.057649\n",
      "Iteration 901, Loss: 0.057638\n",
      "Iteration 902, Loss: 0.057626\n",
      "Iteration 903, Loss: 0.057615\n",
      "Iteration 904, Loss: 0.057604\n",
      "Iteration 905, Loss: 0.057593\n",
      "Iteration 906, Loss: 0.057582\n",
      "Iteration 907, Loss: 0.057571\n",
      "Iteration 908, Loss: 0.057560\n",
      "Iteration 909, Loss: 0.057549\n",
      "Iteration 910, Loss: 0.057538\n",
      "Iteration 911, Loss: 0.057527\n",
      "Iteration 912, Loss: 0.057516\n",
      "Iteration 913, Loss: 0.057505\n",
      "Iteration 914, Loss: 0.057494\n",
      "Iteration 915, Loss: 0.057483\n",
      "Iteration 916, Loss: 0.057472\n",
      "Iteration 917, Loss: 0.057461\n",
      "Iteration 918, Loss: 0.057450\n",
      "Iteration 919, Loss: 0.057439\n",
      "Iteration 920, Loss: 0.057428\n",
      "Iteration 921, Loss: 0.057417\n",
      "Iteration 922, Loss: 0.057407\n",
      "Iteration 923, Loss: 0.057396\n",
      "Iteration 924, Loss: 0.057385\n",
      "Iteration 925, Loss: 0.057374\n",
      "Iteration 926, Loss: 0.057363\n",
      "Iteration 927, Loss: 0.057353\n",
      "Iteration 928, Loss: 0.057342\n",
      "Iteration 929, Loss: 0.057331\n",
      "Iteration 930, Loss: 0.057321\n",
      "Iteration 931, Loss: 0.057310\n",
      "Iteration 932, Loss: 0.057299\n",
      "Iteration 933, Loss: 0.057289\n",
      "Iteration 934, Loss: 0.057278\n",
      "Iteration 935, Loss: 0.057267\n",
      "Iteration 936, Loss: 0.057257\n",
      "Iteration 937, Loss: 0.057246\n",
      "Iteration 938, Loss: 0.057236\n",
      "Iteration 939, Loss: 0.057225\n",
      "Iteration 940, Loss: 0.057215\n",
      "Iteration 941, Loss: 0.057204\n",
      "Iteration 942, Loss: 0.057194\n",
      "Iteration 943, Loss: 0.057183\n",
      "Iteration 944, Loss: 0.057173\n",
      "Iteration 945, Loss: 0.057162\n",
      "Iteration 946, Loss: 0.057152\n",
      "Iteration 947, Loss: 0.057141\n",
      "Iteration 948, Loss: 0.057131\n",
      "Iteration 949, Loss: 0.057121\n",
      "Iteration 950, Loss: 0.057110\n",
      "Iteration 951, Loss: 0.057100\n",
      "Iteration 952, Loss: 0.057090\n",
      "Iteration 953, Loss: 0.057079\n",
      "Iteration 954, Loss: 0.057069\n",
      "Iteration 955, Loss: 0.057059\n",
      "Iteration 956, Loss: 0.057048\n",
      "Iteration 957, Loss: 0.057038\n",
      "Iteration 958, Loss: 0.057028\n",
      "Iteration 959, Loss: 0.057018\n",
      "Iteration 960, Loss: 0.057008\n",
      "Iteration 961, Loss: 0.056997\n",
      "Iteration 962, Loss: 0.056987\n",
      "Iteration 963, Loss: 0.056977\n",
      "Iteration 964, Loss: 0.056967\n",
      "Iteration 965, Loss: 0.056957\n",
      "Iteration 966, Loss: 0.056947\n",
      "Iteration 967, Loss: 0.056937\n",
      "Iteration 968, Loss: 0.056926\n",
      "Iteration 969, Loss: 0.056916\n",
      "Iteration 970, Loss: 0.056906\n",
      "Iteration 971, Loss: 0.056896\n",
      "Iteration 972, Loss: 0.056886\n",
      "Iteration 973, Loss: 0.056876\n",
      "Iteration 974, Loss: 0.056866\n",
      "Iteration 975, Loss: 0.056856\n",
      "Iteration 976, Loss: 0.056846\n",
      "Iteration 977, Loss: 0.056836\n",
      "Iteration 978, Loss: 0.056826\n",
      "Iteration 979, Loss: 0.056817\n",
      "Iteration 980, Loss: 0.056807\n",
      "Iteration 981, Loss: 0.056797\n",
      "Iteration 982, Loss: 0.056787\n",
      "Iteration 983, Loss: 0.056777\n",
      "Iteration 984, Loss: 0.056767\n",
      "Iteration 985, Loss: 0.056757\n",
      "Iteration 986, Loss: 0.056748\n",
      "Iteration 987, Loss: 0.056738\n",
      "Iteration 988, Loss: 0.056728\n",
      "Iteration 989, Loss: 0.056718\n",
      "Iteration 990, Loss: 0.056708\n",
      "Iteration 991, Loss: 0.056699\n",
      "Iteration 992, Loss: 0.056689\n",
      "Iteration 993, Loss: 0.056679\n",
      "Iteration 994, Loss: 0.056670\n",
      "Iteration 995, Loss: 0.056660\n",
      "Iteration 996, Loss: 0.056650\n",
      "Iteration 997, Loss: 0.056641\n",
      "Iteration 998, Loss: 0.056631\n",
      "Iteration 999, Loss: 0.056621\n",
      "Iteration 1000, Loss: 0.056612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BaselineModel at 0x1e83fe9a1e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = BaselineModel(learning_rate=0.1, n_iterations=1000)\n",
    "lr_model.fit(X_train_bias, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "515d662a-84b4-4939-88c8-1dd9e183f20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Loss: 0.908922\n"
     ]
    }
   ],
   "source": [
    "# Get predicted probabilities on test set\n",
    "y_test_lr = lr_model.predict(X_test_bias)  # shape: (n_test,)\n",
    "\n",
    "# Compute test loss using same loss function\n",
    "test_loss = lr_model._compute_loss(y_test, y_test_lr)\n",
    "\n",
    "print(f\"Logistic Regression Test Loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6317a43-84b9-4cc4-89be-a7c6a5411f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Accuracy, Precision, Recall, F1 for binary classification.\n",
    "    Assumes labels are 0 and 1.\n",
    "    \"\"\"\n",
    "    # True Positives, False Positives, etc.\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    # Handle division by zero\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc0cd969-750a-4c6b-abd6-af32473b3b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
      " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0\n",
      " 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1\n",
      " 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5092fbc4-6039-4899-b239-05e999fef3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "  Accuracy: 0.9737\n",
      "  Precision: 0.9859\n",
      "  Recall: 0.9722\n",
      "  F1: 0.9790\n"
     ]
    }
   ],
   "source": [
    "lr_metrics = compute_metrics(y_test, y_test_lr)\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "for metric, value in lr_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b30fdfd-388a-45b6-b48d-01b0a4092900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, architecture, learning_rate=0.01, n_iterations=1000):\n",
    "        self.architecture = architecture  # e.g., [30, 16, 8, 1]\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.loss_history = []\n",
    "        self.parameters = {}\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(42)\n",
    "        for l in range(1, len(self.architecture)):\n",
    "            self.parameters[f'W{l}'] = np.random.randn(\n",
    "                self.architecture[l], self.architecture[l-1]\n",
    "            ) * np.sqrt(2 / self.architecture[l-1])  # He initialization\n",
    "            self.parameters[f'b{l}'] = np.zeros((self.architecture[l], 1))\n",
    "\n",
    "    def _relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def _relu_derivative(self, Z):\n",
    "        return (Z > 0).astype(float)\n",
    "\n",
    "    def _sigmoid(self, Z):\n",
    "        Z = np.clip(Z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        A = X.T  # shape: (n_features, m)\n",
    "        self.cache['A0'] = A\n",
    "\n",
    "        L = len(self.architecture) - 1\n",
    "        for l in range(1, L):\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            Z = np.dot(W, A) + b\n",
    "            A = self._relu(Z)\n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            self.cache[f'A{l}'] = A\n",
    "\n",
    "        # Output layer (sigmoid for binary classification)\n",
    "        W = self.parameters[f'W{L}']\n",
    "        b = self.parameters[f'b{L}']\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = self._sigmoid(Z)\n",
    "        self.cache[f'Z{L}'] = Z\n",
    "        self.cache[f'A{L}'] = A\n",
    "\n",
    "        return A\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        y_true = y_true.reshape(1, -1)\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "\n",
    "    def backward_propagation(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        y = y.reshape(1, -1)\n",
    "        L = len(self.architecture) - 1\n",
    "\n",
    "        # Output layer gradient\n",
    "        dZ = self.cache[f'A{L}'] - y  # derivative of BCE + sigmoid\n",
    "        self.grads[f'dW{L}'] = (1/m) * np.dot(dZ, self.cache[f'A{L-1}'].T)\n",
    "        self.grads[f'db{L}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        # Hidden layers (backprop)\n",
    "        for l in range(L-1, 0, -1):\n",
    "            dA = np.dot(self.parameters[f'W{l+1}'].T, dZ)\n",
    "            dZ = dA * self._relu_derivative(self.cache[f'Z{l}'])\n",
    "            self.grads[f'dW{l}'] = (1/m) * np.dot(dZ, self.cache[f'A{l-1}'].T)\n",
    "            self.grads[f'db{l}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_iterations):\n",
    "            # Forward\n",
    "            A_final = self.forward_propagation(X)\n",
    "            # Loss\n",
    "            loss = self.compute_loss(y, A_final)\n",
    "            print('Loss:',loss)\n",
    "            self.loss_history.append(loss)\n",
    "            # Backward\n",
    "            self.backward_propagation(X, y)\n",
    "            # Update\n",
    "            L = len(self.architecture) - 1\n",
    "            for l in range(1, L+1):\n",
    "                self.parameters[f'W{l}'] -= self.lr * self.grads[f'dW{l}']\n",
    "                self.parameters[f'b{l}'] -= self.lr * self.grads[f'db{l}']\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        A = self.forward_propagation(X)\n",
    "        return A.flatten()\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        return (probs >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97ddc998-baf5-4e23-ba86-ebf42e4cb122",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2382ab8d-717e-4ec5-a24d-fc3ee99a24e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7587982566743475\n",
      "Loss: 0.753238791404811\n",
      "Loss: 0.7478527778992872\n",
      "Loss: 0.7426387490959018\n",
      "Loss: 0.7375305274503849\n",
      "Loss: 0.7324008066075841\n",
      "Loss: 0.7274395410123217\n",
      "Loss: 0.7225913421922461\n",
      "Loss: 0.7178238801055761\n",
      "Loss: 0.7129335251747646\n",
      "Loss: 0.7081006797713262\n",
      "Loss: 0.7033618261487791\n",
      "Loss: 0.6986683374374407\n",
      "Loss: 0.6942135187447988\n",
      "Loss: 0.6899886594392676\n",
      "Loss: 0.6858927321559997\n",
      "Loss: 0.6819702075959525\n",
      "Loss: 0.6781948044179056\n",
      "Loss: 0.6744632246203129\n",
      "Loss: 0.6708241162223305\n",
      "Loss: 0.6672450199988735\n",
      "Loss: 0.6636585691273323\n",
      "Loss: 0.6602061115322885\n",
      "Loss: 0.6568708447326117\n",
      "Loss: 0.6535846737269355\n",
      "Loss: 0.6504213195262302\n",
      "Loss: 0.647333542475045\n",
      "Loss: 0.644310629283448\n",
      "Loss: 0.6413477463540238\n",
      "Loss: 0.6383667048583838\n",
      "Loss: 0.6353986333106815\n",
      "Loss: 0.6325212793177877\n",
      "Loss: 0.6297561517657447\n",
      "Loss: 0.6270854762041629\n",
      "Loss: 0.6244925596011808\n",
      "Loss: 0.6219402110499739\n",
      "Loss: 0.6194648020535486\n",
      "Loss: 0.6170539128771104\n",
      "Loss: 0.6146986753574722\n",
      "Loss: 0.6123588192229635\n",
      "Loss: 0.6099485197533623\n",
      "Loss: 0.6075293419474647\n",
      "Loss: 0.6051119572845355\n",
      "Loss: 0.6027295280759681\n",
      "Loss: 0.6003811742411235\n",
      "Loss: 0.5980785284501755\n",
      "Loss: 0.5958579930905376\n",
      "Loss: 0.5936794546804371\n",
      "Loss: 0.5915400134782898\n",
      "Loss: 0.589453736922248\n",
      "Loss: 0.5873816364442586\n",
      "Loss: 0.5852688114556213\n",
      "Loss: 0.5831198277542583\n",
      "Loss: 0.5810168446459598\n",
      "Loss: 0.5789425455956373\n",
      "Loss: 0.57690520338346\n",
      "Loss: 0.574921798505498\n",
      "Loss: 0.5729931316809478\n",
      "Loss: 0.5710964828319001\n",
      "Loss: 0.5693083663877395\n",
      "Loss: 0.5675910627742263\n",
      "Loss: 0.5659087422943243\n",
      "Loss: 0.5642494861984727\n",
      "Loss: 0.562632178833289\n",
      "Loss: 0.5610315749193482\n",
      "Loss: 0.55943256144613\n",
      "Loss: 0.5578504828717694\n",
      "Loss: 0.5563010464426406\n",
      "Loss: 0.5547879696767771\n",
      "Loss: 0.5532974477091201\n",
      "Loss: 0.5518231190499466\n",
      "Loss: 0.5503605135439279\n",
      "Loss: 0.5489182492707332\n",
      "Loss: 0.5474240507985423\n",
      "Loss: 0.5459536765793581\n",
      "Loss: 0.5445241119069646\n",
      "Loss: 0.5431483923053897\n",
      "Loss: 0.5417784543139251\n",
      "Loss: 0.5404040021001167\n",
      "Loss: 0.5390564695305602\n",
      "Loss: 0.537733447229737\n",
      "Loss: 0.5364220983682395\n",
      "Loss: 0.5351184229052054\n",
      "Loss: 0.5338265305034037\n",
      "Loss: 0.5325462103678468\n",
      "Loss: 0.5312772569700271\n",
      "Loss: 0.5300276788836115\n",
      "Loss: 0.5288105130164797\n",
      "Loss: 0.5276054839295438\n",
      "Loss: 0.5264183479440232\n",
      "Loss: 0.525254381829842\n",
      "Loss: 0.5241143087116853\n",
      "Loss: 0.5229852227678681\n",
      "Loss: 0.5218359848962167\n",
      "Loss: 0.5206996526027191\n",
      "Loss: 0.5195794609881509\n",
      "Loss: 0.5184677721951784\n",
      "Loss: 0.5173668747145572\n",
      "Loss: 0.5162853005374108\n",
      "Loss: 0.5152243227261898\n",
      "Loss: 0.5141768953259772\n",
      "Loss: 0.5131443345264456\n",
      "Loss: 0.5121328802850882\n",
      "Loss: 0.5111281987041978\n",
      "Loss: 0.5101331517392375\n",
      "Loss: 0.5091509400681721\n",
      "Loss: 0.5081845265870316\n",
      "Loss: 0.5072298902838689\n",
      "Loss: 0.5062883188223428\n",
      "Loss: 0.5053439647764224\n",
      "Loss: 0.5043740518045194\n",
      "Loss: 0.503411416067243\n",
      "Loss: 0.5024383281280758\n",
      "Loss: 0.5014715041710825\n",
      "Loss: 0.5005115778805166\n",
      "Loss: 0.4995608415156864\n",
      "Loss: 0.4986164304727455\n",
      "Loss: 0.4976901112206278\n",
      "Loss: 0.496769411686453\n",
      "Loss: 0.4958542588626556\n",
      "Loss: 0.49494458123905405\n",
      "Loss: 0.4940403087648137\n",
      "Loss: 0.49314137281147125\n",
      "Loss: 0.4922477061369892\n",
      "Loss: 0.49135924285080995\n",
      "Loss: 0.4904759183798816\n",
      "Loss: 0.4896008063818169\n",
      "Loss: 0.4887365105577534\n",
      "Loss: 0.4878770314562155\n",
      "Loss: 0.48702729420456115\n",
      "Loss: 0.4861880818885574\n",
      "Loss: 0.48535483519350264\n",
      "Loss: 0.4845185333120201\n",
      "Loss: 0.4836866515804548\n",
      "Loss: 0.482859139592721\n",
      "Loss: 0.48203594789515175\n",
      "Loss: 0.481217027963477\n",
      "Loss: 0.48040233218042483\n",
      "Loss: 0.4795918138139276\n",
      "Loss: 0.4787854269959147\n",
      "Loss: 0.4779772985672601\n",
      "Loss: 0.47717069322854894\n",
      "Loss: 0.4763681491596756\n",
      "Loss: 0.4755696226292411\n",
      "Loss: 0.47477507069353375\n",
      "Loss: 0.47398445117800303\n",
      "Loss: 0.47319772265923415\n",
      "Loss: 0.47241484444740534\n",
      "Loss: 0.47163743555299015\n",
      "Loss: 0.47087010140492674\n",
      "Loss: 0.47010639316347896\n",
      "Loss: 0.4693462746035883\n",
      "Loss: 0.46858971011599154\n",
      "Loss: 0.4678366646933014\n",
      "Loss: 0.4670871039164542\n",
      "Loss: 0.466340993941514\n",
      "Loss: 0.46559186077261705\n",
      "Loss: 0.4648412083712823\n",
      "Loss: 0.4641002234494706\n",
      "Loss: 0.46336255921210895\n",
      "Loss: 0.46262818421415397\n",
      "Loss: 0.4618970675248725\n",
      "Loss: 0.4611669155403902\n",
      "Loss: 0.46043540524555016\n",
      "Loss: 0.4597071200088332\n",
      "Loss: 0.4589820300032793\n",
      "Loss: 0.45826010588235516\n",
      "Loss: 0.4575413187695071\n",
      "Loss: 0.4568256402479828\n",
      "Loss: 0.4561130423509082\n",
      "Loss: 0.4554034975516213\n",
      "Loss: 0.45469697875424764\n",
      "Loss: 0.4539934592845164\n",
      "Loss: 0.45329291288080703\n",
      "Loss: 0.4525953136854211\n",
      "Loss: 0.4519006362360733\n",
      "Loss: 0.45120885545759476\n",
      "Loss: 0.45051994665384376\n",
      "Loss: 0.4498338854998175\n",
      "Loss: 0.4491506480339576\n",
      "Loss: 0.44847021065064907\n",
      "Loss: 0.4477925500929007\n",
      "Loss: 0.44711764344520694\n",
      "Loss: 0.4464454681265843\n",
      "Loss: 0.44577134293382115\n",
      "Loss: 0.44509031134734417\n",
      "Loss: 0.4444120655345557\n",
      "Loss: 0.44373658240818104\n",
      "Loss: 0.4430638392167395\n",
      "Loss: 0.442393813537788\n",
      "Loss: 0.4417267255941913\n",
      "Loss: 0.44106664905020687\n",
      "Loss: 0.4404091745714434\n",
      "Loss: 0.43975459245569837\n",
      "Loss: 0.4391014551654233\n",
      "Loss: 0.4384381666078593\n",
      "Loss: 0.43777746465890216\n",
      "Loss: 0.4371193289784778\n",
      "Loss: 0.4364637395101288\n",
      "Loss: 0.4358106764755057\n",
      "Loss: 0.43516012036898694\n",
      "Loss: 0.43451205195242626\n",
      "Loss: 0.4338664522500187\n",
      "Loss: 0.433220269440112\n",
      "Loss: 0.43256657987905195\n",
      "Loss: 0.4319153824017953\n",
      "Loss: 0.4312666579567501\n",
      "Loss: 0.4306203877504722\n",
      "Loss: 0.42997655324278683\n",
      "Loss: 0.42933513614202096\n",
      "Loss: 0.42869611840034366\n",
      "Loss: 0.4280594822092128\n",
      "Loss: 0.42742520999492206\n",
      "Loss: 0.4267932844142504\n",
      "Loss: 0.42616368835020746\n",
      "Loss: 0.42553640490787387\n",
      "Loss: 0.4249114174103344\n",
      "Loss: 0.42429160102127594\n",
      "Loss: 0.4236769079735558\n",
      "Loss: 0.423064394769066\n",
      "Loss: 0.42245404639536294\n",
      "Loss: 0.42184584802540187\n",
      "Loss: 0.4212298164288912\n",
      "Loss: 0.42059668895805513\n",
      "Loss: 0.41996597582035133\n",
      "Loss: 0.41933765883659635\n",
      "Loss: 0.41871172006380863\n",
      "Loss: 0.4180881417909298\n",
      "Loss: 0.4174669065346366\n",
      "Loss: 0.4168479970352462\n",
      "Loss: 0.41623307758379413\n",
      "Loss: 0.41562281605533685\n",
      "Loss: 0.41501479098430366\n",
      "Loss: 0.4144089863894792\n",
      "Loss: 0.41380538648765464\n",
      "Loss: 0.4132039756901608\n",
      "Loss: 0.41260594574928827\n",
      "Loss: 0.4120135150000766\n",
      "Loss: 0.4114231742012996\n",
      "Loss: 0.41083490915611065\n",
      "Loss: 0.41024870583588813\n",
      "Loss: 0.40966455037738214\n",
      "Loss: 0.4090824290799211\n",
      "Loss: 0.40850232840267514\n",
      "Loss: 0.40792423496197705\n",
      "Loss: 0.4073481355286957\n",
      "Loss: 0.40677401702566507\n",
      "Loss: 0.40620186652516405\n",
      "Loss: 0.40563167124644933\n",
      "Loss: 0.4050634185533351\n",
      "Loss: 0.4044970959518248\n",
      "Loss: 0.40393269108778845\n",
      "Loss: 0.4033701917446869\n",
      "Loss: 0.40280958584134224\n",
      "Loss: 0.4022508614297524\n",
      "Loss: 0.401694006692948\n",
      "Loss: 0.4011390099428942\n",
      "Loss: 0.4005858596184314\n",
      "Loss: 0.400034544283259\n",
      "Loss: 0.39948505262395634\n",
      "Loss: 0.3989373734480444\n",
      "Loss: 0.398391495682085\n",
      "Loss: 0.3978474083698162\n",
      "Loss: 0.3973051006703256\n",
      "Loss: 0.3967645618562571\n",
      "Loss: 0.39622578131205427\n",
      "Loss: 0.39568874853223657\n",
      "Loss: 0.39515345311970884\n",
      "Loss: 0.39461988478410326\n",
      "Loss: 0.39408803334015435\n",
      "Loss: 0.39355788870610237\n",
      "Loss: 0.39302944090212966\n",
      "Loss: 0.39250268004882527\n",
      "Loss: 0.3919775963656798\n",
      "Loss: 0.3914541801696072\n",
      "Loss: 0.39093244111391295\n",
      "Loss: 0.3904150525126341\n",
      "Loss: 0.38989927744911085\n",
      "Loss: 0.3893851068649886\n",
      "Loss: 0.388872531789111\n",
      "Loss: 0.3883615433362613\n",
      "Loss: 0.38785213270592345\n",
      "Loss: 0.38734429118106806\n",
      "Loss: 0.38683801012695995\n",
      "Loss: 0.38633328098998715\n",
      "Loss: 0.3858300952965114\n",
      "Loss: 0.38532844465173965\n",
      "Loss: 0.3848283207386158\n",
      "Loss: 0.3843297153167326\n",
      "Loss: 0.3838341263906731\n",
      "Loss: 0.3833434941386352\n",
      "Loss: 0.38285430802069353\n",
      "Loss: 0.3823665605621464\n",
      "Loss: 0.3818811424211752\n",
      "Loss: 0.38139875056973094\n",
      "Loss: 0.3809177573734319\n",
      "Loss: 0.38043815576907847\n",
      "Loss: 0.37995993875387185\n",
      "Loss: 0.3794830993846262\n",
      "Loss: 0.3790076307769924\n",
      "Loss: 0.37853352610469565\n",
      "Loss: 0.37806077859878573\n",
      "Loss: 0.3775893815469011\n",
      "Loss: 0.3771193282925427\n",
      "Loss: 0.37665061223436425\n",
      "Loss: 0.3761832268254692\n",
      "Loss: 0.37571716557272344\n",
      "Loss: 0.37525242203607845\n",
      "Loss: 0.374788989827904\n",
      "Loss: 0.37432686261233483\n",
      "Loss: 0.37386603410462477\n",
      "Loss: 0.37340649807051535\n",
      "Loss: 0.37294824832561035\n",
      "Loss: 0.3724912787347647\n",
      "Loss: 0.37203558321148067\n",
      "Loss: 0.3715811557173152\n",
      "Loss: 0.3711279902612958\n",
      "Loss: 0.37067608089934745\n",
      "Loss: 0.3702254217337269\n",
      "Loss: 0.3697760069124675\n",
      "Loss: 0.3693278306288325\n",
      "Loss: 0.36888088712077655\n",
      "Loss: 0.368435170670417\n",
      "Loss: 0.36799067560351173\n",
      "Loss: 0.3675473962889477\n",
      "Loss: 0.3671053271382358\n",
      "Loss: 0.3666644626050133\n",
      "Loss: 0.36622479718455603\n",
      "Loss: 0.3657863254132971\n",
      "Loss: 0.3653490418683523\n",
      "Loss: 0.36491294116705486\n",
      "Loss: 0.3644780179664952\n",
      "Loss: 0.36404426696306996\n",
      "Loss: 0.3636116828920363\n",
      "Loss: 0.36318026052707464\n",
      "Loss: 0.3627499946798553\n",
      "Loss: 0.3623208801996163\n",
      "Loss: 0.3618919010262058\n",
      "Loss: 0.3614610966696567\n",
      "Loss: 0.361031432114451\n",
      "Loss: 0.3606029023082676\n",
      "Loss: 0.3601755022352192\n",
      "Loss: 0.3597492269154475\n",
      "Loss: 0.3593240714047245\n",
      "Loss: 0.3589000307940591\n",
      "Loss: 0.35847710020931006\n",
      "Loss: 0.35805527481080607\n",
      "Loss: 0.3576345497929691\n",
      "Loss: 0.3572149203839453\n",
      "Loss: 0.356796381845241\n",
      "Loss: 0.35637892947136307\n",
      "Loss: 0.35596255858946696\n",
      "Loss: 0.3555472645590069\n",
      "Loss: 0.35513304277139396\n",
      "Loss: 0.3547198886496576\n",
      "Loss: 0.35430779764811243\n",
      "Loss: 0.35389676525203057\n",
      "Loss: 0.35348678697731806\n",
      "Loss: 0.3530778583701955\n",
      "Loss: 0.3526699750068848\n",
      "Loss: 0.3522631324932994\n",
      "Loss: 0.3518573264647388\n",
      "Loss: 0.35145255258558783\n",
      "Loss: 0.35104880654902093\n",
      "Loss: 0.3506460840767093\n",
      "Loss: 0.35024438091853244\n",
      "Loss: 0.34984369285229505\n",
      "Loss: 0.3494440156834464\n",
      "Loss: 0.34904534524480496\n",
      "Loss: 0.34864767739628455\n",
      "Loss: 0.34825100802462816\n",
      "Loss: 0.3478553330431416\n",
      "Loss: 0.3474606483914332\n",
      "Loss: 0.34706742367755117\n",
      "Loss: 0.3466759924227\n",
      "Loss: 0.34628552531624657\n",
      "Loss: 0.3458960185261923\n",
      "Loss: 0.34550746824432355\n",
      "Loss: 0.345119870685987\n",
      "Loss: 0.3447332220898704\n",
      "Loss: 0.344347518717784\n",
      "Loss: 0.3439630446188331\n",
      "Loss: 0.3435814963434488\n",
      "Loss: 0.34320086477736966\n",
      "Loss: 0.34282114640762934\n",
      "Loss: 0.3424423377420917\n",
      "Loss: 0.34206443530926695\n",
      "Loss: 0.341687435658126\n",
      "Loss: 0.3413113353579208\n",
      "Loss: 0.34093613099800485\n",
      "Loss: 0.34056181918765693\n",
      "Loss: 0.3401883965559062\n",
      "Loss: 0.3398158597513606\n",
      "Loss: 0.33944420544203585\n",
      "Loss: 0.33907343031518894\n",
      "Loss: 0.3387035310771509\n",
      "Loss: 0.3383345044531628\n",
      "Loss: 0.33796634718721547\n",
      "Loss: 0.33759905604188784\n",
      "Loss: 0.33723262779818985\n",
      "Loss: 0.3368670592554066\n",
      "Loss: 0.3365023472309435\n",
      "Loss: 0.33613848856017475\n",
      "Loss: 0.3357754800962921\n",
      "Loss: 0.33541331871015706\n",
      "Loss: 0.3350520012901531\n",
      "Loss: 0.3346915247420415\n",
      "Loss: 0.33433188598881675\n",
      "Loss: 0.33397308197056563\n",
      "Loss: 0.33361510964432717\n",
      "Loss: 0.3332579659839539\n",
      "Loss: 0.3329016479799748\n",
      "Loss: 0.33254615263946086\n",
      "Loss: 0.33219147698589074\n",
      "Loss: 0.3318376180590188\n",
      "Loss: 0.3314845729147448\n",
      "Loss: 0.33113233862498426\n",
      "Loss: 0.3307809122775415\n",
      "Loss: 0.33043029097598303\n",
      "Loss: 0.33008047183951283\n",
      "Loss: 0.3297314520028493\n",
      "Loss: 0.3293832286161032\n",
      "Loss: 0.32903579884465683\n",
      "Loss: 0.3286891598690453\n",
      "Loss: 0.32834330888483815\n",
      "Loss: 0.32799824310252335\n",
      "Loss: 0.3276539597473914\n",
      "Loss: 0.3273104560594216\n",
      "Loss: 0.32696772929316964\n",
      "Loss: 0.3266257767176556\n",
      "Loss: 0.3262845956162538\n",
      "Loss: 0.3259441832865836\n",
      "Loss: 0.32560453704040193\n",
      "Loss: 0.32526565420349624\n",
      "Loss: 0.32492753211557857\n",
      "Loss: 0.32459016813018154\n",
      "Loss: 0.3242535596145552\n",
      "Loss: 0.32391770394956415\n",
      "Loss: 0.3235825985295867\n",
      "Loss: 0.32324824076241526\n",
      "Loss: 0.32291462806915655\n",
      "Loss: 0.32258175788413357\n",
      "Loss: 0.3222496276547899\n",
      "Loss: 0.32191823484159215\n",
      "Loss: 0.32158757691793616\n",
      "Loss: 0.3212576513700523\n",
      "Loss: 0.3209284556969131\n",
      "Loss: 0.3205999874101407\n",
      "Loss: 0.32027224403391613\n",
      "Loss: 0.3199452231048891\n",
      "Loss: 0.31961892217208815\n",
      "Loss: 0.31929333879683386\n",
      "Loss: 0.31896847055264954\n",
      "Loss: 0.31864431502517593\n",
      "Loss: 0.3183208698120853\n",
      "Loss: 0.3179981325229961\n",
      "Loss: 0.3176761007793896\n",
      "Loss: 0.31735477221452685\n",
      "Loss: 0.3170341444733652\n",
      "Loss: 0.3167142152124786\n",
      "Loss: 0.3163949820999754\n",
      "Loss: 0.3160764428154191\n",
      "Loss: 0.31575859504974924\n",
      "Loss: 0.31544143650520295\n",
      "Loss: 0.31512496489523745\n",
      "Loss: 0.31480917794445284\n",
      "Loss: 0.3144940733885165\n",
      "Loss: 0.3141796489740874\n",
      "Loss: 0.31386590245874196\n",
      "Loss: 0.3135528316108993\n",
      "Loss: 0.31324043420974873\n",
      "Loss: 0.3129287080451768\n",
      "Loss: 0.3126176509176959\n",
      "Loss: 0.3123072606383726\n",
      "Loss: 0.3119975350287577\n",
      "Loss: 0.3116884719208156\n",
      "Loss: 0.31138006915685584\n",
      "Loss: 0.31107232458946454\n",
      "Loss: 0.3107652360814361\n",
      "Loss: 0.310459396038375\n",
      "Loss: 0.31015488377638445\n",
      "Loss: 0.30985101615267613\n",
      "Loss: 0.3095479522089762\n",
      "Loss: 0.3092458574968177\n",
      "Loss: 0.30894440254133176\n",
      "Loss: 0.30864358527819064\n",
      "Loss: 0.30834340365279395\n",
      "Loss: 0.30804385562020625\n",
      "Loss: 0.3077449391450937\n",
      "Loss: 0.3074466522016632\n",
      "Loss: 0.3071489927735997\n",
      "Loss: 0.3068519588540067\n",
      "Loss: 0.3065555484453448\n",
      "Loss: 0.3062597595593733\n",
      "Loss: 0.3059645902170897\n",
      "Loss: 0.3056700384486716\n",
      "Loss: 0.30537610229341844\n",
      "Loss: 0.30508277979969406\n",
      "Loss: 0.30479006902486927\n",
      "Loss: 0.3044979680352648\n",
      "Loss: 0.30420647490609587\n",
      "Loss: 0.3039155877214157\n",
      "Loss: 0.303625304574061\n",
      "Loss: 0.30333562356559707\n",
      "Loss: 0.3030465428062626\n",
      "Loss: 0.30275806041491693\n",
      "Loss: 0.3024701745189865\n",
      "Loss: 0.30218288325441145\n",
      "Loss: 0.30189618476559293\n",
      "Loss: 0.30161007720534216\n",
      "Loss: 0.3013245587348279\n",
      "Loss: 0.3010396275235255\n",
      "Loss: 0.3007552817491662\n",
      "Loss: 0.30047151959768653\n",
      "Loss: 0.30018833926317867\n",
      "Loss: 0.2999057389478413\n",
      "Loss: 0.29962371686192946\n",
      "Loss: 0.2993422712237069\n",
      "Loss: 0.299061400259397\n",
      "Loss: 0.29878110220313575\n",
      "Loss: 0.2985013752969229\n",
      "Loss: 0.29822221779057606\n",
      "Loss: 0.29794362794168366\n",
      "Loss: 0.29766560401555764\n",
      "Loss: 0.29738814428518917\n",
      "Loss: 0.2971112470312012\n",
      "Loss: 0.2968349105418042\n",
      "Loss: 0.29655913311275145\n",
      "Loss: 0.2962839130472935\n",
      "Loss: 0.2960092486561348\n",
      "Loss: 0.2957351382573897\n",
      "Loss: 0.2954615801765384\n",
      "Loss: 0.2951885727463844\n",
      "Loss: 0.2949161143070114\n",
      "Loss: 0.2946442032057408\n",
      "Loss: 0.2943728377970897\n",
      "Loss: 0.2941020164427283\n",
      "Loss: 0.29383173751143976\n",
      "Loss: 0.2935619993790776\n",
      "Loss: 0.29329280042852546\n",
      "Loss: 0.29302413904965674\n",
      "Loss: 0.29275601363929404\n",
      "Loss: 0.2924884226011686\n",
      "Loss: 0.292221364345882\n",
      "Loss: 0.2919548372908657\n",
      "Loss: 0.2916888398603423\n",
      "Loss: 0.29142337048528666\n",
      "Loss: 0.29115842760338817\n",
      "Loss: 0.290894009659011\n",
      "Loss: 0.2906301151031583\n",
      "Loss: 0.2903667423934325\n",
      "Loss: 0.29010388999399916\n",
      "Loss: 0.28984155637554954\n",
      "Loss: 0.28957974001526465\n",
      "Loss: 0.2893184393967768\n",
      "Loss: 0.2890576530101357\n",
      "Loss: 0.28879737935177036\n",
      "Loss: 0.28853761692445495\n",
      "Loss: 0.28827836423727254\n",
      "Loss: 0.28801961980558005\n",
      "Loss: 0.28776138215097335\n",
      "Loss: 0.28750364980125237\n",
      "Loss: 0.28724642129038724\n",
      "Loss: 0.2869896951584833\n",
      "Loss: 0.2867334699517476\n",
      "Loss: 0.2864777442224553\n",
      "Loss: 0.2862225165289158\n",
      "Loss: 0.28596778543543994\n",
      "Loss: 0.2857135495123065\n",
      "Loss: 0.2854598073357303\n",
      "Loss: 0.28520655748782836\n",
      "Loss: 0.28495379855658903\n",
      "Loss: 0.284701529135839\n",
      "Loss: 0.28444974782521143\n",
      "Loss: 0.2841984532301152\n",
      "Loss: 0.2839476439617025\n",
      "Loss: 0.28369731863683806\n",
      "Loss: 0.28344747587806834\n",
      "Loss: 0.28319811431359054\n",
      "Loss: 0.28294923257722204\n",
      "Loss: 0.28270082930837076\n",
      "Loss: 0.2824529031520037\n",
      "Loss: 0.2822054527586181\n",
      "Loss: 0.2819584767842113\n",
      "Loss: 0.2817119738902522\n",
      "Loss: 0.2814659427436497\n",
      "Loss: 0.28122038201672617\n",
      "Loss: 0.28097529038718694\n",
      "Loss: 0.28073066653809203\n",
      "Loss: 0.2804865091578279\n",
      "Loss: 0.2802428169400785\n",
      "Loss: 0.27999958858379786\n",
      "Loss: 0.2797568227931817\n",
      "Loss: 0.2795145182776399\n",
      "Loss: 0.2792726737517685\n",
      "Loss: 0.27903128793532317\n",
      "Loss: 0.2787903595531907\n",
      "Loss: 0.27854988733536346\n",
      "Loss: 0.2783098700169116\n",
      "Loss: 0.2780703063379563\n",
      "Loss: 0.27783119504364423\n",
      "Loss: 0.2775925348841201\n",
      "Loss: 0.27735432461450105\n",
      "Loss: 0.27711656299485116\n",
      "Loss: 0.2768792487901545\n",
      "Loss: 0.27664238077029035\n",
      "Loss: 0.2764059577100079\n",
      "Loss: 0.27616997838889995\n",
      "Loss: 0.275934441591379\n",
      "Loss: 0.2756973186410832\n",
      "Loss: 0.27545580133986053\n",
      "Loss: 0.27521471456931645\n",
      "Loss: 0.2749740572349193\n",
      "Loss: 0.27473382824624665\n",
      "Loss: 0.27449402651696514\n",
      "Loss: 0.2742546509648102\n",
      "Loss: 0.27401570051156643\n",
      "Loss: 0.2737771740830469\n",
      "Loss: 0.27353907060907423\n",
      "Loss: 0.27330138902346035\n",
      "Loss: 0.2730641282639872\n",
      "Loss: 0.2728272872723869\n",
      "Loss: 0.27259086499432295\n",
      "Loss: 0.2723548603793707\n",
      "Loss: 0.272119272380998\n",
      "Loss: 0.2718840999565466\n",
      "Loss: 0.27164934206721325\n",
      "Loss: 0.2714149976780306\n",
      "Loss: 0.2711810657578485\n",
      "Loss: 0.27094754527931575\n",
      "Loss: 0.27071443521886135\n",
      "Loss: 0.2704817345566762\n",
      "Loss: 0.270249442276695\n",
      "Loss: 0.2700175573665778\n",
      "Loss: 0.269786078817692\n",
      "Loss: 0.2695550056250946\n",
      "Loss: 0.26932433678751416\n",
      "Loss: 0.26909407130733304\n",
      "Loss: 0.2688642081905697\n",
      "Loss: 0.2686347464468617\n",
      "Loss: 0.2684056850894469\n",
      "Loss: 0.2681770231351477\n",
      "Loss: 0.26794875960435266\n",
      "Loss: 0.26772089352100004\n",
      "Loss: 0.2674934239125597\n",
      "Loss: 0.26726634981001757\n",
      "Loss: 0.2670399376902281\n",
      "Loss: 0.26681439560865905\n",
      "Loss: 0.2665892400411608\n",
      "Loss: 0.2663644700803724\n",
      "Loss: 0.2661400848220063\n",
      "Loss: 0.2659160833648352\n",
      "Loss: 0.26569246481067765\n",
      "Loss: 0.26546922826438496\n",
      "Loss: 0.26524637283382774\n",
      "Loss: 0.26502389762988154\n",
      "Loss: 0.2648018017664144\n",
      "Loss: 0.2645800843602732\n",
      "Loss: 0.26435874453127034\n",
      "Loss: 0.26413778140217015\n",
      "Loss: 0.2639169079426046\n",
      "Loss: 0.26369300895919157\n",
      "Loss: 0.2634694796455836\n",
      "Loss: 0.26324631918055025\n",
      "Loss: 0.2630235267455256\n",
      "Loss: 0.26280110152459657\n",
      "Loss: 0.26257904270449145\n",
      "Loss: 0.26235734947456735\n",
      "Loss: 0.26213602102679956\n",
      "Loss: 0.2619150565557696\n",
      "Loss: 0.2616944552586535\n",
      "Loss: 0.26147421633521056\n",
      "Loss: 0.2612543389877721\n",
      "Loss: 0.26103482242123005\n",
      "Loss: 0.2608156658430253\n",
      "Loss: 0.2605968684631369\n",
      "Loss: 0.26037842949407064\n",
      "Loss: 0.2601603481508483\n",
      "Loss: 0.25994262365099596\n",
      "Loss: 0.25972525521453366\n",
      "Loss: 0.25950824206396383\n",
      "Loss: 0.2592915834242607\n",
      "Loss: 0.25907527852285944\n",
      "Loss: 0.2588593265896453\n",
      "Loss: 0.258643726856943\n",
      "Loss: 0.25842847855950546\n",
      "Loss: 0.25821358093450403\n",
      "Loss: 0.25799903322151724\n",
      "Loss: 0.25778483466252056\n",
      "Loss: 0.2575709845018754\n",
      "Loss: 0.25735748198631964\n",
      "Loss: 0.2571443263649561\n",
      "Loss: 0.2569315168892431\n",
      "Loss: 0.2567190528129834\n",
      "Loss: 0.2565069333923148\n",
      "Loss: 0.25629515788569907\n",
      "Loss: 0.25608372555391257\n",
      "Loss: 0.25587263566003543\n",
      "Loss: 0.2556618874694421\n",
      "Loss: 0.255451480249791\n",
      "Loss: 0.2552414132710145\n",
      "Loss: 0.25503168580530966\n",
      "Loss: 0.2548222971271273\n",
      "Loss: 0.2546132465131631\n",
      "Loss: 0.2544045332423474\n",
      "Loss: 0.25419615659583555\n",
      "Loss: 0.2539881158569983\n",
      "Loss: 0.253780410311412\n",
      "Loss: 0.25357303924684954\n",
      "Loss: 0.25336600195327\n",
      "Loss: 0.25315929772280976\n",
      "Loss: 0.25295292584977297\n",
      "Loss: 0.2527468856306218\n",
      "Loss: 0.2525411763639677\n",
      "Loss: 0.2523357973505616\n",
      "Loss: 0.25213074789328477\n",
      "Loss: 0.2519260272971397\n",
      "Loss: 0.2517216348692408\n",
      "Loss: 0.2515175699188053\n",
      "Loss: 0.25131383175714445\n",
      "Loss: 0.25111041969765385\n",
      "Loss: 0.2509073330558051\n",
      "Loss: 0.2507045711491365\n",
      "Loss: 0.25050213329724424\n",
      "Loss: 0.25030001882177344\n",
      "Loss: 0.25009822704640955\n",
      "Loss: 0.2498967572968693\n",
      "Loss: 0.24969560890089196\n",
      "Loss: 0.24949478118823099\n",
      "Loss: 0.24929427349064512\n",
      "Loss: 0.24909408514188977\n",
      "Loss: 0.24889421547770818\n",
      "Loss: 0.24869466383582367\n",
      "Loss: 0.2484954295559303\n",
      "Loss: 0.248296511979685\n",
      "Loss: 0.24809791045069893\n",
      "Loss: 0.2478996243145288\n",
      "Loss: 0.24770165291866938\n",
      "Loss: 0.2475039956125444\n",
      "Loss: 0.24730665174749883\n",
      "Loss: 0.2471096206767903\n",
      "Loss: 0.24691290175558106\n",
      "Loss: 0.24671649434093007\n",
      "Loss: 0.2465203977917846\n",
      "Loss: 0.24632491640898183\n",
      "Loss: 0.2461304125783851\n",
      "Loss: 0.2459362145281472\n",
      "Loss: 0.2457423216322367\n",
      "Loss: 0.24554873326645005\n",
      "Loss: 0.24535544880840288\n",
      "Loss: 0.24516246763752308\n",
      "Loss: 0.24496978913504258\n",
      "Loss: 0.2447774126839905\n",
      "Loss: 0.2445853376691847\n",
      "Loss: 0.24439356347722432\n",
      "Loss: 0.24420208949648334\n",
      "Loss: 0.24401091511710146\n",
      "Loss: 0.24382003973097807\n",
      "Loss: 0.2436294627317642\n",
      "Loss: 0.24343918351485505\n",
      "Loss: 0.2432492014773831\n",
      "Loss: 0.24305951601821033\n",
      "Loss: 0.24287012653792142\n",
      "Loss: 0.2426810324388162\n",
      "Loss: 0.2424922331249025\n",
      "Loss: 0.24230372800188937\n",
      "Loss: 0.24211551647717927\n",
      "Loss: 0.24192759795986157\n",
      "Loss: 0.24173997186070534\n",
      "Loss: 0.24155263759215223\n",
      "Loss: 0.24136559456830955\n",
      "Loss: 0.2411788422049433\n",
      "Loss: 0.24099237991947142\n",
      "Loss: 0.24080620713095666\n",
      "Loss: 0.24062032326009972\n",
      "Loss: 0.24043472772923288\n",
      "Loss: 0.24024941996231264\n",
      "Loss: 0.24006439938491358\n",
      "Loss: 0.23987966542422084\n",
      "Loss: 0.23969521750902414\n",
      "Loss: 0.2395110550697109\n",
      "Loss: 0.2393271775382598\n",
      "Loss: 0.23914358434823357\n",
      "Loss: 0.23896027493477312\n",
      "Loss: 0.238777248734591\n",
      "Loss: 0.23859450518596445\n",
      "Loss: 0.23841204372872923\n",
      "Loss: 0.23822986380427316\n",
      "Loss: 0.2380479648555298\n",
      "Loss: 0.2378663463269717\n",
      "Loss: 0.23768500766460499\n",
      "Loss: 0.2375039483159619\n",
      "Loss: 0.23732316773009537\n",
      "Loss: 0.23714266535757236\n",
      "Loss: 0.23696244065046798\n",
      "Loss: 0.2367824930623589\n",
      "Loss: 0.23660282204831765\n",
      "Loss: 0.23642342706490593\n",
      "Loss: 0.23624430757016948\n",
      "Loss: 0.2360654630236306\n",
      "Loss: 0.23588689288628378\n",
      "Loss: 0.2357085966205884\n",
      "Loss: 0.23553057369046318\n",
      "Loss: 0.23535282356128054\n",
      "Loss: 0.2351753456998603\n",
      "Loss: 0.234998139574464\n",
      "Loss: 0.234821204654789\n",
      "Loss: 0.23464485111257197\n",
      "Loss: 0.2344697283239422\n",
      "Loss: 0.2342948749820157\n",
      "Loss: 0.23412029055147043\n",
      "Loss: 0.23394597449846108\n",
      "Loss: 0.2337719262906127\n",
      "Loss: 0.23359814539701526\n",
      "Loss: 0.23342463128821667\n",
      "Loss: 0.2332513834362179\n",
      "Loss: 0.233078401314466\n",
      "Loss: 0.23290568439784887\n",
      "Loss: 0.2327332321626891\n",
      "Loss: 0.23256104408673794\n",
      "Loss: 0.23238911964916983\n",
      "Loss: 0.23221745833057617\n",
      "Loss: 0.23204605961295974\n",
      "Loss: 0.23187492297972898\n",
      "Loss: 0.23170409659785215\n",
      "Loss: 0.2315342012981783\n",
      "Loss: 0.23136456641826292\n",
      "Loss: 0.23119519143817632\n",
      "Loss: 0.23102607583943482\n",
      "Loss: 0.23085721910499554\n",
      "Loss: 0.23068862071925045\n",
      "Loss: 0.23052028016801956\n",
      "Loss: 0.23035219693854583\n",
      "Loss: 0.23018437051948895\n",
      "Loss: 0.23001680040091982\n",
      "Loss: 0.22984948607431396\n",
      "Loss: 0.22968242703254638\n",
      "Loss: 0.22951562276988566\n",
      "Loss: 0.229349072781988\n",
      "Loss: 0.22918277656589198\n",
      "Loss: 0.2290167336200119\n",
      "Loss: 0.22885094344413318\n",
      "Loss: 0.22868540553940656\n",
      "Loss: 0.22852011940834172\n",
      "Loss: 0.2283550845548027\n",
      "Loss: 0.22819030048400146\n",
      "Loss: 0.22802576670249314\n",
      "Loss: 0.2278614827181704\n",
      "Loss: 0.2276974480402572\n",
      "Loss: 0.2275336621793046\n",
      "Loss: 0.22737012464718462\n",
      "Loss: 0.22720683495708474\n",
      "Loss: 0.22704379262350302\n",
      "Loss: 0.22688099716224264\n",
      "Loss: 0.22671844809040648\n",
      "Loss: 0.22655614492639184\n",
      "Loss: 0.22639408718988535\n",
      "Loss: 0.22623227440185792\n",
      "Loss: 0.22607070608455895\n",
      "Loss: 0.225909381761512\n",
      "Loss: 0.22574830095750895\n",
      "Loss: 0.2255874631986053\n",
      "Loss: 0.22542688893288726\n",
      "Loss: 0.2252665806470539\n",
      "Loss: 0.22510651310901772\n",
      "Loss: 0.22494668585639688\n",
      "Loss: 0.22478709842799555\n",
      "Loss: 0.2246277503637995\n",
      "Loss: 0.224468641204971\n",
      "Loss: 0.22430977049384537\n",
      "Loss: 0.22415113999013778\n",
      "Loss: 0.22399274640458583\n",
      "Loss: 0.2238345886241114\n",
      "Loss: 0.22367666747013795\n",
      "Loss: 0.22351898249081273\n",
      "Loss: 0.22336153323542873\n",
      "Loss: 0.22320431925441994\n",
      "Loss: 0.2230473400993574\n",
      "Loss: 0.22289059532294458\n",
      "Loss: 0.2227340844790136\n",
      "Loss: 0.22257780712252018\n",
      "Loss: 0.22242176495340546\n",
      "Loss: 0.22226595668930152\n",
      "Loss: 0.22211037744285242\n",
      "Loss: 0.221955029912766\n",
      "Loss: 0.22179991365957152\n",
      "Loss: 0.22164502824489682\n",
      "Loss: 0.22149037323146423\n",
      "Loss: 0.22133594818308677\n",
      "Loss: 0.2211817538955637\n",
      "Loss: 0.2210277930880878\n",
      "Loss: 0.22087405562487689\n",
      "Loss: 0.2207205463908273\n",
      "Loss: 0.2205672649551596\n",
      "Loss: 0.2204142108881602\n",
      "Loss: 0.2202613837611777\n",
      "Loss: 0.22010878594269043\n",
      "Loss: 0.21995641653022305\n",
      "Loss: 0.21980426794930194\n",
      "Loss: 0.21965234460243985\n",
      "Loss: 0.2195006460662527\n",
      "Loss: 0.2193491719183957\n",
      "Loss: 0.21919792512258238\n",
      "Loss: 0.2190469039118094\n",
      "Loss: 0.21889610068510623\n",
      "Loss: 0.21874552016582874\n",
      "Loss: 0.21859516193678322\n",
      "Loss: 0.2184450257447508\n",
      "Loss: 0.21829512023384945\n",
      "Loss: 0.21814542665628694\n",
      "Loss: 0.21799595370851946\n",
      "Loss: 0.2178467009784107\n",
      "Loss: 0.2176976688669159\n",
      "Loss: 0.21754886479724006\n",
      "Loss: 0.21740027052511135\n",
      "Loss: 0.2172518948302986\n",
      "Loss: 0.21710373730563126\n",
      "Loss: 0.21695580278448506\n",
      "Loss: 0.21680808611617833\n",
      "Loss: 0.216660580930772\n",
      "Loss: 0.2165132922948988\n",
      "Loss: 0.21636622178655193\n",
      "Loss: 0.21621937460310112\n",
      "Loss: 0.21607273346337805\n",
      "Loss: 0.215926307268116\n",
      "Loss: 0.21578009675048143\n",
      "Loss: 0.21563411021284526\n",
      "Loss: 0.21548832671700166\n",
      "Loss: 0.21534275657586394\n",
      "Loss: 0.21519740204276758\n",
      "Loss: 0.21505226741894978\n",
      "Loss: 0.21490733522621389\n",
      "Loss: 0.21476261481352665\n",
      "Loss: 0.21461811227475666\n",
      "Loss: 0.21447382093799977\n",
      "Loss: 0.2143297337677617\n",
      "Loss: 0.21418585681779959\n",
      "Loss: 0.2140422022991566\n",
      "Loss: 0.2138987457261696\n",
      "Loss: 0.21375549735712998\n",
      "Loss: 0.21361246466319536\n",
      "Loss: 0.21346964036170257\n",
      "Loss: 0.21332701710500743\n",
      "Loss: 0.21318460410260573\n",
      "Loss: 0.2130424072871528\n",
      "Loss: 0.2129004056986522\n",
      "Loss: 0.2127586104940235\n",
      "Loss: 0.2126170358030535\n",
      "Loss: 0.21247565272152974\n",
      "Loss: 0.21233447463200225\n",
      "Loss: 0.21219351459992097\n",
      "Loss: 0.21205274808536181\n",
      "Loss: 0.21191218372275364\n",
      "Loss: 0.2117718362372049\n",
      "Loss: 0.2116316816435863\n",
      "Loss: 0.211491727643303\n",
      "Loss: 0.2113519905980366\n",
      "Loss: 0.21121244332024736\n",
      "Loss: 0.2110730968215371\n",
      "Loss: 0.21093396730164607\n",
      "Loss: 0.21079502323099727\n",
      "Loss: 0.21065628375379128\n",
      "Loss: 0.21051775388297217\n",
      "Loss: 0.21037941131444332\n",
      "Loss: 0.21024127676827295\n",
      "Loss: 0.2101033420487334\n",
      "Loss: 0.20996559770219134\n",
      "Loss: 0.20982806600278578\n",
      "Loss: 0.20969072197580413\n",
      "Loss: 0.2095535759029281\n",
      "Loss: 0.2094166384947281\n",
      "Loss: 0.20927988402581527\n",
      "Loss: 0.2091433374169173\n",
      "Loss: 0.20900698327550282\n",
      "Loss: 0.20887082010216051\n",
      "Loss: 0.20873486760258717\n",
      "Loss: 0.2085990940610929\n",
      "Loss: 0.20846352698493645\n",
      "Loss: 0.20832815043295916\n",
      "Loss: 0.20819296461160217\n",
      "Loss: 0.20805798479542245\n",
      "Loss: 0.20792318333227883\n",
      "Loss: 0.20778859025744023\n",
      "Loss: 0.20765417917407195\n",
      "Loss: 0.20751996518358556\n",
      "Loss: 0.20738594599942625\n",
      "Loss: 0.2072521102589939\n",
      "Loss: 0.2071184810264375\n",
      "Loss: 0.2069850258340347\n",
      "Loss: 0.2068517782219624\n",
      "Loss: 0.2067187077458463\n",
      "Loss: 0.206585836227559\n",
      "Loss: 0.2064531509617875\n",
      "Loss: 0.20632065470848088\n",
      "Loss: 0.20618835274329475\n",
      "Loss: 0.20605623092763165\n",
      "Loss: 0.2059243103640184\n",
      "Loss: 0.20579256216013528\n",
      "Loss: 0.2056610211097425\n",
      "Loss: 0.20552964752444483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x1e8420d21e0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Architecture: [input=30, hidden1=1,output=1]\n",
    "mlp_model = MLP(architecture=[30,1, 1], learning_rate=0.01, n_iterations=1000)\n",
    "mlp_model.fit(X_train, y_train)  # Note: no bias column needed — bias handled internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3006c1bc-a217-482a-8493-1d589e3a867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0\n",
      " 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1\n",
      " 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred_mlp=mlp_model.predict(X_test)\n",
    "print(y_pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9afb685b-e300-40b8-8438-5aaa5660bdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "  Accuracy: 0.9474\n",
      "  Precision: 0.9459\n",
      "  Recall: 0.9722\n",
      "  F1: 0.9589\n"
     ]
    }
   ],
   "source": [
    "mlp_metrics = compute_metrics(y_test, y_pred_mlp)\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "for metric, value in mlp_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a613bb24-0355-4762-bbde-98c488e61575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7587982566743475\n",
      "Loss: 0.7057441995983785\n",
      "Loss: 0.6625366860873356\n",
      "Loss: 0.6304829141701719\n",
      "Loss: 0.6050158057861365\n",
      "Loss: 0.5825488766073464\n",
      "Loss: 0.5632812707666219\n",
      "Loss: 0.5479574627812874\n",
      "Loss: 0.5342087613015446\n",
      "Loss: 0.522134234557612\n",
      "Loss: 0.5114495444220907\n",
      "Loss: 0.50188278655202\n",
      "Loss: 0.49266018392375155\n",
      "Loss: 0.4841400445519554\n",
      "Loss: 0.476052208384858\n",
      "Loss: 0.46836392796295884\n",
      "Loss: 0.46100940973339916\n",
      "Loss: 0.4538603792900812\n",
      "Loss: 0.44700215903324364\n",
      "Loss: 0.44031187287873286\n",
      "Loss: 0.4337539347013748\n",
      "Loss: 0.4273264193540251\n",
      "Loss: 0.421110284704613\n",
      "Loss: 0.4149225742388349\n",
      "Loss: 0.4090190267591997\n",
      "Loss: 0.40331584706252044\n",
      "Loss: 0.3978005587165894\n",
      "Loss: 0.39246831861674564\n",
      "Loss: 0.38732245190966624\n",
      "Loss: 0.3823625605543773\n",
      "Loss: 0.3775886552289084\n",
      "Loss: 0.3729485545945167\n",
      "Loss: 0.3684359900561603\n",
      "Loss: 0.36404516631793793\n",
      "Loss: 0.35976081355785355\n",
      "Loss: 0.35555863836490587\n",
      "Loss: 0.35146536120354704\n",
      "Loss: 0.3474855433690855\n",
      "Loss: 0.3436122703225602\n",
      "Loss: 0.33984435143250036\n",
      "Loss: 0.3361646589386687\n",
      "Loss: 0.3325699801459809\n",
      "Loss: 0.32905727621407227\n",
      "Loss: 0.3256236686380925\n",
      "Loss: 0.3222664271981883\n",
      "Loss: 0.31898295917435127\n",
      "Loss: 0.3157707996558543\n",
      "Loss: 0.31263100111393205\n",
      "Loss: 0.30956696681800894\n",
      "Loss: 0.30656704162047965\n",
      "Loss: 0.3036295413585083\n",
      "Loss: 0.3007569271264686\n",
      "Loss: 0.2979427480161369\n",
      "Loss: 0.295185231904692\n",
      "Loss: 0.29248268326789756\n",
      "Loss: 0.28983347883526683\n",
      "Loss: 0.2872360635663596\n",
      "Loss: 0.2846889469164912\n",
      "Loss: 0.2821906993640767\n",
      "Loss: 0.27973994917523426\n",
      "Loss: 0.2773353793841706\n",
      "Loss: 0.27497572497037964\n",
      "Loss: 0.272645458288152\n",
      "Loss: 0.2703021709018199\n",
      "Loss: 0.2680034356504292\n",
      "Loss: 0.2657475007891421\n",
      "Loss: 0.26352976365699554\n",
      "Loss: 0.26134934599462684\n",
      "Loss: 0.2591900240344054\n",
      "Loss: 0.25704362327196\n",
      "Loss: 0.25493172700516675\n",
      "Loss: 0.25285360464666573\n",
      "Loss: 0.25080854793167046\n",
      "Loss: 0.24879586996547343\n",
      "Loss: 0.24681629839012714\n",
      "Loss: 0.24487597606169934\n",
      "Loss: 0.2429657830731893\n",
      "Loss: 0.24108511647227085\n",
      "Loss: 0.23923339035199997\n",
      "Loss: 0.23741003514779754\n",
      "Loss: 0.23561485509236957\n",
      "Loss: 0.23384705764194208\n",
      "Loss: 0.23210594281281813\n",
      "Loss: 0.23039692055109437\n",
      "Loss: 0.22872031538485563\n",
      "Loss: 0.2270721808941073\n",
      "Loss: 0.225452851025962\n",
      "Loss: 0.22385768733431163\n",
      "Loss: 0.22228628659217697\n",
      "Loss: 0.22073811126841592\n",
      "Loss: 0.21921275581857128\n",
      "Loss: 0.21770979531586607\n",
      "Loss: 0.21622881513802455\n",
      "Loss: 0.21476945323789484\n",
      "Loss: 0.21333133474097915\n",
      "Loss: 0.211913927935976\n",
      "Loss: 0.21051696212304336\n",
      "Loss: 0.2091401797653792\n",
      "Loss: 0.20778294510803738\n",
      "Loss: 0.20644517185949607\n",
      "Loss: 0.2051263532728848\n",
      "Loss: 0.20382622304552317\n",
      "Loss: 0.20254444114845424\n",
      "Loss: 0.2012806826344074\n",
      "Loss: 0.20003459407516963\n",
      "Loss: 0.19880598812746803\n",
      "Loss: 0.1975943606761247\n",
      "Loss: 0.19639964273914035\n",
      "Loss: 0.19522138786586332\n",
      "Loss: 0.1940593195868971\n",
      "Loss: 0.19291324633103932\n",
      "Loss: 0.19178287678446002\n",
      "Loss: 0.19066784936237963\n",
      "Loss: 0.189567976649835\n",
      "Loss: 0.18848300019176903\n",
      "Loss: 0.18741266922909358\n",
      "Loss: 0.18635676017817904\n",
      "Loss: 0.18531495124203043\n",
      "Loss: 0.18428705550764812\n",
      "Loss: 0.18327283880865086\n",
      "Loss: 0.18227207148788957\n",
      "Loss: 0.18128452828725983\n",
      "Loss: 0.18030998824189728\n",
      "Loss: 0.17934823457850657\n",
      "Loss: 0.17839905461758376\n",
      "Loss: 0.17746223967930932\n",
      "Loss: 0.17653758499290398\n",
      "Loss: 0.17562488960924724\n",
      "Loss: 0.17472395631657625\n",
      "Loss: 0.1738345915590902\n",
      "Loss: 0.17295660535829802\n",
      "Loss: 0.17208981123695746\n",
      "Loss: 0.17123402614546085\n",
      "Loss: 0.17038907039053727\n",
      "Loss: 0.16955476756614243\n",
      "Loss: 0.16873094448642117\n",
      "Loss: 0.16791743112063323\n",
      "Loss: 0.16711406052993888\n",
      "Loss: 0.16632066880594945\n",
      "Loss: 0.16553709501095423\n",
      "Loss: 0.16476318111973795\n",
      "Loss: 0.16399877196291404\n",
      "Loss: 0.163243715171699\n",
      "Loss: 0.16249786112405995\n",
      "Loss: 0.161761062892174\n",
      "Loss: 0.16103317619113813\n",
      "Loss: 0.16031405932887646\n",
      "Loss: 0.159603573157192\n",
      "Loss: 0.15890194382305897\n",
      "Loss: 0.15820959737059245\n",
      "Loss: 0.15752699574858792\n",
      "Loss: 0.15685243792305428\n",
      "Loss: 0.1561857977325776\n",
      "Loss: 0.15552695128726773\n",
      "Loss: 0.1548757769246026\n",
      "Loss: 0.15423215516633154\n",
      "Loss: 0.15359596867639236\n",
      "Loss: 0.15296710221979998\n",
      "Loss: 0.1523454426224664\n",
      "Loss: 0.1517308787319171\n",
      "Loss: 0.15112330137886795\n",
      "Loss: 0.15052260333963216\n",
      "Loss: 0.149922709810573\n",
      "Loss: 0.14931036095561034\n",
      "Loss: 0.1487088222413269\n",
      "Loss: 0.1481141975859378\n",
      "Loss: 0.14752637134561933\n",
      "Loss: 0.1469452304223246\n",
      "Loss: 0.14637066419357603\n",
      "Loss: 0.14580256444451595\n",
      "Loss: 0.14524082530214416\n",
      "Loss: 0.14468534317167506\n",
      "Loss: 0.1441400564220524\n",
      "Loss: 0.14360390307816864\n",
      "Loss: 0.14307348278842896\n",
      "Loss: 0.1425487094563671\n",
      "Loss: 0.14202949852017813\n",
      "Loss: 0.1415168220184406\n",
      "Loss: 0.14101073851111623\n",
      "Loss: 0.1405099362220409\n",
      "Loss: 0.140014336832931\n",
      "Loss: 0.1395238634410339\n",
      "Loss: 0.13903844053213876\n",
      "Loss: 0.13855952643717415\n",
      "Loss: 0.13808675072134602\n",
      "Loss: 0.13761873013462553\n",
      "Loss: 0.1371553970789643\n",
      "Loss: 0.13669668512252015\n",
      "Loss: 0.13624252897803363\n",
      "Loss: 0.13579286448159675\n",
      "Loss: 0.13534762857180724\n",
      "Loss: 0.1349067592692982\n",
      "Loss: 0.13447019565664026\n",
      "Loss: 0.13403787785860577\n",
      "Loss: 0.13360974702279088\n",
      "Loss: 0.13318574530058863\n",
      "Loss: 0.13276581582850727\n",
      "Loss: 0.13234990270982755\n",
      "Loss: 0.1319379509965935\n",
      "Loss: 0.1315299066719327\n",
      "Loss: 0.13112571663269743\n",
      "Loss: 0.13072532867242556\n",
      "Loss: 0.13032869146461257\n",
      "Loss: 0.12993575454629178\n",
      "Loss: 0.1295464683019171\n",
      "Loss: 0.12916078394754427\n",
      "Loss: 0.12877865351530404\n",
      "Loss: 0.1284000298381648\n",
      "Loss: 0.12802486653497913\n",
      "Loss: 0.12765311799580892\n",
      "Loss: 0.1272847393675257\n",
      "Loss: 0.12691968653968191\n",
      "Loss: 0.12655791613064735\n",
      "Loss: 0.12619938547400752\n",
      "Loss: 0.12584405260522016\n",
      "Loss: 0.1254918762485236\n",
      "Loss: 0.12514281580409511\n",
      "Loss: 0.12479683133545379\n",
      "Loss: 0.12445388355710377\n",
      "Loss: 0.12411393382241497\n",
      "Loss: 0.12377694411173501\n",
      "Loss: 0.12344287702073124\n",
      "Loss: 0.12311169574895657\n",
      "Loss: 0.12278336408863577\n",
      "Loss: 0.12245784641367014\n",
      "Loss: 0.12213510766885338\n",
      "Loss: 0.12181511335929776\n",
      "Loss: 0.12149782954006666\n",
      "Loss: 0.12118322280600663\n",
      "Loss: 0.12087126028178073\n",
      "Loss: 0.12056190961209369\n",
      "Loss: 0.12025513895211003\n",
      "Loss: 0.11995091695805907\n",
      "Loss: 0.1196492127780239\n",
      "Loss: 0.11934999604291109\n",
      "Loss: 0.11905323685759842\n",
      "Loss: 0.11875890579225515\n",
      "Loss: 0.11846697387383436\n",
      "Loss: 0.11817741257773172\n",
      "Loss: 0.11789019381960938\n",
      "Loss: 0.11760528994737993\n",
      "Loss: 0.11732267373334954\n",
      "Loss: 0.1170423183665147\n",
      "Loss: 0.11676419744501242\n",
      "Loss: 0.11648828496871796\n",
      "Loss: 0.11621475872267562\n",
      "Loss: 0.1159432292959821\n",
      "Loss: 0.11567402354493585\n",
      "Loss: 0.11540676121585028\n",
      "Loss: 0.11514223710556631\n",
      "Loss: 0.11488040569901084\n",
      "Loss: 0.11462064239237531\n",
      "Loss: 0.11436277540740669\n",
      "Loss: 0.11410704251559976\n",
      "Loss: 0.11385313327966114\n",
      "Loss: 0.11360123381999887\n",
      "Loss: 0.11335123916890844\n",
      "Loss: 0.11310310572417166\n",
      "Loss: 0.11285691990919715\n",
      "Loss: 0.11261264213378812\n",
      "Loss: 0.11237031925368837\n",
      "Loss: 0.11212975332521385\n",
      "Loss: 0.1118909693274478\n",
      "Loss: 0.11165394790357382\n",
      "Loss: 0.11141876846296189\n",
      "Loss: 0.11118522449032985\n",
      "Loss: 0.1109533818043594\n",
      "Loss: 0.11072326354108318\n",
      "Loss: 0.11049486245318164\n",
      "Loss: 0.11026803207121866\n",
      "Loss: 0.11004285723700229\n",
      "Loss: 0.10981938944936386\n",
      "Loss: 0.10959741237011623\n",
      "Loss: 0.10937706459729946\n",
      "Loss: 0.10915833374670311\n",
      "Loss: 0.10894105792306502\n",
      "Loss: 0.10872542241672815\n",
      "Loss: 0.10851124423483124\n",
      "Loss: 0.1082985659330225\n",
      "Loss: 0.1080874504661334\n",
      "Loss: 0.10787768969453297\n",
      "Loss: 0.1076695445553882\n",
      "Loss: 0.10746268978742339\n",
      "Loss: 0.10725738607590864\n",
      "Loss: 0.10705340596374105\n",
      "Loss: 0.1068509015740615\n",
      "Loss: 0.10664972231944869\n",
      "Loss: 0.10644997538253831\n",
      "Loss: 0.10625152621696315\n",
      "Loss: 0.10605449513782579\n",
      "Loss: 0.10585870815299873\n",
      "Loss: 0.1056643516441001\n",
      "Loss: 0.10547117147084473\n",
      "Loss: 0.10527943140424594\n",
      "Loss: 0.10508884407747392\n",
      "Loss: 0.10489959924897455\n",
      "Loss: 0.10471159251702905\n",
      "Loss: 0.10452478442692309\n",
      "Loss: 0.10433931782953636\n",
      "Loss: 0.10415495221037131\n",
      "Loss: 0.10397208659219678\n",
      "Loss: 0.10379147413617164\n",
      "Loss: 0.10361205025259346\n",
      "Loss: 0.10343373034196952\n",
      "Loss: 0.10325665312570528\n",
      "Loss: 0.10308067459789629\n",
      "Loss: 0.1029057994205512\n",
      "Loss: 0.1027321634228795\n",
      "Loss: 0.10255951986688623\n",
      "Loss: 0.10238804775587129\n",
      "Loss: 0.10221766396986019\n",
      "Loss: 0.10204830909558128\n",
      "Loss: 0.10188014221542938\n",
      "Loss: 0.10171291333588633\n",
      "Loss: 0.1015468329997532\n",
      "Loss: 0.10138174191258632\n",
      "Loss: 0.10121769642955943\n",
      "Loss: 0.10105470729457942\n",
      "Loss: 0.10089266745554015\n",
      "Loss: 0.10073173384798974\n",
      "Loss: 0.1005716756871514\n",
      "Loss: 0.10041274113479573\n",
      "Loss: 0.10025465666728908\n",
      "Loss: 0.10009764852977605\n",
      "Loss: 0.09994151809269614\n",
      "Loss: 0.09978640839026262\n",
      "Loss: 0.09963219063627163\n",
      "Loss: 0.09947895162289894\n",
      "Loss: 0.09932660665525112\n",
      "Loss: 0.09917521083256034\n",
      "Loss: 0.09902470013649346\n",
      "Loss: 0.09887512026596515\n",
      "Loss: 0.09872640664426018\n",
      "Loss: 0.09857861575791882\n",
      "Loss: 0.09843166327030856\n",
      "Loss: 0.09828563468000406\n",
      "Loss: 0.0981404085861494\n",
      "Loss: 0.09799611589155403\n",
      "Loss: 0.09785258259732472\n",
      "Loss: 0.09770999969275651\n",
      "Loss: 0.09756813404914837\n",
      "Loss: 0.09742722102886169\n",
      "Loss: 0.09728701748170303\n",
      "Loss: 0.09714771111100737\n",
      "Loss: 0.09700915983475071\n",
      "Loss: 0.09687143057460479\n",
      "Loss: 0.09673450614848057\n",
      "Loss: 0.09659832537027627\n",
      "Loss: 0.09646300271215417\n",
      "Loss: 0.09632835663890724\n",
      "Loss: 0.0961945832834576\n",
      "Loss: 0.09606148428769062\n",
      "Loss: 0.09592918502846508\n",
      "Loss: 0.09579763279236148\n",
      "Loss: 0.09566678230827964\n",
      "Loss: 0.09553675256128144\n",
      "Loss: 0.09540735593338784\n",
      "Loss: 0.09527876542135963\n",
      "Loss: 0.09515084520240538\n",
      "Loss: 0.09502363694993467\n",
      "Loss: 0.09489718617764281\n",
      "Loss: 0.09477135864328534\n",
      "Loss: 0.09464631186038523\n",
      "Loss: 0.09452189428140817\n",
      "Loss: 0.0943981684571514\n",
      "Loss: 0.0942751685124854\n",
      "Loss: 0.09415276796824891\n",
      "Loss: 0.09403111264138572\n",
      "Loss: 0.09391007140114191\n",
      "Loss: 0.09378968039763079\n",
      "Loss: 0.09367000630215408\n",
      "Loss: 0.09355089823741224\n",
      "Loss: 0.09343249350662106\n",
      "Loss: 0.09331470559515348\n",
      "Loss: 0.09319751274106458\n",
      "Loss: 0.0930810428712267\n",
      "Loss: 0.09296509983495255\n",
      "Loss: 0.09284981509494845\n",
      "Loss: 0.09273516093992384\n",
      "Loss: 0.09262105948971303\n",
      "Loss: 0.0925076349380164\n",
      "Loss: 0.09239475875822038\n",
      "Loss: 0.09228246766064577\n",
      "Loss: 0.09217083608456902\n",
      "Loss: 0.0920597090488488\n",
      "Loss: 0.09194920901499491\n",
      "Loss: 0.09183928781601977\n",
      "Loss: 0.09172989258441415\n",
      "Loss: 0.09162114078975446\n",
      "Loss: 0.09151289391470062\n",
      "Loss: 0.09140520593567866\n",
      "Loss: 0.0912981334212031\n",
      "Loss: 0.091191537149527\n",
      "Loss: 0.09108553592558365\n",
      "Loss: 0.09098007317390533\n",
      "Loss: 0.09087510755380583\n",
      "Loss: 0.09077075459908453\n",
      "Loss: 0.09066686731459721\n",
      "Loss: 0.09056351600047984\n",
      "Loss: 0.09046073613941838\n",
      "Loss: 0.09035840762993569\n",
      "Loss: 0.09025664960956734\n",
      "Loss: 0.09015538375749031\n",
      "Loss: 0.09005459314297991\n",
      "Loss: 0.08995439326204263\n",
      "Loss: 0.08985466054592238\n",
      "Loss: 0.0897554284624534\n",
      "Loss: 0.08965669232727135\n",
      "Loss: 0.08955847191392259\n",
      "Loss: 0.08946073812519428\n",
      "Loss: 0.08936344937989743\n",
      "Loss: 0.08926666030835556\n",
      "Loss: 0.08917034946676718\n",
      "Loss: 0.0890745219977389\n",
      "Loss: 0.0889791824140247\n",
      "Loss: 0.08888427736910669\n",
      "Loss: 0.08878984449108718\n",
      "Loss: 0.08869587299147992\n",
      "Loss: 0.08860237766579053\n",
      "Loss: 0.08850933719951277\n",
      "Loss: 0.08841673072222314\n",
      "Loss: 0.08832479108311989\n",
      "Loss: 0.08823352398970055\n",
      "Loss: 0.0881426966421298\n",
      "Loss: 0.08805230596976252\n",
      "Loss: 0.08796234893179654\n",
      "Loss: 0.08787282251678764\n",
      "Loss: 0.08778372374218131\n",
      "Loss: 0.08769504965385938\n",
      "Loss: 0.08760679732570122\n",
      "Loss: 0.08751896385915942\n",
      "Loss: 0.08743154638284763\n",
      "Loss: 0.08734454205214216\n",
      "Loss: 0.08725794804879348\n",
      "Loss: 0.08717176158055084\n",
      "Loss: 0.08708597988079615\n",
      "Loss: 0.08700060020818844\n",
      "Loss: 0.08691561984631815\n",
      "Loss: 0.08683103610336987\n",
      "Loss: 0.0867468463117947\n",
      "Loss: 0.08666304782798956\n",
      "Loss: 0.08657963803198622\n",
      "Loss: 0.08649661432714607\n",
      "Loss: 0.08641397413986304\n",
      "Loss: 0.08633171491927344\n",
      "Loss: 0.08624983413697204\n",
      "Loss: 0.08616832928673458\n",
      "Loss: 0.08608719788424612\n",
      "Loss: 0.08600643746683558\n",
      "Loss: 0.08592604559321572\n",
      "Loss: 0.08584601984322777\n",
      "Loss: 0.08576635781759245\n",
      "Loss: 0.08568705713766485\n",
      "Loss: 0.08560811544519419\n",
      "Loss: 0.08552953040208847\n",
      "Loss: 0.08545129969018382\n",
      "Loss: 0.08537342101101679\n",
      "Loss: 0.0852958920856025\n",
      "Loss: 0.08521871065421514\n",
      "Loss: 0.08514187447617336\n",
      "Loss: 0.08506538132962854\n",
      "Loss: 0.0849892290113575\n",
      "Loss: 0.08491341533655816\n",
      "Loss: 0.08483793813864828\n",
      "Loss: 0.08476279526906805\n",
      "Loss: 0.08468798459708557\n",
      "Loss: 0.08461350400960507\n",
      "Loss: 0.08453935141097894\n",
      "Loss: 0.08446552472282166\n",
      "Loss: 0.0843920218838274\n",
      "Loss: 0.08431884084958963\n",
      "Loss: 0.08424597959242384\n",
      "Loss: 0.084173436101193\n",
      "Loss: 0.08410120838113491\n",
      "Loss: 0.08402929445369274\n",
      "Loss: 0.08395769235634755\n",
      "Loss: 0.08388640014245323\n",
      "Loss: 0.08381541588107363\n",
      "Loss: 0.0837447376568225\n",
      "Loss: 0.08367436356970491\n",
      "Loss: 0.083604291734961\n",
      "Loss: 0.0835345202829125\n",
      "Loss: 0.08346504735880973\n",
      "Loss: 0.0833958711226829\n",
      "Loss: 0.0833269897491932\n",
      "Loss: 0.08325840142748672\n",
      "Loss: 0.08319010436105098\n",
      "Loss: 0.08312209676757198\n",
      "Loss: 0.08305437687879416\n",
      "Loss: 0.08298694294038082\n",
      "Loss: 0.08291979321177852\n",
      "Loss: 0.08285299341096264\n",
      "Loss: 0.08278649645851775\n",
      "Loss: 0.08272028015464804\n",
      "Loss: 0.0826543427583219\n",
      "Loss: 0.08258868254294002\n",
      "Loss: 0.0825232977961766\n",
      "Loss: 0.08245818681982393\n",
      "Loss: 0.082393347929639\n",
      "Loss: 0.08232877945519228\n",
      "Loss: 0.08226447973971882\n",
      "Loss: 0.08220044713997184\n",
      "Loss: 0.08213668002607753\n",
      "Loss: 0.08207317678139332\n",
      "Loss: 0.08200993580236693\n",
      "Loss: 0.081946955498398\n",
      "Loss: 0.08188423429170173\n",
      "Loss: 0.08182177061717434\n",
      "Loss: 0.08175956292226012\n",
      "Loss: 0.08169760966682084\n",
      "Loss: 0.08163590932300649\n",
      "Loss: 0.08157446037512817\n",
      "Loss: 0.08151326131953238\n",
      "Loss: 0.08145231066447718\n",
      "Loss: 0.08139160693001027\n",
      "Loss: 0.08133114864784804\n",
      "Loss: 0.08127093436125672\n",
      "Loss: 0.08121096262493507\n",
      "Loss: 0.08115123200489881\n",
      "Loss: 0.08109174107836589\n",
      "Loss: 0.08103248843364377\n",
      "Loss: 0.0809734726700182\n",
      "Loss: 0.08091469239764333\n",
      "Loss: 0.08085614623743297\n",
      "Loss: 0.08079783282095375\n",
      "Loss: 0.08073975079031942\n",
      "Loss: 0.08068189879808599\n",
      "Loss: 0.08062427550714926\n",
      "Loss: 0.08056687959064261\n",
      "Loss: 0.08050970973183684\n",
      "Loss: 0.08045276462404045\n",
      "Loss: 0.08039604297050239\n",
      "Loss: 0.08033954348431438\n",
      "Loss: 0.08028326488831602\n",
      "Loss: 0.08022720591500018\n",
      "Loss: 0.08017136530641952\n",
      "Loss: 0.08011574181409453\n",
      "Loss: 0.0800603341989228\n",
      "Loss: 0.08000514123108879\n",
      "Loss: 0.07995016168997514\n",
      "Loss: 0.0798953943640748\n",
      "Loss: 0.0798408380509049\n",
      "Loss: 0.07978649993950548\n",
      "Loss: 0.07973237881599517\n",
      "Loss: 0.07967844956467092\n",
      "Loss: 0.0796247265779342\n",
      "Loss: 0.07957120869776843\n",
      "Loss: 0.07951789477467051\n",
      "Loss: 0.07946480388827486\n",
      "Loss: 0.07941190579569182\n",
      "Loss: 0.07935919782137633\n",
      "Loss: 0.07930668927635286\n",
      "Loss: 0.07925438139361489\n",
      "Loss: 0.0792023025653946\n",
      "Loss: 0.07915038636649872\n",
      "Loss: 0.07909866521030416\n",
      "Loss: 0.0790471433339578\n",
      "Loss: 0.0789958447089432\n",
      "Loss: 0.0789447027338358\n",
      "Loss: 0.07889375154766495\n",
      "Loss: 0.07884301732162546\n",
      "Loss: 0.07879246236489981\n",
      "Loss: 0.07874207760607319\n",
      "Loss: 0.0786918990993787\n",
      "Loss: 0.07864191568475751\n",
      "Loss: 0.07859208801130507\n",
      "Loss: 0.07854246774777972\n",
      "Loss: 0.07849303458743802\n",
      "Loss: 0.07844375488047516\n",
      "Loss: 0.078394694755462\n",
      "Loss: 0.078345791649346\n",
      "Loss: 0.07829705869759813\n",
      "Loss: 0.07824854630260591\n",
      "Loss: 0.07820016007412753\n",
      "Loss: 0.07815199252474997\n",
      "Loss: 0.07810397523605728\n",
      "Loss: 0.07805613817656834\n",
      "Loss: 0.07800848870704373\n",
      "Loss: 0.07796098450074038\n",
      "Loss: 0.07791369280921871\n",
      "Loss: 0.07786652357271028\n",
      "Loss: 0.07781958000006968\n",
      "Loss: 0.07777274761733131\n",
      "Loss: 0.07772614286650648\n",
      "Loss: 0.0776796490022562\n",
      "Loss: 0.07763337411949614\n",
      "Loss: 0.07758722023191095\n",
      "Loss: 0.07754126658919679\n",
      "Loss: 0.07749545394198774\n",
      "Loss: 0.07744981322053182\n",
      "Loss: 0.07740434289439797\n",
      "Loss: 0.0773590070691519\n",
      "Loss: 0.07731387997263518\n",
      "Loss: 0.07726886225765288\n",
      "Loss: 0.07722403823803829\n",
      "Loss: 0.07717936202830451\n",
      "Loss: 0.07713482410814652\n",
      "Loss: 0.07709048984497979\n",
      "Loss: 0.07704627060967832\n",
      "Loss: 0.0770022053709153\n",
      "Loss: 0.07695833744417192\n",
      "Loss: 0.07691457991295872\n",
      "Loss: 0.07687097782655246\n",
      "Loss: 0.07682758044669642\n",
      "Loss: 0.07678444082931798\n",
      "Loss: 0.07674145607353519\n",
      "Loss: 0.07669862237583905\n",
      "Loss: 0.0766559516929081\n",
      "Loss: 0.0766134274656742\n",
      "Loss: 0.07657103806600142\n",
      "Loss: 0.07652879647349624\n",
      "Loss: 0.07648670184594866\n",
      "Loss: 0.07644475335126846\n",
      "Loss: 0.07640295861224147\n",
      "Loss: 0.0763613063716738\n",
      "Loss: 0.07631978805600637\n",
      "Loss: 0.0762784127995573\n",
      "Loss: 0.07623717980706261\n",
      "Loss: 0.0761960882921404\n",
      "Loss: 0.07615513747701207\n",
      "Loss: 0.0761143265922387\n",
      "Loss: 0.07607365487647097\n",
      "Loss: 0.07603312157621304\n",
      "Loss: 0.07599272594559818\n",
      "Loss: 0.07595246724617608\n",
      "Loss: 0.07591234474671174\n",
      "Loss: 0.07587235772299399\n",
      "Loss: 0.07583250545765367\n",
      "Loss: 0.07579278723999146\n",
      "Loss: 0.0757532023658139\n",
      "Loss: 0.0757137501372771\n",
      "Loss: 0.07567442986273884\n",
      "Loss: 0.07563524085661698\n",
      "Loss: 0.07559618243925503\n",
      "Loss: 0.0755572539367939\n",
      "Loss: 0.07551845468104927\n",
      "Loss: 0.075479784009395\n",
      "Loss: 0.07544124126465188\n",
      "Loss: 0.07540282579498025\n",
      "Loss: 0.07536453695377866\n",
      "Loss: 0.07532637409958638\n",
      "Loss: 0.07528833659598927\n",
      "Loss: 0.07525042381153106\n",
      "Loss: 0.07521263511962668\n",
      "Loss: 0.0751749698984806\n",
      "Loss: 0.07513742753100693\n",
      "Loss: 0.07510000740475402\n",
      "Loss: 0.07506270891183084\n",
      "Loss: 0.07502553144883682\n",
      "Loss: 0.07498847441679375\n",
      "Loss: 0.07495153722108078\n",
      "Loss: 0.07491471927137089\n",
      "Loss: 0.07487801998157066\n",
      "Loss: 0.07484143876976089\n",
      "Loss: 0.07480497505814\n",
      "Loss: 0.07476862827296919\n",
      "Loss: 0.07473239784451906\n",
      "Loss: 0.07469628320701822\n",
      "Loss: 0.07466028379860301\n",
      "Loss: 0.07462439906126965\n",
      "Loss: 0.07458862844082648\n",
      "Loss: 0.07455297138684897\n",
      "Loss: 0.0745174273526347\n",
      "Loss: 0.07448199579516074\n",
      "Loss: 0.07444667617504129\n",
      "Loss: 0.07441146795648698\n",
      "Loss: 0.07437637060726478\n",
      "Loss: 0.07434138359865951\n",
      "Loss: 0.07430650640543555\n",
      "Loss: 0.07427173850580011\n",
      "Loss: 0.07423707938136725\n",
      "Loss: 0.07420252851712207\n",
      "Loss: 0.0741680854013869\n",
      "Loss: 0.07413374952578722\n",
      "Loss: 0.07409952038521841\n",
      "Loss: 0.0740653974778143\n",
      "Loss: 0.07403138030491445\n",
      "Loss: 0.07399746837103406\n",
      "Loss: 0.0739636611838329\n",
      "Loss: 0.07392995825408621\n",
      "Loss: 0.07389635909565487\n",
      "Loss: 0.07386286322545708\n",
      "Loss: 0.07382947016343987\n",
      "Loss: 0.07379617943255203\n",
      "Loss: 0.07376299055871625\n",
      "Loss: 0.07372990307080286\n",
      "Loss: 0.07369691650060338\n",
      "Loss: 0.07366403038280483\n",
      "Loss: 0.07363124425496434\n",
      "Loss: 0.07359855765748395\n",
      "Loss: 0.07356597013358623\n",
      "Loss: 0.07353333324883068\n",
      "Loss: 0.07347318127040002\n",
      "Loss: 0.07341380385833494\n",
      "Loss: 0.07335516827666734\n",
      "Loss: 0.07329724347853213\n",
      "Loss: 0.07324000002458317\n",
      "Loss: 0.07318341000441682\n",
      "Loss: 0.07312754175465551\n",
      "Loss: 0.07307366169378027\n",
      "Loss: 0.07302029778542218\n",
      "Loss: 0.07296743073228305\n",
      "Loss: 0.07291504225679234\n",
      "Loss: 0.0728631150463424\n",
      "Loss: 0.0728116327010905\n",
      "Loss: 0.07276057968425131\n",
      "Loss: 0.07270994127479755\n",
      "Loss: 0.07265970352248906\n",
      "Loss: 0.07260985320514805\n",
      "Loss: 0.07256037778809778\n",
      "Loss: 0.0725112653856821\n",
      "Loss: 0.0724625047247847\n",
      "Loss: 0.07241408511026494\n",
      "Loss: 0.07236599639223207\n",
      "Loss: 0.07231822893507646\n",
      "Loss: 0.07227077358818228\n",
      "Loss: 0.07222362165824356\n",
      "Loss: 0.07217676488311095\n",
      "Loss: 0.0721301954070957\n",
      "Loss: 0.07208390575766192\n",
      "Loss: 0.07203788882343766\n",
      "Loss: 0.07199213783347994\n",
      "Loss: 0.07194664633772975\n",
      "Loss: 0.0719014081885956\n",
      "Loss: 0.07185641752360725\n",
      "Loss: 0.07181166874908188\n",
      "Loss: 0.07176715652474977\n",
      "Loss: 0.07172287574928625\n",
      "Loss: 0.07167882154670052\n",
      "Loss: 0.07163498925353447\n",
      "Loss: 0.0715913744068246\n",
      "Loss: 0.07154797273278539\n",
      "Loss: 0.07150478013617241\n",
      "Loss: 0.07146179269028523\n",
      "Loss: 0.07141900662757433\n",
      "Loss: 0.07137650317391123\n",
      "Loss: 0.07133424907235793\n",
      "Loss: 0.07129217414630189\n",
      "Loss: 0.0712502758637103\n",
      "Loss: 0.0712085517692449\n",
      "Loss: 0.0711669994806371\n",
      "Loss: 0.07112561668527548\n",
      "Loss: 0.07108440113699127\n",
      "Loss: 0.07104335065303038\n",
      "Loss: 0.07100246311120133\n",
      "Loss: 0.07096173644718477\n",
      "Loss: 0.07092116865199916\n",
      "Loss: 0.07088075776960902\n",
      "Loss: 0.07084050189466883\n",
      "Loss: 0.07080039917039344\n",
      "Loss: 0.07076044778654747\n",
      "Loss: 0.07072090854407836\n",
      "Loss: 0.07068199136534967\n",
      "Loss: 0.0706432105373797\n",
      "Loss: 0.0706045647301747\n",
      "Loss: 0.07056605263617838\n",
      "Loss: 0.07052767296980668\n",
      "Loss: 0.07048942446699392\n",
      "Loss: 0.07045130588474796\n",
      "Loss: 0.07041331600071694\n",
      "Loss: 0.0703754536127649\n",
      "Loss: 0.07033771753855737\n",
      "Loss: 0.07030010661515618\n",
      "Loss: 0.07026261969862363\n",
      "Loss: 0.0702252556636354\n",
      "Loss: 0.07018801340310235\n",
      "Loss: 0.07015089182780054\n",
      "Loss: 0.07011388986600998\n",
      "Loss: 0.07007700646316117\n",
      "Loss: 0.07004024058148951\n",
      "Loss: 0.0700035911996979\n",
      "Loss: 0.069967057312626\n",
      "Loss: 0.06993063793092791\n",
      "Loss: 0.06989433208075603\n",
      "Loss: 0.06985813880345276\n",
      "Loss: 0.06982205715524843\n",
      "Loss: 0.06978608620696633\n",
      "Loss: 0.06975022504373396\n",
      "Loss: 0.06971447276470064\n",
      "Loss: 0.06967896981370283\n",
      "Loss: 0.06964377968761011\n",
      "Loss: 0.06960869390364878\n",
      "Loss: 0.06957371147160099\n",
      "Loss: 0.06953888917816715\n",
      "Loss: 0.06950420701494456\n",
      "Loss: 0.06946962424523104\n",
      "Loss: 0.06943514013147264\n",
      "Loss: 0.06940075394701276\n",
      "Loss: 0.06936646497584821\n",
      "Loss: 0.06933227251239414\n",
      "Loss: 0.06929817586125588\n",
      "Loss: 0.06926417433700845\n",
      "Loss: 0.06923026726398349\n",
      "Loss: 0.06919645397606168\n",
      "Loss: 0.06916273381647257\n",
      "Loss: 0.0691291061376\n",
      "Loss: 0.06909557030079318\n",
      "Loss: 0.06906212567618301\n",
      "Loss: 0.06902877164250483\n",
      "Loss: 0.06899550758692433\n",
      "Loss: 0.0689623329048702\n",
      "Loss: 0.06892924699986952\n",
      "Loss: 0.06889624928338944\n",
      "Loss: 0.06886333917468145\n",
      "Loss: 0.06883051610063104\n",
      "Loss: 0.06879777949561021\n",
      "Loss: 0.06876512880133426\n",
      "Loss: 0.0687325634667225\n",
      "Loss: 0.06870008294776189\n",
      "Loss: 0.06866768670737422\n",
      "Loss: 0.0686353742152867\n",
      "Loss: 0.06860314494790572\n",
      "Loss: 0.06857099838819307\n",
      "Loss: 0.0685389340255461\n",
      "Loss: 0.06850695135567954\n",
      "Loss: 0.06847504988051122\n",
      "Loss: 0.06844322910804942\n",
      "Loss: 0.06841148855228352\n",
      "Loss: 0.06837982773307665\n",
      "Loss: 0.0683482461760613\n",
      "Loss: 0.06831674341253632\n",
      "Loss: 0.06828531897936775\n",
      "Loss: 0.06825397241888988\n",
      "Loss: 0.06822270327881018\n",
      "Loss: 0.06819151111211545\n",
      "Loss: 0.06816039547698002\n",
      "Loss: 0.0681293559366763\n",
      "Loss: 0.06809839205948671\n",
      "Loss: 0.06806750341861811\n",
      "Loss: 0.0680366895921174\n",
      "Loss: 0.06800595016278936\n",
      "Loss: 0.06797528471811592\n",
      "Loss: 0.06794469285017753\n",
      "Loss: 0.06791417415557537\n",
      "Loss: 0.06788372823535592\n",
      "Loss: 0.06785335469493693\n",
      "Loss: 0.06782305314403447\n",
      "Loss: 0.06779282319659206\n",
      "Loss: 0.06776266447071068\n",
      "Loss: 0.06773257658858062\n",
      "Loss: 0.06770255917641457\n",
      "Loss: 0.06767261186438185\n",
      "Loss: 0.0676427342865441\n",
      "Loss: 0.06761292608079268\n",
      "Loss: 0.06758318688878627\n",
      "Loss: 0.06755351635589071\n",
      "Loss: 0.06752391413111988\n",
      "Loss: 0.06749437986707671\n",
      "Loss: 0.06746491321989706\n",
      "Loss: 0.06743551384919305\n",
      "Loss: 0.06740618141799884\n",
      "Loss: 0.06737691559271612\n",
      "Loss: 0.0673478707906178\n",
      "Loss: 0.06731907766143726\n",
      "Loss: 0.06729035022432406\n",
      "Loss: 0.06726168804302761\n",
      "Loss: 0.06723309069508118\n",
      "Loss: 0.06720455777087239\n",
      "Loss: 0.06717608887278635\n",
      "Loss: 0.06714768361441478\n",
      "Loss: 0.06711934161982636\n",
      "Loss: 0.06709106252289321\n",
      "Loss: 0.06706284596666912\n",
      "Loss: 0.06703469160281551\n",
      "Loss: 0.06700659909107083\n",
      "Loss: 0.06697856809876028\n",
      "Loss: 0.06695059830034376\n",
      "Loss: 0.06692268937699586\n",
      "Loss: 0.06689484101621994\n",
      "Loss: 0.06686705291148874\n",
      "Loss: 0.06683932476191386\n",
      "Loss: 0.0668116562719379\n",
      "Loss: 0.066784047151051\n",
      "Loss: 0.06675663797210254\n",
      "Loss: 0.06672939706299796\n",
      "Loss: 0.06670221786785716\n",
      "Loss: 0.06667509974954347\n",
      "Loss: 0.06664804210514486\n",
      "Loss: 0.06662104436331708\n",
      "Loss: 0.06659410598184431\n",
      "Loss: 0.06656722644539659\n",
      "Loss: 0.06654040526347098\n",
      "Loss: 0.06651364196849861\n",
      "Loss: 0.06648693611410485\n",
      "Loss: 0.06646028727351062\n",
      "Loss: 0.06643369503806162\n",
      "Loss: 0.06640715901587682\n",
      "Loss: 0.06638067883060447\n",
      "Loss: 0.06635425412027889\n",
      "Loss: 0.06632788453626791\n",
      "Loss: 0.06630156974230482\n",
      "Loss: 0.06627530941359716\n",
      "Loss: 0.06624910323600704\n",
      "Loss: 0.06622295090529541\n",
      "Loss: 0.06619685212642763\n",
      "Loss: 0.06617080661293281\n",
      "Loss: 0.06614481408631433\n",
      "Loss: 0.06611887427550689\n",
      "Loss: 0.06609298691637538\n",
      "Loss: 0.06606715175125434\n",
      "Loss: 0.06604136852852202\n",
      "Loss: 0.06601563700220901\n",
      "Loss: 0.0659899569316365\n",
      "Loss: 0.06596432808108273\n",
      "Loss: 0.0659387502194753\n",
      "Loss: 0.06591322312010771\n",
      "Loss: 0.06588774656037685\n",
      "Loss: 0.06586232032154102\n",
      "Loss: 0.06583694418849684\n",
      "Loss: 0.06581161794957265\n",
      "Loss: 0.06578634139633742\n",
      "Loss: 0.0657611143234248\n",
      "Loss: 0.06573593652836958\n",
      "Loss: 0.06571080781145731\n",
      "Loss: 0.06568572797558411\n",
      "Loss: 0.06566069682612767\n",
      "Loss: 0.06563571417082774\n",
      "Loss: 0.06561077981967448\n",
      "Loss: 0.06558589358480649\n",
      "Loss: 0.06556105528041438\n",
      "Loss: 0.06553626472265324\n",
      "Loss: 0.06551152172955979\n",
      "Loss: 0.0654868261209762\n",
      "Loss: 0.065462177718479\n",
      "Loss: 0.0654375763453132\n",
      "Loss: 0.0654130218263311\n",
      "Loss: 0.06538851398793415\n",
      "Loss: 0.06536405265802077\n",
      "Loss: 0.06533963766593572\n",
      "Loss: 0.06531526884242403\n",
      "Loss: 0.06529094601958764\n",
      "Loss: 0.06526666903084474\n",
      "Loss: 0.06524243771089192\n",
      "Loss: 0.0652182518956685\n",
      "Loss: 0.06519411142232343\n",
      "Loss: 0.06517001612918384\n",
      "Loss: 0.06514596585572562\n",
      "Loss: 0.06512196044254634\n",
      "Loss: 0.06509799973133851\n",
      "Loss: 0.06507408356486583\n",
      "Loss: 0.06505021178693962\n",
      "Loss: 0.06502638424239732\n",
      "Loss: 0.0650026007770818\n",
      "Loss: 0.0649788612378219\n",
      "Loss: 0.064955165472414\n",
      "Loss: 0.0649315133296045\n",
      "Loss: 0.06490790465907309\n",
      "Loss: 0.06488433931141722\n",
      "Loss: 0.06486081713813688\n",
      "Loss: 0.06483733799162024\n",
      "Loss: 0.06481390172513017\n",
      "Loss: 0.06479050819279092\n",
      "Loss: 0.06476715724957575\n",
      "Loss: 0.06474384875129528\n",
      "Loss: 0.06472058255458547\n",
      "Loss: 0.06469735851689691\n",
      "Loss: 0.06467417649648453\n",
      "Loss: 0.06465103635239669\n",
      "Loss: 0.06462793794446631\n",
      "Loss: 0.06460488113330064\n",
      "Loss: 0.06458186578027267\n",
      "Loss: 0.06455889174751207\n",
      "Loss: 0.06453595889789686\n",
      "Loss: 0.06451306709504478\n",
      "Loss: 0.06449021620330603\n",
      "Loss: 0.06446740608775478\n",
      "Loss: 0.0644446366141821\n",
      "Loss: 0.06442190764908858\n",
      "Loss: 0.06439921905967697\n",
      "Loss: 0.0643765707138457\n",
      "Loss: 0.0643539624801819\n",
      "Loss: 0.0643313942279548\n",
      "Loss: 0.06430886582710946\n",
      "Loss: 0.0642863771482603\n",
      "Loss: 0.06426392806268533\n",
      "Loss: 0.06424151844232012\n",
      "Loss: 0.06421914815975141\n",
      "Loss: 0.06419681708821233\n",
      "Loss: 0.06417452510157606\n",
      "Loss: 0.06415227207435056\n",
      "Loss: 0.06413005788167342\n",
      "Loss: 0.06410788239930623\n",
      "Loss: 0.06408574550362979\n",
      "Loss: 0.06406364707163853\n",
      "Loss: 0.06404158698093608\n",
      "Loss: 0.06401956510973023\n",
      "Loss: 0.06399758133682758\n",
      "Loss: 0.06397563554162959\n",
      "Loss: 0.06395372760412746\n",
      "Loss: 0.0639318574048976\n",
      "Loss: 0.06391002482509721\n",
      "Loss: 0.06388822974645983\n",
      "Loss: 0.06386647205129083\n",
      "Loss: 0.06384475162246332\n",
      "Loss: 0.06382306834341361\n",
      "Loss: 0.06380142209813758\n",
      "Loss: 0.06377981277118572\n",
      "Loss: 0.06375824024765983\n",
      "Loss: 0.06373670441320864\n",
      "Loss: 0.06371520515402382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x1e8462f4440>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Architecture: [input=30, hidden1=1,output=1]\n",
    "mlp_model1 = MLP(architecture=[30,1, 1], learning_rate=0.1, n_iterations=1000)\n",
    "mlp_model1.fit(X_train, y_train)  # Note: no bias column needed — bias handled internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "70ff07a6-ffef-4a82-bbe8-da8153b3997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
      " 1 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0\n",
      " 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1\n",
      " 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred_mlp1=mlp_model1.predict(X_test)\n",
    "print(y_pred_mlp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe6151d6-64a7-4b42-a027-78be386d3be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "  Accuracy: 0.9474\n",
      "  Precision: 0.9853\n",
      "  Recall: 0.9306\n",
      "  F1: 0.9571\n"
     ]
    }
   ],
   "source": [
    "lr_metrics = compute_metrics(y_test, y_pred_mlp1)\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "for metric, value in lr_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ccc568a2-a0a6-4883-915e-192cd0856040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5254213516959997\n",
      "Loss: 0.46831531627594813\n",
      "Loss: 0.42380539014136254\n",
      "Loss: 0.3880938392106437\n",
      "Loss: 0.3590873473347669\n",
      "Loss: 0.33531271107509997\n",
      "Loss: 0.3153154495660114\n",
      "Loss: 0.29808632464174595\n",
      "Loss: 0.28324750005506477\n",
      "Loss: 0.27040708558957893\n",
      "Loss: 0.259128999703834\n",
      "Loss: 0.24903670750206552\n",
      "Loss: 0.239914997775862\n",
      "Loss: 0.23172788278221282\n",
      "Loss: 0.22433197967916443\n",
      "Loss: 0.21758790683010323\n",
      "Loss: 0.21140057895875683\n",
      "Loss: 0.20571195610400791\n",
      "Loss: 0.20044694573239977\n",
      "Loss: 0.19548424896154026\n",
      "Loss: 0.19085242965966392\n",
      "Loss: 0.18650900531524006\n",
      "Loss: 0.1824274191839848\n",
      "Loss: 0.17855885725118542\n",
      "Loss: 0.17492784884356935\n",
      "Loss: 0.17150323391054498\n",
      "Loss: 0.1682780926460671\n",
      "Loss: 0.16523807397178503\n",
      "Loss: 0.16236559457909425\n",
      "Loss: 0.159645617410491\n",
      "Loss: 0.15706306189666946\n",
      "Loss: 0.15459589332835408\n",
      "Loss: 0.15224671723460315\n",
      "Loss: 0.14999539591600816\n",
      "Loss: 0.14784281965732038\n",
      "Loss: 0.1457864655559394\n",
      "Loss: 0.1438210967964964\n",
      "Loss: 0.1419439277595094\n",
      "Loss: 0.14014672002036554\n",
      "Loss: 0.1384226200797907\n",
      "Loss: 0.13677143373391604\n",
      "Loss: 0.1351838090146547\n",
      "Loss: 0.13365802960878329\n",
      "Loss: 0.13218681764790466\n",
      "Loss: 0.1307621534361947\n",
      "Loss: 0.1293837371093078\n",
      "Loss: 0.12805549534524402\n",
      "Loss: 0.12677738797571875\n",
      "Loss: 0.125541018783734\n",
      "Loss: 0.12434671445901173\n",
      "Loss: 0.12318915390903695\n",
      "Loss: 0.12206744477253349\n",
      "Loss: 0.12097931337950885\n",
      "Loss: 0.11992407305874724\n",
      "Loss: 0.11890174643993304\n",
      "Loss: 0.11790833655415929\n",
      "Loss: 0.11694383591962164\n",
      "Loss: 0.11599961487956577\n",
      "Loss: 0.11508312077031316\n",
      "Loss: 0.1141900868787234\n",
      "Loss: 0.11331983235601056\n",
      "Loss: 0.1124752325116688\n",
      "Loss: 0.11165209077586194\n",
      "Loss: 0.11085334306010375\n",
      "Loss: 0.11007259230475043\n",
      "Loss: 0.1093071647244519\n",
      "Loss: 0.1085523820380475\n",
      "Loss: 0.10781673159342402\n",
      "Loss: 0.10709936139965391\n",
      "Loss: 0.10639921624679452\n",
      "Loss: 0.10571696712631551\n",
      "Loss: 0.10505213199760738\n",
      "Loss: 0.10439931767928533\n",
      "Loss: 0.1037613431746659\n",
      "Loss: 0.1031374243796998\n",
      "Loss: 0.10252705902076163\n",
      "Loss: 0.10193045051791252\n",
      "Loss: 0.1013418795760083\n",
      "Loss: 0.10075128116322424\n",
      "Loss: 0.10017883317900173\n",
      "Loss: 0.09960672877504495\n",
      "Loss: 0.09904678558601852\n",
      "Loss: 0.09849762073433388\n",
      "Loss: 0.09795971465968263\n",
      "Loss: 0.09743368207445892\n",
      "Loss: 0.096916689237622\n",
      "Loss: 0.09640955639092463\n",
      "Loss: 0.09591388881632933\n",
      "Loss: 0.09542710371085504\n",
      "Loss: 0.0949488877513434\n",
      "Loss: 0.09448138293024738\n",
      "Loss: 0.09402116329174717\n",
      "Loss: 0.0935687739269422\n",
      "Loss: 0.09312604157577267\n",
      "Loss: 0.09268909436283693\n",
      "Loss: 0.09226038296325596\n",
      "Loss: 0.09183949849864596\n",
      "Loss: 0.09142433266244569\n",
      "Loss: 0.09101680491249843\n",
      "Loss: 0.09061377290204796\n",
      "Loss: 0.0902177583605064\n",
      "Loss: 0.08983690982485473\n",
      "Loss: 0.08945982442753316\n",
      "Loss: 0.08908898196438106\n",
      "Loss: 0.08872322481506256\n",
      "Loss: 0.08836310593615203\n",
      "Loss: 0.08800814080831308\n",
      "Loss: 0.08765672938108626\n",
      "Loss: 0.08731278055926246\n",
      "Loss: 0.08697195113446028\n",
      "Loss: 0.08663503664692433\n",
      "Loss: 0.08630373193238436\n",
      "Loss: 0.08597586184342795\n",
      "Loss: 0.08565333935013976\n",
      "Loss: 0.08533259895091193\n",
      "Loss: 0.08501526168349106\n",
      "Loss: 0.08470381798458727\n",
      "Loss: 0.08439593528418907\n",
      "Loss: 0.08409169391119606\n",
      "Loss: 0.08379348719124113\n",
      "Loss: 0.08349767166935106\n",
      "Loss: 0.08320560135548968\n",
      "Loss: 0.0829185942012577\n",
      "Loss: 0.08263414769207847\n",
      "Loss: 0.08235321857155489\n",
      "Loss: 0.08207235042452338\n",
      "Loss: 0.08179477068038235\n",
      "Loss: 0.08151965695404827\n",
      "Loss: 0.08124883101896645\n",
      "Loss: 0.08098155784412904\n",
      "Loss: 0.0807163971054039\n",
      "Loss: 0.08045284716645797\n",
      "Loss: 0.08018237804876713\n",
      "Loss: 0.07991572024956495\n",
      "Loss: 0.07965249935916079\n",
      "Loss: 0.07939205914141531\n",
      "Loss: 0.07913482359327362\n",
      "Loss: 0.07888046850068516\n",
      "Loss: 0.07862896009466552\n",
      "Loss: 0.07838090100346706\n",
      "Loss: 0.07813532398484177\n",
      "Loss: 0.0778923984886195\n",
      "Loss: 0.07765235640369407\n",
      "Loss: 0.07741482175898406\n",
      "Loss: 0.07717966906742263\n",
      "Loss: 0.07694736626555644\n",
      "Loss: 0.07671717994158342\n",
      "Loss: 0.07648966589990486\n",
      "Loss: 0.07626450778384272\n",
      "Loss: 0.07604182818305287\n",
      "Loss: 0.07582135325891765\n",
      "Loss: 0.07560295895249337\n",
      "Loss: 0.07538713220507447\n",
      "Loss: 0.07517329217518819\n",
      "Loss: 0.07496140939281754\n",
      "Loss: 0.07474807393734685\n",
      "Loss: 0.07453675239710636\n",
      "Loss: 0.07432689664483927\n",
      "Loss: 0.07411912595052819\n",
      "Loss: 0.07391374528454643\n",
      "Loss: 0.07371013711407462\n",
      "Loss: 0.07350880842994867\n",
      "Loss: 0.07330920083734624\n",
      "Loss: 0.07311181924288122\n",
      "Loss: 0.07291570980067393\n",
      "Loss: 0.07272137837585889\n",
      "Loss: 0.07252888496352859\n",
      "Loss: 0.07233818606211305\n",
      "Loss: 0.07214934000379368\n",
      "Loss: 0.07196184197159257\n",
      "Loss: 0.07177512525144626\n",
      "Loss: 0.0715902925377221\n",
      "Loss: 0.0714069749974529\n",
      "Loss: 0.07122574423267945\n",
      "Loss: 0.07104633706984514\n",
      "Loss: 0.07086839737872919\n",
      "Loss: 0.07069237456108741\n",
      "Loss: 0.07051750530824029\n",
      "Loss: 0.07034421691473713\n",
      "Loss: 0.07017265478689057\n",
      "Loss: 0.07000235406088713\n",
      "Loss: 0.06983361926605001\n",
      "Loss: 0.06966653349798205\n",
      "Loss: 0.06950112311240282\n",
      "Loss: 0.06933715077592538\n",
      "Loss: 0.06917446051933308\n",
      "Loss: 0.06901295144574367\n",
      "Loss: 0.06885325977184326\n",
      "Loss: 0.06869438796452887\n",
      "Loss: 0.06853697221844753\n",
      "Loss: 0.06838093703963868\n",
      "Loss: 0.06822611300664999\n",
      "Loss: 0.06807214033642382\n",
      "Loss: 0.06791964861681205\n",
      "Loss: 0.0677680377049254\n",
      "Loss: 0.067617608217406\n",
      "Loss: 0.06746847124192171\n",
      "Loss: 0.06732027043144326\n",
      "Loss: 0.0671731798684512\n",
      "Loss: 0.06702735757902796\n",
      "Loss: 0.06688269821983145\n",
      "Loss: 0.06673903957968579\n",
      "Loss: 0.0665944285050748\n",
      "Loss: 0.06645087650776943\n",
      "Loss: 0.06630828369805179\n",
      "Loss: 0.06616668252325193\n",
      "Loss: 0.06602623365734782\n",
      "Loss: 0.06588681809170766\n",
      "Loss: 0.06574837299993275\n",
      "Loss: 0.06561080646756462\n",
      "Loss: 0.0654742764272768\n",
      "Loss: 0.06533867979426332\n",
      "Loss: 0.06520401797214133\n",
      "Loss: 0.0650702828324309\n",
      "Loss: 0.06493747284735901\n",
      "Loss: 0.06480562637519638\n",
      "Loss: 0.06467462832454964\n",
      "Loss: 0.06454448484838471\n",
      "Loss: 0.06441515250243925\n",
      "Loss: 0.06428676725830122\n",
      "Loss: 0.06415920263823124\n",
      "Loss: 0.06403242628255224\n",
      "Loss: 0.06390627299648106\n",
      "Loss: 0.06378193870673386\n",
      "Loss: 0.06365914717501174\n",
      "Loss: 0.063537171294875\n",
      "Loss: 0.06341592274805534\n",
      "Loss: 0.0632954727548096\n",
      "Loss: 0.06317578477802774\n",
      "Loss: 0.06305691312530547\n",
      "Loss: 0.06293915922397855\n",
      "Loss: 0.06282213163762565\n",
      "Loss: 0.06270583396091084\n",
      "Loss: 0.06259023829687994\n",
      "Loss: 0.06247537414172626\n",
      "Loss: 0.062361097151734185\n",
      "Loss: 0.06224760653732148\n",
      "Loss: 0.0621346832426043\n",
      "Loss: 0.06202254579813284\n",
      "Loss: 0.06191104113126778\n",
      "Loss: 0.06180009345466555\n",
      "Loss: 0.06168970731942526\n",
      "Loss: 0.06157990370123188\n",
      "Loss: 0.061470739258207584\n",
      "Loss: 0.06136213425686451\n",
      "Loss: 0.061254239487665285\n",
      "Loss: 0.06114691893009364\n",
      "Loss: 0.061040384439583484\n",
      "Loss: 0.06093442897024316\n",
      "Loss: 0.06082907311477525\n",
      "Loss: 0.06072432509633305\n",
      "Loss: 0.06062020452788093\n",
      "Loss: 0.06051674865649142\n",
      "Loss: 0.06041385104858697\n",
      "Loss: 0.060311493915942066\n",
      "Loss: 0.06020969684285186\n",
      "Loss: 0.06010843746243727\n",
      "Loss: 0.06000766862791995\n",
      "Loss: 0.059907492589892135\n",
      "Loss: 0.059807810321127156\n",
      "Loss: 0.05970864336651882\n",
      "Loss: 0.059610012588248225\n",
      "Loss: 0.05951201202764313\n",
      "Loss: 0.05941452953975763\n",
      "Loss: 0.059317483461494104\n",
      "Loss: 0.05922091584441174\n",
      "Loss: 0.059124911701205055\n",
      "Loss: 0.059029409868121406\n",
      "Loss: 0.05893431229827739\n",
      "Loss: 0.05883982012284644\n",
      "Loss: 0.058745827483053265\n",
      "Loss: 0.058652230489302896\n",
      "Loss: 0.058559148912302864\n",
      "Loss: 0.05846636022264587\n",
      "Loss: 0.05837371731335284\n",
      "Loss: 0.058280118055300985\n",
      "Loss: 0.05818746170876134\n",
      "Loss: 0.05809561046621346\n",
      "Loss: 0.058004283385389224\n",
      "Loss: 0.057913356711026265\n",
      "Loss: 0.057822951944739284\n",
      "Loss: 0.057732970415340396\n",
      "Loss: 0.057643334310560325\n",
      "Loss: 0.05755427516724795\n",
      "Loss: 0.05746604283978603\n",
      "Loss: 0.057379002144265896\n",
      "Loss: 0.057293060951672074\n",
      "Loss: 0.05720792940708531\n",
      "Loss: 0.05712298093850204\n",
      "Loss: 0.05703866748184322\n",
      "Loss: 0.0569546664230969\n",
      "Loss: 0.056871720680297874\n",
      "Loss: 0.056788898147385795\n",
      "Loss: 0.05670672828970346\n",
      "Loss: 0.05662503853284304\n",
      "Loss: 0.056543987273261216\n",
      "Loss: 0.05646316659117227\n",
      "Loss: 0.056383079205064655\n",
      "Loss: 0.05630308404292314\n",
      "Loss: 0.05622409277714516\n",
      "Loss: 0.056144804198772526\n",
      "Loss: 0.05606618411347279\n",
      "Loss: 0.05598770157340232\n",
      "Loss: 0.05590965910512863\n",
      "Loss: 0.055831920901684054\n",
      "Loss: 0.0557545433403692\n",
      "Loss: 0.05567731046421455\n",
      "Loss: 0.05560077252539892\n",
      "Loss: 0.05552412997648391\n",
      "Loss: 0.05544817126517503\n",
      "Loss: 0.05537231363896906\n",
      "Loss: 0.05529684304528825\n",
      "Loss: 0.055221670431724205\n",
      "Loss: 0.055146883847011816\n",
      "Loss: 0.055072296280406295\n",
      "Loss: 0.054998360595573996\n",
      "Loss: 0.05492423618616551\n",
      "Loss: 0.05485089891654517\n",
      "Loss: 0.054777448796377186\n",
      "Loss: 0.05470447317111915\n",
      "Loss: 0.054631714039030825\n",
      "Loss: 0.05455935692728099\n",
      "Loss: 0.05448696051934989\n",
      "Loss: 0.05441535213280142\n",
      "Loss: 0.05434359520536059\n",
      "Loss: 0.05427243718591597\n",
      "Loss: 0.054201281969512814\n",
      "Loss: 0.05413047083834256\n",
      "Loss: 0.054060051764954695\n",
      "Loss: 0.053989780590018265\n",
      "Loss: 0.05391978476959513\n",
      "Loss: 0.05385021569191694\n",
      "Loss: 0.053780615109126614\n",
      "Loss: 0.05371144050638276\n",
      "Loss: 0.05364254574454579\n",
      "Loss: 0.053573792581504694\n",
      "Loss: 0.0535055067660641\n",
      "Loss: 0.05343721066404754\n",
      "Loss: 0.053369234664872596\n",
      "Loss: 0.05330165762534572\n",
      "Loss: 0.0532343204769506\n",
      "Loss: 0.05316710219752485\n",
      "Loss: 0.053100770511428544\n",
      "Loss: 0.05303609453249995\n",
      "Loss: 0.05297212095243938\n",
      "Loss: 0.05290780622112092\n",
      "Loss: 0.052843818641711574\n",
      "Loss: 0.052779966490295226\n",
      "Loss: 0.05271676201274534\n",
      "Loss: 0.05265351008320614\n",
      "Loss: 0.05259053585416884\n",
      "Loss: 0.052527551154748026\n",
      "Loss: 0.05246463304087402\n",
      "Loss: 0.05240270057168217\n",
      "Loss: 0.05234036134209472\n",
      "Loss: 0.052278424289509365\n",
      "Loss: 0.05221639328725626\n",
      "Loss: 0.052154689872001614\n",
      "Loss: 0.052093689149975704\n",
      "Loss: 0.05203246274908188\n",
      "Loss: 0.051971257322680355\n",
      "Loss: 0.05191043307609597\n",
      "Loss: 0.05184966712291042\n",
      "Loss: 0.051789619061976114\n",
      "Loss: 0.05172947057570571\n",
      "Loss: 0.05166913550476163\n",
      "Loss: 0.05160942795742891\n",
      "Loss: 0.05154988034517196\n",
      "Loss: 0.051490361950416515\n",
      "Loss: 0.05142940211389\n",
      "Loss: 0.05136831340296791\n",
      "Loss: 0.05130762914140094\n",
      "Loss: 0.05124756947847164\n",
      "Loss: 0.05118752629732229\n",
      "Loss: 0.05112738981736786\n",
      "Loss: 0.0510673918956085\n",
      "Loss: 0.051007784654230864\n",
      "Loss: 0.050948798967821764\n",
      "Loss: 0.050889545384492765\n",
      "Loss: 0.05083021743963754\n",
      "Loss: 0.05077119591053866\n",
      "Loss: 0.050712758154402064\n",
      "Loss: 0.05065435560717569\n",
      "Loss: 0.05059590813936861\n",
      "Loss: 0.050537402472820875\n",
      "Loss: 0.050479682260511444\n",
      "Loss: 0.05042205029152439\n",
      "Loss: 0.050364244304994296\n",
      "Loss: 0.050306677404316984\n",
      "Loss: 0.050249154900553386\n",
      "Loss: 0.05019266080357042\n",
      "Loss: 0.05013551063544218\n",
      "Loss: 0.05007836733065961\n",
      "Loss: 0.05002125665126644\n",
      "Loss: 0.04996514515823473\n",
      "Loss: 0.04990891806618472\n",
      "Loss: 0.04985247147222251\n",
      "Loss: 0.04979631669816477\n",
      "Loss: 0.049740819009900836\n",
      "Loss: 0.04968554624062269\n",
      "Loss: 0.0496296954784122\n",
      "Loss: 0.04957447581513744\n",
      "Loss: 0.04951933137013683\n",
      "Loss: 0.049464856641878836\n",
      "Loss: 0.04940986902412309\n",
      "Loss: 0.04935493503507203\n",
      "Loss: 0.04930063336581139\n",
      "Loss: 0.049246736437443844\n",
      "Loss: 0.049192335445914885\n",
      "Loss: 0.049138238576035505\n",
      "Loss: 0.04908428175632622\n",
      "Loss: 0.04903084887338812\n",
      "Loss: 0.04897740954622483\n",
      "Loss: 0.048923468540997224\n",
      "Loss: 0.048870153673606184\n",
      "Loss: 0.04881772642095357\n",
      "Loss: 0.04876442845026163\n",
      "Loss: 0.048711375279563374\n",
      "Loss: 0.04865872137005106\n",
      "Loss: 0.04860640369787761\n",
      "Loss: 0.04855420398226876\n",
      "Loss: 0.048501626032229884\n",
      "Loss: 0.04844960436411981\n",
      "Loss: 0.04839855749398461\n",
      "Loss: 0.04834652326676205\n",
      "Loss: 0.04829496783436516\n",
      "Loss: 0.04824352217062207\n",
      "Loss: 0.04819258754108701\n",
      "Loss: 0.04814168362507238\n",
      "Loss: 0.048090444277556876\n",
      "Loss: 0.048039238371683446\n",
      "Loss: 0.047989520162629536\n",
      "Loss: 0.04793852954507732\n",
      "Loss: 0.04788813529516701\n",
      "Loss: 0.04783756366896622\n",
      "Loss: 0.047787871292138684\n",
      "Loss: 0.0477380684249734\n",
      "Loss: 0.04768784083200152\n",
      "Loss: 0.04763758520348391\n",
      "Loss: 0.04758921366846535\n",
      "Loss: 0.047538978671779364\n",
      "Loss: 0.04748969467442733\n",
      "Loss: 0.047440319840899095\n",
      "Loss: 0.04739159692685023\n",
      "Loss: 0.04734267122340602\n",
      "Loss: 0.047293541269112525\n",
      "Loss: 0.0472447092199805\n",
      "Loss: 0.0471968808440232\n",
      "Loss: 0.04714781723913668\n",
      "Loss: 0.047099654915192773\n",
      "Loss: 0.04705143059618057\n",
      "Loss: 0.047003615414288276\n",
      "Loss: 0.046955457862426865\n",
      "Loss: 0.046907421104199126\n",
      "Loss: 0.04686005579360792\n",
      "Loss: 0.04681228982916662\n",
      "Loss: 0.04676407716453802\n",
      "Loss: 0.04671710006979536\n",
      "Loss: 0.04666949706203721\n",
      "Loss: 0.0466222170407025\n",
      "Loss: 0.04657472369119605\n",
      "Loss: 0.04652823521087281\n",
      "Loss: 0.04648093382053872\n",
      "Loss: 0.04643374184077922\n",
      "Loss: 0.046388060156681156\n",
      "Loss: 0.04634114806748354\n",
      "Loss: 0.04629421018946978\n",
      "Loss: 0.046248890390600136\n",
      "Loss: 0.04620222844659451\n",
      "Loss: 0.04615620844953298\n",
      "Loss: 0.0461102866523188\n",
      "Loss: 0.046064656389223774\n",
      "Loss: 0.0460188666997666\n",
      "Loss: 0.04597288174143186\n",
      "Loss: 0.045928141483279616\n",
      "Loss: 0.04588241842326144\n",
      "Loss: 0.0458367396014895\n",
      "Loss: 0.04579235828120477\n",
      "Loss: 0.04574688762847293\n",
      "Loss: 0.045701921245036414\n",
      "Loss: 0.045657802428630836\n",
      "Loss: 0.045614622820613356\n",
      "Loss: 0.045571155710280864\n",
      "Loss: 0.04552749798520573\n",
      "Loss: 0.0454854400699285\n",
      "Loss: 0.04544175354488798\n",
      "Loss: 0.04539850876542132\n",
      "Loss: 0.04535666193414232\n",
      "Loss: 0.045313388028728335\n",
      "Loss: 0.045270516904937066\n",
      "Loss: 0.04522853062153495\n",
      "Loss: 0.04518600048271899\n",
      "Loss: 0.04514337724482868\n",
      "Loss: 0.045101298277821916\n",
      "Loss: 0.04505968052906591\n",
      "Loss: 0.045016863555616814\n",
      "Loss: 0.04497544781578232\n",
      "Loss: 0.04493367330698219\n",
      "Loss: 0.04489171391729733\n",
      "Loss: 0.04484981987205783\n",
      "Loss: 0.04480874119405141\n",
      "Loss: 0.04476717829460556\n",
      "Loss: 0.044725335798429765\n",
      "Loss: 0.044684703668811346\n",
      "Loss: 0.04464320216347272\n",
      "Loss: 0.044601850642408666\n",
      "Loss: 0.04456116430688564\n",
      "Loss: 0.04452008399931686\n",
      "Loss: 0.044479217775803805\n",
      "Loss: 0.04443829310176603\n",
      "Loss: 0.04439776931868889\n",
      "Loss: 0.04435715398304702\n",
      "Loss: 0.044315756242166854\n",
      "Loss: 0.04427647125265676\n",
      "Loss: 0.044235355231703216\n",
      "Loss: 0.04419475008662999\n",
      "Loss: 0.04415506698909895\n",
      "Loss: 0.0441144976670851\n",
      "Loss: 0.044074414174542144\n",
      "Loss: 0.044034295981542675\n",
      "Loss: 0.04399485014674249\n",
      "Loss: 0.04395451218688676\n",
      "Loss: 0.04391481700744484\n",
      "Loss: 0.04387562772395911\n",
      "Loss: 0.043835600952363836\n",
      "Loss: 0.04379600277423237\n",
      "Loss: 0.04375717548149457\n",
      "Loss: 0.043717565142349694\n",
      "Loss: 0.043678083343414414\n",
      "Loss: 0.04363921523823478\n",
      "Loss: 0.043600437706356215\n",
      "Loss: 0.04356074964311628\n",
      "Loss: 0.04352215065855357\n",
      "Loss: 0.043483594683781296\n",
      "Loss: 0.04344400491232678\n",
      "Loss: 0.0434059044018066\n",
      "Loss: 0.043367058046925214\n",
      "Loss: 0.04332848389010598\n",
      "Loss: 0.043289655938859826\n",
      "Loss: 0.04325162751357215\n",
      "Loss: 0.043213113125183895\n",
      "Loss: 0.04317410461062561\n",
      "Loss: 0.04313700916179507\n",
      "Loss: 0.04309810081759582\n",
      "Loss: 0.04305979172139421\n",
      "Loss: 0.04302227718957028\n",
      "Loss: 0.04298404111135641\n",
      "Loss: 0.042945880258096425\n",
      "Loss: 0.04290817357104585\n",
      "Loss: 0.042870686994225575\n",
      "Loss: 0.042832324497785186\n",
      "Loss: 0.0427949447868972\n",
      "Loss: 0.04275757303715573\n",
      "Loss: 0.042719369664599584\n",
      "Loss: 0.04268228471374016\n",
      "Loss: 0.04264492719779964\n",
      "Loss: 0.042607259236184004\n",
      "Loss: 0.042569890532490204\n",
      "Loss: 0.042532870290966765\n",
      "Loss: 0.042495646734797964\n",
      "Loss: 0.04245779542231032\n",
      "Loss: 0.04242179572302923\n",
      "Loss: 0.042384121700278866\n",
      "Loss: 0.04234691293200177\n",
      "Loss: 0.042310742647230865\n",
      "Loss: 0.04227339942860215\n",
      "Loss: 0.04223654046061665\n",
      "Loss: 0.04220007113806824\n",
      "Loss: 0.04216378078407108\n",
      "Loss: 0.04212634685238787\n",
      "Loss: 0.04209073161827297\n",
      "Loss: 0.042053891064711184\n",
      "Loss: 0.04201717756737826\n",
      "Loss: 0.041981426218508845\n",
      "Loss: 0.04194477064884754\n",
      "Loss: 0.04190862752720508\n",
      "Loss: 0.04187254582987216\n",
      "Loss: 0.04183657747727237\n",
      "Loss: 0.04180026408546548\n",
      "Loss: 0.04176466446147162\n",
      "Loss: 0.041728988090029534\n",
      "Loss: 0.04169262087660402\n",
      "Loss: 0.0416575260579943\n",
      "Loss: 0.04162178642453637\n",
      "Loss: 0.04158584528964266\n",
      "Loss: 0.041550924753940705\n",
      "Loss: 0.0415155560003315\n",
      "Loss: 0.04148023672774121\n",
      "Loss: 0.04144500121625168\n",
      "Loss: 0.041410605716186835\n",
      "Loss: 0.04137472120910917\n",
      "Loss: 0.041340226666202344\n",
      "Loss: 0.041305495173885175\n",
      "Loss: 0.041270055769805844\n",
      "Loss: 0.04123567084348722\n",
      "Loss: 0.041200914715333624\n",
      "Loss: 0.041166281233393186\n",
      "Loss: 0.041131285129582154\n",
      "Loss: 0.041097644924902824\n",
      "Loss: 0.041062386548262546\n",
      "Loss: 0.04102831587280495\n",
      "Loss: 0.04099442919321269\n",
      "Loss: 0.04095953190114949\n",
      "Loss: 0.04092575078095959\n",
      "Loss: 0.0408915579511268\n",
      "Loss: 0.04085748424699727\n",
      "Loss: 0.040823223253938785\n",
      "Loss: 0.040789739637810886\n",
      "Loss: 0.0407552153557601\n",
      "Loss: 0.040721578843909687\n",
      "Loss: 0.04068788682084024\n",
      "Loss: 0.04065368570143489\n",
      "Loss: 0.040620436016426266\n",
      "Loss: 0.04058661368758504\n",
      "Loss: 0.04055292137218963\n",
      "Loss: 0.040519531105076306\n",
      "Loss: 0.04048608227815631\n",
      "Loss: 0.04045225453342153\n",
      "Loss: 0.04041914698298825\n",
      "Loss: 0.04038590710109581\n",
      "Loss: 0.0403518354459958\n",
      "Loss: 0.04031949645784938\n",
      "Loss: 0.04028590511787606\n",
      "Loss: 0.040252259639875956\n",
      "Loss: 0.04021991441193149\n",
      "Loss: 0.04018642123810236\n",
      "Loss: 0.04015298662302886\n",
      "Loss: 0.04012073060448446\n",
      "Loss: 0.04008791279401267\n",
      "Loss: 0.04005400978386309\n",
      "Loss: 0.04002255055581265\n",
      "Loss: 0.03998893866726327\n",
      "Loss: 0.03995590797446598\n",
      "Loss: 0.03992391126503891\n",
      "Loss: 0.03989063570660906\n",
      "Loss: 0.039858058552529094\n",
      "Loss: 0.03982549276472912\n",
      "Loss: 0.03979346791040768\n",
      "Loss: 0.03976032436026357\n",
      "Loss: 0.03972869208634953\n",
      "Loss: 0.03969548807197933\n",
      "Loss: 0.03966348470266546\n",
      "Loss: 0.03963109273536535\n",
      "Loss: 0.03959839144317207\n",
      "Loss: 0.03956664625824285\n",
      "Loss: 0.03953404690967415\n",
      "Loss: 0.03950218054652992\n",
      "Loss: 0.039470160336428924\n",
      "Loss: 0.03943810960990167\n",
      "Loss: 0.0394051838034828\n",
      "Loss: 0.039374294423438094\n",
      "Loss: 0.03934154046194245\n",
      "Loss: 0.039309341060293886\n",
      "Loss: 0.03927823466337616\n",
      "Loss: 0.03924595932621006\n",
      "Loss: 0.03921428713894323\n",
      "Loss: 0.03918270915264953\n",
      "Loss: 0.03915060824382516\n",
      "Loss: 0.03911888298711533\n",
      "Loss: 0.03908750894834943\n",
      "Loss: 0.039055253062999895\n",
      "Loss: 0.039024155980227675\n",
      "Loss: 0.038992951157398544\n",
      "Loss: 0.03896047205308348\n",
      "Loss: 0.03893004754890658\n",
      "Loss: 0.03889789143337191\n",
      "Loss: 0.03886647511652671\n",
      "Loss: 0.038835362329486436\n",
      "Loss: 0.038803953976245135\n",
      "Loss: 0.03877275062114307\n",
      "Loss: 0.03874173900093967\n",
      "Loss: 0.03871023130753934\n",
      "Loss: 0.038678956446508664\n",
      "Loss: 0.03864825698179535\n",
      "Loss: 0.03861606587036759\n",
      "Loss: 0.03858641047665176\n",
      "Loss: 0.038554688777182614\n",
      "Loss: 0.038523774111873944\n",
      "Loss: 0.03849317221871306\n",
      "Loss: 0.03846192766684622\n",
      "Loss: 0.03843105225066247\n",
      "Loss: 0.03840106282537001\n",
      "Loss: 0.038370385728293606\n",
      "Loss: 0.03834000467734118\n",
      "Loss: 0.038309960295454484\n",
      "Loss: 0.03827824353965007\n",
      "Loss: 0.03824899946123567\n",
      "Loss: 0.038217761868327135\n",
      "Loss: 0.03818725443431753\n",
      "Loss: 0.03815746908010501\n",
      "Loss: 0.03812681180730473\n",
      "Loss: 0.03809681390345111\n",
      "Loss: 0.038066298817025515\n",
      "Loss: 0.03803584869069932\n",
      "Loss: 0.03800612057769362\n",
      "Loss: 0.03797594476619374\n",
      "Loss: 0.037945844116802005\n",
      "Loss: 0.037916729145768956\n",
      "Loss: 0.037885620307056826\n",
      "Loss: 0.03785653556687557\n",
      "Loss: 0.0378263157134128\n",
      "Loss: 0.037796010319493076\n",
      "Loss: 0.03776744313602325\n",
      "Loss: 0.03773676058202779\n",
      "Loss: 0.03770664266580975\n",
      "Loss: 0.0376767229111969\n",
      "Loss: 0.03764658179183071\n",
      "Loss: 0.03761641223602553\n",
      "Loss: 0.03758694023934682\n",
      "Loss: 0.03755646999539494\n",
      "Loss: 0.03752759568350262\n",
      "Loss: 0.03749708056619227\n",
      "Loss: 0.037467309526821216\n",
      "Loss: 0.03743815705475457\n",
      "Loss: 0.037407506178656035\n",
      "Loss: 0.037378481450923234\n",
      "Loss: 0.03734877125822171\n",
      "Loss: 0.037319193581366476\n",
      "Loss: 0.037289914833828756\n",
      "Loss: 0.03726035812504165\n",
      "Loss: 0.0372300618966208\n",
      "Loss: 0.037201564402403096\n",
      "Loss: 0.03717155263003496\n",
      "Loss: 0.03714247258153182\n",
      "Loss: 0.037113659208593205\n",
      "Loss: 0.03708350474619518\n",
      "Loss: 0.03705524664579249\n",
      "Loss: 0.03702547520184101\n",
      "Loss: 0.036996579584523404\n",
      "Loss: 0.036967919483906604\n",
      "Loss: 0.03693876028661361\n",
      "Loss: 0.03690977102964573\n",
      "Loss: 0.03688149845820552\n",
      "Loss: 0.03685192982864816\n",
      "Loss: 0.03682343621897521\n",
      "Loss: 0.036794488763802556\n",
      "Loss: 0.036763786363439306\n",
      "Loss: 0.03673558545483744\n",
      "Loss: 0.036705171292810194\n",
      "Loss: 0.03667600206103078\n",
      "Loss: 0.03664639211104726\n",
      "Loss: 0.0366164403259102\n",
      "Loss: 0.036587361295478384\n",
      "Loss: 0.03655782404124614\n",
      "Loss: 0.03652778640118581\n",
      "Loss: 0.03649905148095992\n",
      "Loss: 0.03646945879529214\n",
      "Loss: 0.03643984782576085\n",
      "Loss: 0.03641124933892303\n",
      "Loss: 0.03638068561156875\n",
      "Loss: 0.0363525267572867\n",
      "Loss: 0.03632253107209669\n",
      "Loss: 0.036293349128650974\n",
      "Loss: 0.03626458059319403\n",
      "Loss: 0.03623475231037507\n",
      "Loss: 0.03620633147430662\n",
      "Loss: 0.036176969988004765\n",
      "Loss: 0.0361476470140618\n",
      "Loss: 0.036119077164124214\n",
      "Loss: 0.03608994853624386\n",
      "Loss: 0.03606053878905948\n",
      "Loss: 0.03603233658632722\n",
      "Loss: 0.03600243658230316\n",
      "Loss: 0.035974593465117055\n",
      "Loss: 0.035945201314383536\n",
      "Loss: 0.035916311743519135\n",
      "Loss: 0.03588816087680604\n",
      "Loss: 0.035858529098227995\n",
      "Loss: 0.03583067666505106\n",
      "Loss: 0.03580117442662499\n",
      "Loss: 0.03577276775251801\n",
      "Loss: 0.03574451475724582\n",
      "Loss: 0.03571575451463403\n",
      "Loss: 0.035687316386337556\n",
      "Loss: 0.03565896292779183\n",
      "Loss: 0.03562999638435308\n",
      "Loss: 0.03560248826259718\n",
      "Loss: 0.03557348680563677\n",
      "Loss: 0.03554532911098246\n",
      "Loss: 0.03551737189531159\n",
      "Loss: 0.035488110004766905\n",
      "Loss: 0.03546132670264769\n",
      "Loss: 0.035431866022666646\n",
      "Loss: 0.035404347191077455\n",
      "Loss: 0.035375901511183604\n",
      "Loss: 0.03534746055239866\n",
      "Loss: 0.03532010489299949\n",
      "Loss: 0.03529149512945295\n",
      "Loss: 0.03526345167787282\n",
      "Loss: 0.0352356501301187\n",
      "Loss: 0.03520729799264361\n",
      "Loss: 0.035179791027263985\n",
      "Loss: 0.035151813268170864\n",
      "Loss: 0.035123197594233345\n",
      "Loss: 0.035096586169388114\n",
      "Loss: 0.03506762427363512\n",
      "Loss: 0.03504083153162211\n",
      "Loss: 0.03501252370600465\n",
      "Loss: 0.03498470040726129\n",
      "Loss: 0.034957792591096215\n",
      "Loss: 0.034929223011616654\n",
      "Loss: 0.034902428392225564\n",
      "Loss: 0.03487428557824336\n",
      "Loss: 0.034846985724905515\n",
      "Loss: 0.034819660560288214\n",
      "Loss: 0.03479184347683209\n",
      "Loss: 0.03476448876134461\n",
      "Loss: 0.034737126227913806\n",
      "Loss: 0.03470924873354909\n",
      "Loss: 0.03468279752819427\n",
      "Loss: 0.03465516579774677\n",
      "Loss: 0.03462802097660955\n",
      "Loss: 0.03460145623091242\n",
      "Loss: 0.03457350851064772\n",
      "Loss: 0.03454815714858609\n",
      "Loss: 0.03452005513068235\n",
      "Loss: 0.034493766600524146\n",
      "Loss: 0.03446693154153799\n",
      "Loss: 0.034439568557523045\n",
      "Loss: 0.034413766010332136\n",
      "Loss: 0.034386056915707654\n",
      "Loss: 0.034359952820148504\n",
      "Loss: 0.03433294852096173\n",
      "Loss: 0.03430615721537691\n",
      "Loss: 0.03427993654407664\n",
      "Loss: 0.03425378757611366\n",
      "Loss: 0.034227158596375803\n",
      "Loss: 0.034201570536403755\n",
      "Loss: 0.034175518207257984\n",
      "Loss: 0.03414951064407154\n",
      "Loss: 0.03412372173506235\n",
      "Loss: 0.03409817477077772\n",
      "Loss: 0.034072067528846006\n",
      "Loss: 0.034045183292188586\n",
      "Loss: 0.034019460912099606\n",
      "Loss: 0.03399456586589854\n",
      "Loss: 0.033967999562158395\n",
      "Loss: 0.03394146598984921\n",
      "Loss: 0.033916550117508354\n",
      "Loss: 0.03388989991809374\n",
      "Loss: 0.03386364670829636\n",
      "Loss: 0.033839876378908694\n",
      "Loss: 0.03381277211848247\n",
      "Loss: 0.03378735039409601\n",
      "Loss: 0.0337623651139391\n",
      "Loss: 0.03373638263421784\n",
      "Loss: 0.033710052986535226\n",
      "Loss: 0.03368492559271411\n",
      "Loss: 0.03365972565385018\n",
      "Loss: 0.03363397365250513\n",
      "Loss: 0.033609653445734536\n",
      "Loss: 0.033583192407143476\n",
      "Loss: 0.03355790957285138\n",
      "Loss: 0.03353197714673986\n",
      "Loss: 0.0335075036924642\n",
      "Loss: 0.03348175298714839\n",
      "Loss: 0.033457167811176976\n",
      "Loss: 0.03343190198425967\n",
      "Loss: 0.033405784214332546\n",
      "Loss: 0.033380828714240696\n",
      "Loss: 0.03335666798024377\n",
      "Loss: 0.033330530443292476\n",
      "Loss: 0.03330561957323092\n",
      "Loss: 0.03328093105736281\n",
      "Loss: 0.0332550311393838\n",
      "Loss: 0.033230017636333126\n",
      "Loss: 0.033205936188866775\n",
      "Loss: 0.03317972724439394\n",
      "Loss: 0.03315455224668033\n",
      "Loss: 0.03313090734867804\n",
      "Loss: 0.03310536697595229\n",
      "Loss: 0.033079644868143125\n",
      "Loss: 0.033056121522030454\n",
      "Loss: 0.03302991107814979\n",
      "Loss: 0.033004951764124525\n",
      "Loss: 0.03298028432776664\n",
      "Loss: 0.03295543769883508\n",
      "Loss: 0.03293076454414225\n",
      "Loss: 0.032906377003059956\n",
      "Loss: 0.03288129082615208\n",
      "Loss: 0.0328555607092975\n",
      "Loss: 0.03283156518227063\n",
      "Loss: 0.03280633661631219\n",
      "Loss: 0.03278199814672604\n",
      "Loss: 0.03275792223965516\n",
      "Loss: 0.03273305754511749\n",
      "Loss: 0.03270755954509023\n",
      "Loss: 0.032683303667115596\n",
      "Loss: 0.032659725778062074\n",
      "Loss: 0.032633418189061035\n",
      "Loss: 0.03260885446261866\n",
      "Loss: 0.03258366228071934\n",
      "Loss: 0.03255910644599463\n",
      "Loss: 0.03253392240852149\n",
      "Loss: 0.03250987486293596\n",
      "Loss: 0.03248355291458283\n",
      "Loss: 0.032458604051386085\n",
      "Loss: 0.03243538202526127\n",
      "Loss: 0.03240894305659774\n",
      "Loss: 0.032385018272498824\n",
      "Loss: 0.032359554882984996\n",
      "Loss: 0.0323347730335383\n",
      "Loss: 0.03230955636638282\n",
      "Loss: 0.032286444207814666\n",
      "Loss: 0.03225999504632387\n",
      "Loss: 0.03223546939471179\n",
      "Loss: 0.032211047865033204\n",
      "Loss: 0.03218649118816145\n",
      "Loss: 0.032162198715413315\n",
      "Loss: 0.032137698305802605\n",
      "Loss: 0.03211256218368195\n",
      "Loss: 0.03208735129212166\n",
      "Loss: 0.03206501462265027\n",
      "Loss: 0.032038384617744546\n",
      "Loss: 0.032015359590903555\n",
      "Loss: 0.031990104926608635\n",
      "Loss: 0.03196547382050712\n",
      "Loss: 0.03194100954894781\n",
      "Loss: 0.031917476691224404\n",
      "Loss: 0.031892045735698565\n",
      "Loss: 0.031866982126007706\n",
      "Loss: 0.031843760959938114\n",
      "Loss: 0.03181875089714867\n",
      "Loss: 0.03179575072854023\n",
      "Loss: 0.03177127091054645\n",
      "Loss: 0.03174700916904887\n",
      "Loss: 0.03172241665698298\n",
      "Loss: 0.03170002647539624\n",
      "Loss: 0.03167463369782116\n",
      "Loss: 0.03165029904547318\n",
      "Loss: 0.03162823388620251\n",
      "Loss: 0.03160287060153334\n",
      "Loss: 0.03157955594475218\n",
      "Loss: 0.031555666669208705\n",
      "Loss: 0.0315315187880941\n",
      "Loss: 0.031506837439513644\n",
      "Loss: 0.03148509238440101\n",
      "Loss: 0.031459798567066356\n",
      "Loss: 0.031436834525277915\n",
      "Loss: 0.03141304770917778\n",
      "Loss: 0.0313885435025248\n",
      "Loss: 0.03136527135065685\n",
      "Loss: 0.03134212928738901\n",
      "Loss: 0.031317771144963626\n",
      "Loss: 0.031293244083295396\n",
      "Loss: 0.03127051107235998\n",
      "Loss: 0.03124626188790426\n",
      "Loss: 0.031223289624824988\n",
      "Loss: 0.031199758492510107\n",
      "Loss: 0.031175159264281077\n",
      "Loss: 0.031151682888687485\n",
      "Loss: 0.0311290170047405\n",
      "Loss: 0.031104650924381694\n",
      "Loss: 0.031080883391914095\n",
      "Loss: 0.0310581093091407\n",
      "Loss: 0.031033903869527552\n",
      "Loss: 0.03101132810685719\n",
      "Loss: 0.03098839685322847\n",
      "Loss: 0.030963797600837503\n",
      "Loss: 0.030939955653268587\n",
      "Loss: 0.03091786902258895\n",
      "Loss: 0.030893935971026944\n",
      "Loss: 0.030871188988463142\n",
      "Loss: 0.030848313558094597\n",
      "Loss: 0.030823725737345445\n",
      "Loss: 0.030800823236204596\n",
      "Loss: 0.03077867040612209\n",
      "Loss: 0.03075453612837092\n",
      "Loss: 0.030730570363484713\n",
      "Loss: 0.03070819495277314\n",
      "Loss: 0.03068487916871741\n",
      "Loss: 0.030662572910250104\n",
      "Loss: 0.03063887151402626\n",
      "Loss: 0.030616048264859985\n",
      "Loss: 0.03059199857372059\n",
      "Loss: 0.030571192020264364\n",
      "Loss: 0.030546703515462265\n",
      "Loss: 0.030524754065880386\n",
      "Loss: 0.0305012171673957\n",
      "Loss: 0.03047859571425867\n",
      "Loss: 0.030455723765828016\n",
      "Loss: 0.030433600233856096\n",
      "Loss: 0.030410083546579317\n",
      "Loss: 0.030386973819707386\n",
      "Loss: 0.03036465927380384\n",
      "Loss: 0.030342058751767785\n",
      "Loss: 0.030320267693255413\n",
      "Loss: 0.03029637708714268\n",
      "Loss: 0.03027360336096198\n",
      "Loss: 0.030251607323726754\n",
      "Loss: 0.03023017491628607\n",
      "Loss: 0.030205381859284734\n",
      "Loss: 0.030185628187078466\n",
      "Loss: 0.030161452337781697\n",
      "Loss: 0.03013974482034663\n",
      "Loss: 0.030117742770985955\n",
      "Loss: 0.03009462652577825\n",
      "Loss: 0.030072536485882408\n",
      "Loss: 0.030050534299375264\n",
      "Loss: 0.030028692300148933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP at 0x1e8486ea270>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Architecture: [input=30, hidden1=1,output=1]\n",
    "mlp_model2 = MLP(architecture=[30,16, 1], learning_rate=0.1, n_iterations=1000)\n",
    "mlp_model2.fit(X_train, y_train)  # Note: no bias column needed — bias handled internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "db37bb71-0c0d-4a0e-a213-523026b97071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
      " 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0\n",
      " 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1\n",
      " 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred_mlp2=mlp_model2.predict(X_test)\n",
    "print(y_pred_mlp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aab30d87-23ed-4b6f-8cd5-8f3bf86dd266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "  Accuracy: 0.9737\n",
      "  Precision: 0.9859\n",
      "  Recall: 0.9722\n",
      "  F1: 0.9790\n"
     ]
    }
   ],
   "source": [
    "lr_metrics = compute_metrics(y_test, y_pred_mlp2)\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "for metric, value in lr_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bf074861-c6eb-4ad1-a1f2-5e2d9fc8de29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAHqCAYAAAAZLi26AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAADiK0lEQVR4nOzdd3gU5drH8e9uekIK6QFDk96rAh5sSEcpKqiIVBVREVARLICIx/oiNooHAbscFD2oiMRCR0G6Epp0CISaQELqzvvHkE2WhJCEJLshv891zcXMM+2ezRC492kWwzAMRERERERERKTYWZ0dgIiIiIiIiMjVSkm3iIiIiIiISAlR0i0iIiIiIiJSQpR0i4iIiIiIiJQQJd0iIiIiIiIiJURJt4iIiIiIiEgJUdItIiIiIiIiUkKUdIuIiIiIiIiUECXdIiIiIiIiIiVESbeIiJR5FoulQMvSpUuv6D4TJ07EYrEU6dylS5cWSwxXcu+vvvqq1O9dFFu2bGHQoEFUr14db29vKlSoQPPmzXn99dc5deqUs8MTEREpFHdnByAiInKl1qxZ47D90ksv8dtvv/Hrr786lNevX/+K7jN06FA6d+5cpHObN2/OmjVrrjiGq91//vMfhg8fTp06dXj66aepX78+6enp/Pnnn8yYMYM1a9bwzTffODtMERGRAlPSLSIiZV7r1q0dtsPCwrBarbnKL5acnIyvr2+B73PNNddwzTXXFCnGgICAy8ZT3q1Zs4ZHHnmEDh068O233+Ll5WXf16FDB5588kkWL15cLPc6f/483t7eRW65ICIiUlBqXi4iIuXCzTffTMOGDVm+fDlt27bF19eXwYMHAzBv3jw6duxIVFQUPj4+1KtXj7Fjx5KUlORwjbyal1erVo3u3buzePFimjdvjo+PD3Xr1mX27NkOx+XVvHzgwIFUqFCB3bt307VrVypUqEB0dDRPPvkkqampDucfOnSIu+66C39/f4KCgujXrx/r1q3DYrEwd+7cYvmM/vrrL3r06EHFihXx9vamadOmfPTRRw7H2Gw2Jk+eTJ06dfDx8SEoKIjGjRvz9ttv2485fvw4Dz30ENHR0Xh5eREWFsYNN9zAzz//nO/9//3vf2OxWPjggw8cEu4snp6e3HHHHfZti8XCxIkTcx1XrVo1Bg4caN+eO3cuFouFJUuWMHjwYMLCwvD19WXevHlYLBZ++eWXXNeYPn06FouFLVu22Mv+/PNP7rjjDoKDg/H29qZZs2b897//dTgvOTmZp556yt40Pjg4mJYtW/LFF1/k++wiInL1Uk23iIiUG3Fxcdx///2MGTOGf//731it5nfPu3btomvXrowcORI/Pz+2b9/Oa6+9xtq1a3M1Uc/L5s2befLJJxk7diwRERHMmjWLIUOGULNmTW688cZ8z01PT+eOO+5gyJAhPPnkkyxfvpyXXnqJwMBAxo8fD0BSUhK33HILp06d4rXXXqNmzZosXryYvn37XvmHcsGOHTto27Yt4eHhvPPOO4SEhPDpp58ycOBAjh07xpgxYwB4/fXXmThxIs8//zw33ngj6enpbN++nTNnztiv1b9/fzZs2MDLL79M7dq1OXPmDBs2bODkyZOXvH9mZia//vorLVq0IDo6utieK6fBgwfTrVs3PvnkE5KSkujevTvh4eHMmTOH9u3bOxw7d+5cmjdvTuPGjQH47bff6Ny5M9dffz0zZswgMDCQL7/8kr59+5KcnGxP8kePHs0nn3zC5MmTadasGUlJSfz111/5PruIiFzlDBERkavMgAEDDD8/P4eym266yQCMX375Jd9zbTabkZ6ebixbtswAjM2bN9v3TZgwwbj4n86qVasa3t7exv79++1l58+fN4KDg42HH37YXvbbb78ZgPHbb785xAkY//3vfx2u2bVrV6NOnTr27ffff98AjB9//NHhuIcfftgAjDlz5uT7TFn3nj9//iWPueeeewwvLy/jwIEDDuVdunQxfH19jTNnzhiGYRjdu3c3mjZtmu/9KlSoYIwcOTLfYy529OhRAzDuueeeAp8DGBMmTMhVXrVqVWPAgAH27Tlz5hiA8cADD+Q6dvTo0YaPj4/9+QzDMLZt22YAxrvvvmsvq1u3rtGsWTMjPT3d4fzu3bsbUVFRRmZmpmEYhtGwYUOjZ8+eBX4GERG5+ql5uYiIlBsVK1bk1ltvzVW+Z88e7rvvPiIjI3Fzc8PDw4ObbroJgNjY2Mtet2nTplSpUsW+7e3tTe3atdm/f/9lz7VYLNx+++0OZY0bN3Y4d9myZfj7++caxO3ee++97PUL6tdff6V9+/a5apkHDhxIcnKyfbC66667js2bNzN8+HB++uknEhMTc13ruuuuY+7cuUyePJnff/+d9PT0YovzStx55525ygYPHsz58+eZN2+evWzOnDl4eXlx3333AbB79262b99Ov379AMjIyLAvXbt2JS4ujh07dgDms//444+MHTuWpUuXcv78+VJ4MhERcWVKukVEpNyIiorKVXbu3DnatWvHH3/8weTJk1m6dCnr1q1jwYIFAAVKmkJCQnKVeXl5FehcX19fvL29c52bkpJi3z558iQRERG5zs2rrKhOnjyZ5+dTqVIl+36AcePG8eabb/L777/TpUsXQkJCaN++PX/++af9nHnz5jFgwABmzZpFmzZtCA4O5oEHHuDo0aOXvH9oaCi+vr7s3bu32J7pYnk9X4MGDWjVqhVz5swBzGbun376KT169CA4OBiAY8eOAfDUU0/h4eHhsAwfPhyAEydOAPDOO+/wzDPP8O2333LLLbcQHBxMz5492bVrV4k9l4iIuDYl3SIiUm7kNVL1r7/+ypEjR5g9ezZDhw7lxhtvpGXLlvj7+zshwryFhITYE7+c8ktii3KPuLi4XOVHjhwBzKQYwN3dndGjR7NhwwZOnTrFF198wcGDB+nUqRPJycn2Y6dOncq+ffvYv38/r7zyCgsWLHAY3Oxibm5utG/fnvXr13Po0KECxezl5ZVrwDngkv2nLzVS+aBBg/j999+JjY1l8eLFxMXFMWjQIPv+rGcfN24c69aty3Np2rQpAH5+frz44ots376do0ePMn36dH7//fdcrRlERKT8UNItIiLlWlYidvFo2TNnznRGOHm66aabOHv2LD/++KND+Zdfflls92jfvr39C4icPv74Y3x9ffOc7iwoKIi77rqLRx99lFOnTrFv375cx1SpUoXHHnuMDh06sGHDhnxjGDduHIZh8OCDD5KWlpZrf3p6Ot999519u1q1ag6ji4P5Jcq5c+fyvc/F7r33Xry9vZk7dy5z586lcuXKdOzY0b6/Tp061KpVi82bN9OyZcs8l7y+pImIiGDgwIHce++97Nixw/6lhIiIlC8avVxERMq1tm3bUrFiRYYNG8aECRPw8PDgs88+Y/Pmzc4OzW7AgAG89dZb3H///UyePJmaNWvy448/8tNPPwHYR2G/nN9//z3P8ptuuokJEybw/fffc8sttzB+/HiCg4P57LPP+OGHH3j99dcJDAwE4Pbbb6dhw4a0bNmSsLAw9u/fz9SpU6latSq1atUiISGBW265hfvuu4+6devi7+/PunXrWLx4Mb179843vjZt2jB9+nSGDx9OixYteOSRR2jQoAHp6els3LiRDz74gIYNG9prjfv3788LL7zA+PHjuemmm9i2bRvvvfeePdaCCgoKolevXsydO5czZ87w1FNP5fpMZ86cSZcuXejUqRMDBw6kcuXKnDp1itjYWDZs2MD8+fMBuP766+nevTuNGzemYsWKxMbG8sknn9CmTZtCzQkvIiJXDyXdIiJSroWEhPDDDz/w5JNPcv/99+Pn50ePHj2YN28ezZs3d3Z4gNlk+ddff2XkyJGMGTMGi8VCx44dmTZtGl27diUoKKhA1/m///u/PMt/++03br75ZlavXs2zzz7Lo48+yvnz56lXrx5z5sxxaBZ+yy238PXXXzNr1iwSExOJjIykQ4cOvPDCC3h4eODt7c3111/PJ598wr59+0hPT6dKlSo888wz9mnH8vPggw9y3XXX8dZbb/Haa69x9OhRPDw8qF27Nvfddx+PPfaY/dinn36axMRE5s6dy5tvvsl1113Hf//7X3r06FGgzyOnQYMG2efSzqsZ/C233MLatWt5+eWXGTlyJKdPnyYkJIT69evTp08f+3G33norCxcu5K233iI5OZnKlSvzwAMP8NxzzxU6JhERuTpYDMMwnB2EiIiIFN6///1vnn/+eQ4cOMA111zj7HBEREQkD6rpFhERKQPee+89AOrWrUt6ejq//vor77zzDvfff78SbhERERempFtERKQM8PX15a233mLfvn2kpqbam2w///zzzg5NRERE8qHm5SIiIiIiIiIlRFOGiYiIiIiIiJQQJd0iIiIiIiIiJURJt4iIiIiIiEgJKXcDqdlsNo4cOYK/vz8Wi8XZ4YiIiIiIiEgZZBgGZ8+epVKlSlitl67PLndJ95EjR4iOjnZ2GCIiIiIiInIVOHjwYL7Td5a7pNvf3x8wP5iAgAAnR5M3m83G8ePHCQsLy/cbExFn0PsprkrvprgqvZviqvRuiqsqK+9mYmIi0dHR9hzzUspd0p3VpDwgIMClk+6UlBQCAgJc+iWT8knvp7gqvZviqvRuiqvSuymuqqy9m5frtuz6TyAiIiIiIiJSRinpFhERERERESkhSrpFRERERERESki569MtIiIiIq7BZrORlpbm7DDKLZvNRnp6OikpKWWi36yUH67ybnp4eODm5nbF11HSLSIiIiKlLi0tjb1792Kz2ZwdSrllGAY2m42zZ89ediAokdLkSu9mUFAQkZGRVxSHkm4RERERKVWGYRAXF4ebmxvR0dGqZXUSwzDIyMjA3d3d6YmNSE6u8G4ahkFycjLx8fEAREVFFflaSrpFREREpFRlZGSQnJxMpUqV8PX1dXY45ZYrJDYieXGVd9PHxweA+Ph4wsPDi9zUXF8rioiIiEipyszMBMDT09PJkYiI5C/ri8H09PQiX0NJt4iIiIg4hWpXRcTVFcfvKSXdIiIiIiIiIiVESbeIiIiIiIuoVq0aU6dOLfL5c+fOJSgoqNjiuZrcfPPNjBw50tlhSDmkpFtEREREpAAGDhxIz549S/Qe69at46GHHirQsXkl6H379mXnzp1Fvv/cuXOxWCz2JSIigttvv52///67yNd0FQsWLOCll15ydhhSDinpFhERERFxEWFhYVc0oruPjw/h4eFXFENAQABxcXEcOXKEH374gaSkJLp160ZaWtoVXfdyrmSgqoIIDg7G39+/RO8hkhcl3SIiIiIixWDZsmVcd911eHl5ERUVxdixY8nIyLDvP3v2LP369cPPz4+oqCjeeuutXE2eL669njhxIlWqVMHLy4tKlSoxYsQIwGwqvX//fkaNGmWvlYa8m5cvXLiQli1b4u3tTWhoKL179873OSwWC5GRkURFRdGyZUtGjRrF/v372bFjh/2Y1atXc+ONN+Lj40N0dDQjRowgKSnJvj8uLo5u3brh4+ND9erV+fzzz3M9m8ViYcaMGfTo0QM/Pz8mT54MwHfffUeLFi3w9vamRo0avPjiiw6f46U+E4Bp06ZRq1YtvL29iYiI4K677rLvu/izPn36NA888AAVK1bE19eXLl26sGvXLvv+rM/yp59+ol69elSoUIHOnTsTFxeX7+cncjEl3S5o36E1fLDiUV76qC0/b/zA2eGIiIiIyGUcPnyYrl270qpVKzZv3sz06dP58MMP7YkkwOjRo1m1ahULFy4kJiaGFStWsGHDhkte86uvvuKtt95i5syZ7Nq1i2+//ZZGjRoBZlPpa665hkmTJhEXF3fJRPCHH36gd+/edOvWjY0bN/LLL7/QsmXLAj/XmTNn+PzzzwHw8PAAYOvWrXTq1InevXuzZcsW5s2bx8qVK3nsscfs5z3wwAMcOXKEpUuX8vXXX/PBBx8QHx+f6/oTJkygR48ebN26lcGDB/PTTz9x//33M2LECLZt28bMmTOZO3cuL7/88mU/kz///JMRI0YwadIkduzYweLFi7nxxhsv+WwDBw7kzz//ZOHChaxZswbDMOjatatDjXtycjJvvvkmn3zyCcuXL+fAgQM89dRTBf78RADcnR2A5HZy72/MT94JVvDfs4TbmhWsX4+IiIhIWXX7uys5fja11O8b5u/Fd4//64qvM23aNKKjo3nvvfewWCzUrVuXI0eO8MwzzzB+/HiSkpL46KOP+Pzzz2nfvj0Ac+bMoVKlSpe85oEDB4iMjOS2227Dw8ODKlWqcN111wFmU2k3Nzf8/f2JjIy85DVefvll7rnnHl588UV7WZMmTfJ9loSEBCpUqIBhGCQnJwNwxx13ULduXQDeeOMN7rvvPnutca1atXjnnXe46aabmD59Ovv27ePnn39m3bp19gR/1qxZ1KpVK9e97rvvPgYPHmzf7t+/P2PHjmXAgAEA1KhRg5deeokxY8YwYcKEfD+TAwcO4OfnR/fu3fH396dq1ao0a9Ysz2fctWsXCxcuZNWqVbRt2xaAzz77jOjoaL799lvuvvtuwGzyPmPGDK699loAHnvsMSZNmpTv5ydyMSXdLsiz6g2wbx4Aaaf3ODkaERERkZJ3/GwqRxNTnB1GkcXGxtKmTRuHOX1vuOEGzp07x6FDhzh9+jTp6en2BBEgMDCQOnXqXPKad999N1OnTqVGjRp07tyZrl27cvvtt+PuXvD/wm/atIkHH3ywUM/i7+/Phg0byMjIYNmyZbzxxhvMmDHDvn/9+vXs3r2bzz77zF5mGAY2m429e/eyc+dO3N3dad68uX1/zZo1qVixYq57XVzrvn79etatW2ev2QbIzMwkJSWF5OTkfD+TDh06ULVqVfu+zp0706tXrzz7yMfGxuLu7s71119vLwsJCaFOnTrExsbay3x9fe0JN0BUVFSeNfYi+VHS7YK8/KPs66nnT8HpfVCxmtPiERERESlpYf5eZfq+hmE4JNxZZWD2Xc65ntcxeYmOjmbHjh3ExMTw888/M3z4cN544w2WLVtmb+p9OT4+PoV5DACsVis1a9YEoG7duhw9epS+ffuyfPlyAGw2Gw8//LBDX+osVapUcej7nVNez+rn5+ewbbPZePHFF/Psd+7t7Z3vZ5L1ZcHSpUtZsmQJ48ePZ+LEiaxbty5XP/dLfe4X/xwv/pxz/ixFCkpJtwvydPO0r6dZLLDjR2j9iBMjEhERESlZxdHE25nq16/P119/7ZC0rV69Gn9/fypXrkxQUBAeHh6sXbuW6OhoABITE9m1axc33XTTJa/r4+PDHXfcwR133MGjjz5K3bp12bp1K82bN8fT05PMzMx842rcuDG//PILgwYNKvKzjRo1iilTpvDNN9/Qq1cvmjdvzt9//21PzC9Wt25dMjIy2LhxIy1atABg9+7dnDlz5rL3at68OTt27LjktSH/z8Td3Z3bbruN2267jQkTJhAUFMSvv/6aK4mvX78+GRkZ/PHHH/bm5SdPnmTnzp3Uq1evgJ+MSMEo6XZBXm7Z37imWiywY5GSbhEREREXkJCQwKZNmxzKgoODGT58OFOnTuXxxx/nscceY8eOHUyYMIHRo0djtVrx9/dnwIABPP300wQHBxMeHs6ECROwWq25ar+zzJ07l8zMTK6//np8fX355JNP8PHxoWrVqoA50vny5cu555578PLyIjQ0NNc1JkyYQPv27bn22mu55557yMjI4Mcff2TMmDEFfuaAgACGDh3KhAkT6NmzJ8888wytW7fm0Ucf5cEHH8TPz4/Y2FhiYmJ49913qVu3LrfddhsPPfQQ06dPx8PDgyeffBIfH59LPmuW8ePH0717d6Kjo7n77ruxWq1s2bKFrVu3Mnny5Hw/k++//549e/Zw4403UrFiRRYtWoTNZsuzCX+tWrXo0aMHDz74IDNnzsTf35+xY8dSuXJlevToUeDPRqQgNHq5C/K0Ztd0p1ossG8VnD/txIhEREREBGDp0qU0a9bMYRk/fjyVK1dm0aJFrF27liZNmjBs2DCGDBnC888/bz93ypQptGnThu7du3Pbbbdxww03UK9ePby9vfO8V1BQEP/5z3+44YYb7DXW3333HSEhIQBMmjSJffv2ce211xIWFpbnNW6++Wbmz5/PwoULadq0Kbfeeit//PFHoZ/7iSeeIDY2lvnz59O4cWOWLVvGrl27aNeuHc2aNeOFF14gKiq7i+THH39MREQEN954I7169eLBBx/E39//ks+apVOnTnz//ffExMTQqlUrWrduzZQpU+xfNOT3mQQFBbFgwQJuvfVW6tWrx4wZM/jiiy9o0KBBnveaM2cOLVq0oHv37rRp0wbDMFi0aFGBm+6LFJTFKGedEhITEwkMDCQhIYGAgABnh5OnhJQE/jXPbGLVNvk8M48dh96zoPHdTo5MxOxrFR8fT3h4OFarvrcT16F3U1yV3s3cUlJS2Lt3L9WrV79sEnY1S0pKonLlyvzf//0fQ4YMKfX7G4ZBRkYG7u7ul62BvlKHDh0iOjqan3/+2T56u8illOa7eTn5/b4qaG6p5uUuKFfzcjCbmCvpFhERESmzNm7cyPbt27nuuutISEiwTz11NTZn/vXXXzl37hyNGjUiLi6OMWPGUK1atXznzRa5WinpdkEe1uwmLWluF9Z3/wwZqeDunJE9RUREROTKvfnmm+zYsQNPT09atGjBihUr8uyLXdalp6fz7LPPsmfPHvz9/Wnbti2fffaZmm5LuaSk2wVZLBY8rZ6k2dJI9fY3C1MTYc8yqN3RucGJiIiISJE0a9aM9evXOzuMUtGpUyc6derk7DBEXII6FrmorMHU0jx9swu3feucYERERERERKRIlHS7qKykO9XqDp4VzMLt30NGmhOjEhERERERkcJQ0u2iPN0uJN22NKjd2SxMSYB9y50YlYiIiIiIiBSGkm4XlTWYWlpmGtTPMaLltv85KSIREREREREpLCXdLsrevDwzFWreBh4X+nbHfg+Z6U6MTERERERERApKSbeLykq6023p2Dy8ofaF0R/Pn4J9K50YmYiIiIiIiBSUkm4XlZV0g5qYi4iIiIiIlFVKul1U1kBqkNXEvAO4+5gFsd+BLdNJkYmIiIiUTwMHDsRisTBs2LBc+4YPH47FYmHgwIEOx/fs2fOS16tWrRoWiwWLxYKvry8NGzZk5syZVxTjtGnTqF69Ot7e3rRo0YIVK1Zc9pxly5bRokULvL29qVGjBjNmzHDY//fff3PnnXfa4506deplr7l06VIsFgtnzpzJc//EiRPtz261WqlUqRL9+vXj4MGDBXlMkTJFSbeLylXT7VUBat1mFiSfgP2rnBSZiIiISPkVHR3Nl19+yfnz5+1lKSkpfPHFF1SpUqXQ15s0aRJxcXFs2bKFnj17MmzYMObNm5fnsRMnTnRI6i82b948Ro4cyXPPPcfGjRtp164dXbp04cCBA5c8Z+/evXTr1o127dqxceNGnn32WUaMGMHXX39tPyY5OZkaNWrw6quvEhkZWehnvJQGDRoQFxfHoUOHmDdvHlu3bqVPnz7Fdn0RV6Gk20VljV4OF2q6Aer3zD7gr68RERERkdLVvHlzqlSpwoIFC+xlCxYsIDo6mmbNmhX6ev7+/kRGRlKzZk0mT55MrVq1+Pbbb4sU25QpUxgyZAhDhw6lXr16TJ06lejoaKZPn37Jcz744AOqVKnC1KlTqVevHkOHDmXw4MG8+eab9mNatWrFG2+8wT333IOXl1eRYsuLu7s7kZGRVKpUiXbt2vHggw/y+++/k5iYWGz3EHEFSrpdVK6aboA6XbJHMd/2P8hIc0JkIiIiIuXboEGDmDNnjn179uzZDB48uFiu7e3tTXp64WeqSUtLY/369XTs2NGhvGPHjqxevfqS5/3xxx906NDBoaxTp078+eefRYqjqI4ePcqCBQtwc3PDzc2t1O4rUhrcnR2A5C1Xn24ATz8z8f7razh/Gvb8lj2quYiIiEhZNvMmOBdf+vetEA4PLyvUKf3792fcuHHs27cPi8XCqlWr+PLLL1m6dGmRw8jIyODTTz9l69atPPLII4U+/8SJE2RmZhIREeFQHhERwdGjRy953tGjR/M8JyMjgxMnThAVFVXoWApq69atVKhQAZvNZm+uP2LECPz8/ErsniLOoKTbReWs6bYn3QAN78puWr51vpJuERERuTqci4ezR5wdRYGEhobSrVs3PvroIwzDoFu3boSGhhbpWs888wzPP/88qampeHp68vTTT/Pwww8DsGLFCrp06WI/Ni0tDcMw+Oqrr+xlzz77LM8++6x922KxOFzfMIxcZRfL65y8yotbnTp1WLhwIampqfzvf/9j/vz5vPzyyyV6TxFnUNLtonIm3em2HE17at4G3kGQcga2L4K0ZPD0LfX4RERERIpVhfAydd/Bgwfz2GOPAfD+++8X+fZPP/00AwcOxNfXl6ioKIdEt2XLlmzatMm+/c4773D48GFee+01e1lwcDBgfhHg5uaWq1Y7Pj4+V012TpGRkXme4+7uTkhISJGfqyA8PT2pWbMmYA6qtmvXLh555BE++eSTEr2vSGlT0u2iLlnT7e4J9e+ADR9DehLs/BEa3umECEVERESKUSGbeDtb586dSUszx9fp1KnoLQ9DQ0PtiefFfHx8HPYFBweTmJiY5/Genp60aNGCmJgYevXqZS+PiYmhR48el7z/9ddfz6JFixzKlixZQsuWLfHw8LjEWSXjhRdeoHbt2owaNYrmzZuX6r1FSpKSbheV5+jlWRreZSbdAFu/VtItIiIiUsrc3NyIjY21r19KQkKCQ201mMlzUaYXu5zRo0fTv39/WrZsSZs2bfjggw84cOCAw7zi48aN4/Dhw3z8sfl/yYceeojp06czevRoHnzwQdasWcOHH37IF198YT8nLS2Nbdu22dcPHz7Mpk2bqFChwiW/MMiydetW/P39HcqaNm2a57E1atSgR48ejB8/nu+//74oH4GIS3L66OXTpk2jevXqeHt706JFC1asWHHJYwcOHIjFYsm1NGjQoBQjLh05B1Kzj16epdq/oMKFORJ3LTEHVRMRERGRUhUQEEBAQEC+xyxdupRmzZo5LOPHjy+RePr27cvUqVOZNGkSTZs2Zfny5SxatIiqVavaj4mLi3OYt7t69er88MMPLF26lKZNm/LSSy/xzjvvcOed2ZU6R44cscceFxfHm2++SbNmzRg6dOhlY7rxxhtzPX9+nnzySX744Qf++OOPInwCIq7JYmSNlOAE8+bNo3///kybNo0bbriBmTNnMmvWLLZt25bnt38JCQn2kQ3BHOWxSZMmPP7440ycOLFA90xMTCQwMJCEhITL/pJ0FpvNxuz1s3l729sATGo7iV61ejketHgc/D7NXL/jXWj+QClHKeWVzWYjPj6e8PBwrFanf28nYqd3U1yV3s3cUlJS2Lt3r73iRZzDMAwyMjJwd3cv8UHTRArDld7N/H5fFTS3dOpv/ilTpjBkyBCGDh1KvXr1mDp1KtHR0UyfPj3P4wMDA4mMjLQvf/75J6dPn2bQoEGlHHnJ87J62ddzNS8Hs4l5lq1f5d4vIiIiIiIiTue0pDstLY3169fTsWNHh/KOHTuyevXqAl3jww8/5LbbbnNoMnO1yNm8PCUjJfcBlZtDxerm+r4VcPbS8y+KiIiIiIiIczhtILUTJ06QmZmZawqDiIiIXNMW5CUuLo4ff/yRzz//PN/jUlNTSU3NrilOTEwEzKZeNputCJGXPJvN5lDTfT7jfJ6xWhreiWXFm2DYsP21AK4flusYkeJms9kwDMNl//5I+aV3U1yV3s3csj6TrEWcJ+vz189BXI2rvJtZv6fyyh8L+nvd6aOXX9xG3zCMArXbnzt3LkFBQfTs2TPf41555RVefPHFXOXHjx8nJSWPGmQXYLPZSD+fPTf3qcRTxMfH5zrOLeoWwngTgIyNX3Kqeu9Si1HKL5vNRkJCAoZhqG+iuBS9m+Kq9G7mlp6ejs1mIyMjg4yMDGeHU24ZhkFmZiaQ+//kIs7kSu9mRkYGNpuNkydP5ppG7+zZswW6htOS7tDQUNzc3HLVasfHx+eq/b6YYRjMnj2b/v374+npme+x48aNY/To0fbtxMREoqOjCQsLc9mB1E6eS+HQueyabquXlfDw8NwHhodjRDTAcuxvPOM3E+6RDBWrlV6gUi7ZbDYsFgthYWH6z6O4FL2b4qr0buaWkpLC2bNncXd3x93d6XVA5V5pz8ctUlCu8G66u7tjtVoJCQnJNZBaQQeCdNpvOU9PT1q0aEFMTAy9emWPzB0TE0OPHj3yPXfZsmXs3r2bIUOGXPY+Xl5eeHl55Sq3Wq0u+w/fP8eTeXf5cfxqmNupmamXjrXhXXDsbwCsW76EW54tpSilPLNYLC79d0jKL72b4qr0bjqyWq0O07+Kc+RsYaqfg7gSV3o3s35P5fU7vKC/0536m3/06NHMmjWL2bNnExsby6hRozhw4ADDhpl9k8eNG8cDD+SeCuvDDz/k+uuvp2HDhqUdcqmwWi0YtuzvQ1Iy82kG37gvWC78GDd8DJlqoiUiIiIiIuIqnNqep2/fvpw8eZJJkyYRFxdHw4YNWbRokX008ri4OA4cOOBwTkJCAl9//TVvv/22M0IuFVYLYFxm9PIsgZWhdhfY8QOcjYNdP0HdbiUfpIiIiIiIiFyW0zvRDB8+nOHDh+e5b+7cubnKAgMDSU5OLuGonMvNYsGwZfdfyDfpBmg5yEy6Af6craRbRERERETERahjkQuyWCyQM+nOr3k5wLW3QlAVc333L3B6X8kFJyIiIiIiIgWmpNsFWS0AbhiGOWjAZWu6rW7QfMCFDQPWf1SS4YmIiIiUSwMHDsRisdjHH8pp+PDhWCwWBg4c6HB8ftPbVqtWzT5Ik6+vLw0bNmTmzJlXFOO0adOoXr063t7etGjRghUrVuR7fFxcHP369aNOnTpYrVZGjhx52Xvs27cPi8XCpk2b8tw/d+5ch4HyIiIiuP322/n777+L8EQiZZ+SbhfkZrUA2bXdl026AZr1B+uF3gIbPoZ015yDXERERKQsi46O5ssvv+T8+fP2spSUFL744guqVKlS6OtljW20ZcsWevbsybBhw5g3b16ex06cONEhqb/YvHnzGDlyJM899xwbN26kXbt2dOnSJdcYSTmlpqYSGhrKc889R5MmTQod/6UEBAQQFxfHkSNH+OGHH0hKSqJbt26kpaUV2z1Eygol3S4oa1h848JgapdtXg7gHwH17jDXk0/A1v+WVHgiIiIi5Vbz5s2pUqUKCxYssJctWLCA6OhomjVrVujr+fv7ExkZSc2aNZk8eTK1atXi22+/LVJsU6ZMYciQIQwdOpR69eoxdepUoqOjmT59+iXPqVatGm+//TYPPPAAgYGBRbpvXiwWC5GRkURFRdGyZUtGjRrF/v372bFjR7HdQ6SsUNLtgtyypqIrTE03QJvHstfXvA+GUbyBiYiIiAiDBg1izpw59u3Zs2czePDgYrm2t7c36enphT4vLS2N9evX07FjR4fyjh07snr16mKJrajOnDnD559/DoCHh8dljha5+jh99HLJzWqv6b6QdBekphvgmhZQpQ0cWAPHt5uDqtW6raTCFBERESk2fb/vy4nzJ0r9vqE+oczrnndz7kvp378/48aNs/dtXrVqFV9++SVLly4tchwZGRl8+umnbN26lUceeaTQ5584cYLMzEwiIiIcyiMiIjh69GiR4yqqhIQEKlSogGEY9pmH7rjjDurWrVvqsYg4m5JuF2S1XqjqzlHTbRiGvdl5vto8aibdAGveVdItIiIiZcKJ8yeIT453dhgFEhoaSrdu3fjoo48wDINu3boRGhpapGs988wzPP/886SmpuLp6cnTTz/Nww8/DMCKFSvo0qWL/di0tDQMw+Crr76ylz377LM8++yz9u2L/79Y4P9DFjN/f382bNhARkYGy5Yt44033mDGjBmlHoeIK1DS7YKycu6smu5MI5MMWwYebgVojlOnK1SsDqf3wp6lcHQrRDYquWBFREREikGoT9GSVmfdd/DgwTz2mNm17/333y/y/Z9++mkGDhyIr68vUVFRDglyy5YtHUYIf+eddzh8+DCvvfaavSw4OBgwvwhwc3PLVasdHx+fq/a7NFitVmrWrAlA3bp1OXr0KH379mX58uWlHouIsynpdkFu9ppuT3tZSmZKwZJuq5tZ273oKXN7zTTodenBM0RERERcQWGbeDtb586d7SNxd+rUqcjXCQ0NtSenF/Px8XHYFxwcTGJiYp7He3p60qJFC2JiYujVq5e9PCYmhh49ehQ5vuIyatQopkyZwjfffOMQn0h5oKTbBWWPXp7940nJSMHf079gF2h6H/w6GVLOwNb50P4FCKhUApGKiIiIlE9ubm7Exsba1y8lISEh13zWwcHBRZpe7HJGjx5N//79admyJW3atOGDDz7gwIEDDvOKjxs3jsOHD/Pxxx/byzZt2oTFYuHcuXMcP36cTZs24enpSf369fO9X14jkV/qnICAAIYOHcqECRPo2bOnU5q8iziLkm4XZLWPXp6jprugI5gDePpBy8GwcgrY0s2RzDu9XLxBioiIiJRzAQEBlz1m6dKluaYSGzBgAHPnzi32ePr27cvJkyftc383bNiQRYsWUbVqVfsxcXFxuebtbt68uX19/fr1fP7551StWpV9+/ble7977rknV9nevXsvefwTTzzBO++8w/z58+nTp08Bn0qk7LMYRvmaVyoxMZHAwEASEhIK9IvSGY6eSab1q7/hFfk1nhXXAbDgjgXUqlir4Bc5Fw9TG0FGCnj4wai/wDe4hCKW8sRmsxEfH094eDhWq2YdFNehd1Ncld7N3FJSUti7dy/Vq1fH29vb2eGUW4ZhkJGRgbu7u2qexaW40ruZ3++rguaW+s3vguwvlpHdh/t8xvnCXaRCODTrb66nJ8EfM4spOhERERERESkoJd0uKGsgNcPmZS9LSk8q/IVuGAHWCz0I/pgBqWeLIzwREREREREpICXdLii7T3d20p2cnlz4CwVVgUYX+suknIF1H15xbCIiIiIiIlJwSrpdkDWvmu6MItR0A/xrFHAhi1/9LqSeu8LoREREREREpKCUdLsga9aUYTlGLy9S83KAsNrQsLe5nnwC/tCc3SIiIiIiIqVFSbcLcssaSC0ze3S8IifdADePA8uFH/Wqd+H86SuITkRERERERApKSbcLyurTbVxpn+4sobWgyX3memoCrH7vCqITERERERGRglLS7YLy6tN9Lv0K+2LfNAasF6Yg+306nDt+ZdcTERERERGRy1LS7YKy+nRzpVOG5VSxKrQYaK6nJ8HSV67seiIiIiIiInJZSrpdULE3L89y49PgWcFcXz8Xju+48muKiIiIiEiZN3HiRJo2bVoq99q3bx8Wi4VNmzZd8phly5ZhtVo5c+bMJY+ZO3cuQUFBxR5fcVPS7YIsFgsWLpoy7EprugH8I+CGkea6kQkx46/8miIiIiLlxMCBA7FYLAwbNizXvuHDh2OxWBg4cKDD8T179rzk9apVq2b+v89iwdfXl4YNGzJz5swrinHatGlUr14db29vWrRowYoVK/I9Pi4ujn79+lGnTh2sVisjR4687D0ulzDNnTvX/lwWi4WIiAhuv/12/v777yI8UbaHH36Ya6+9Fh8fH8LCwujRowfbt293OCbnff39/WnZsiULFiy4ovuWhoIkoSXtqaee4pdffrmia+SXBFssFr799lsAoqOjiYuLo2HDhld0v7JCSbeLslqAnFOGFXWe7ou1eRT8K5nrOxfDnmXFc10RERGRciA6Opovv/yS8+fP28tSUlL44osvqFKlSqGvN2nSJOLi4tiyZQs9e/Zk2LBhzJs3L89jJ06c6JDUX2zevHmMHDmS5557jo0bN9KuXTu6dOnCgQMHLnlOamoqoaGhPPfcczRp0qTQ8V9KQEAAcXFxHDlyhB9++IGkpCS6detGWlraJc+5+eabmTt37iX3t2jRgjlz5hAbG8tPP/2EYRh07NiRzMxMh+PmzJlDXFwc69ato0mTJtx9992sWbOmSM+RX7zOUJLxVKhQgZCQkBK7fk5ubm5ERkbi7u5eKvdzNiXdLsocTM2KxTBru5PSiinp9vSF9i9kby95Hmy24rm2iIiIyFWuefPmVKlSxaH2dMGCBURHR9OsWbNCX8/f35/IyEhq1qzJ5MmTqVWrlr02sLCmTJnCkCFDGDp0KPXq1WPq1KlER0czffr0S55TrVo13n77bR544AECAwOLdN+8WCwWIiMjiYqKomXLlowaNYr9+/ezY0fRuzc+9NBD3HjjjVSrVo3mzZszefJkDh48yL59+xyOCwoKIjIykrp16zJjxgy8vb1ZuHAhAIcPH6Zv375UrFiRkJAQevTo4XB+VuuEV155hUqVKlG7dm0ADh06xD333ENwcDB+fn60bNmSP/74w37ed999R4sWLfD29qZGjRq8+OKLZGRkOHwe06dPp0uXLvj4+FC9enXmz59v31+9enUAmjVrhsVi4eabb843nq1bt3Lrrbfi4+NDSEgIDz30EOfOZQ+8nHXem2++SVRUFCEhITz66KOkp6df8vO9uHl5Ua5RUHnV7C9atIjatWvj4+PDrbfeyv79+3OdN3fuXKpUqYKvry+9evXi5MmTuY4pyM9i1qxZ9OrVC19fX2rVqmV/P0qKkm4XlfWDsSfdxVXTDdD4HohsZK4f3QJb8v42VURERERyGzRoEHPmzLFvz549m8GDBxfLtb29vYuU1KSlpbF+/Xo6duzoUN6xY0dWr15dLLEV1ZkzZ/j8888B8PDwKJZrJiUlMWfOHKpXr050dPQlj/Pw8MDd3Z309HSSk5O55ZZbqFChAsuXL2flypVUqFCBzp07O9Qg//LLL8TGxhITE8P333/PuXPnuOmmmzhy5AgLFy5k8+bNjBkzBtuFiquffvqJ+++/nxEjRrBt2zZmzpzJ3Llzefnllx1ieeGFF7jzzjvZvHkz999/P/feey+xsbEArF27FoCff/6ZuLg4hy91Lo4nOTmZzp07U7FiRdatW8f8+fP5+eefeeyxxxzu99tvv/HPP//w22+/8dFHHzF37tx8WxLkpTiuURAHDx6kd+/edO3alU2bNjFkyBCee+45h2P++OMPBg8ezPDhw9m0aRO33HILkydPdjimoD+LF198kT59+rBlyxa6du1Kv379OHXqVLE/V5byUZ9fBlmtFsg0sBjeQGLx9OnOvjh0fBk+vsPc/mUS1O9h1oKLiIiIOMHeO+8i48SJUr+ve2go1b/+qlDn9O/fn3Hjxtlr61atWsWXX37J0qVLixxHRkYGn376KVu3buWRRx4p9PknTpwgMzOTiIgIh/KIiAiOHj1a5LiKKiEhgQoVKmAYBsnJ5oDAd9xxB3Xr1r2i606bNo0xY8aQlJRE3bp1iYmJwdPTM89jU1NTeeONN0hMTKR9+/Z8+eWXWK1WZs2aheXCbEFz5swhKCiIpUuX2r+w8PPzY9asWfbrfvDBBxw/fpx169YRHBwMQM2aNe33efnllxk7diwDBgwAoEaNGrz00kuMGTOGCRMm2I+7++67GTp0KAAvvfQSMTExvPvuu0ybNo2wsDAAQkJCiIyMdHiOi+P5z3/+w/nz5/n444/x8/MD4L333uP222/ntddes78DFStW5L333sPNzY26devSrVs3fvnlFx588MECf95FuUbWz74wpk+fTo0aNXjrrbewWCzUrl2bzZs38+abb9qPefvtt+nUqRNjx44FoHbt2qxevZrFixfbjynoz2LgwIHce++9APz73//m3XffZe3atXTu3LlQcReUkm4XlTVrWFZNd3J6MoZh2H9BXLEaN0Htzma/7rNHYOVbcOtzlz9PREREpARknDhBxrFjzg6jQEJDQ+nWrRsfffQRhmHQrVs3QkNDi3StZ555hueff57U1FQ8PT15+umnefjhhwFYsWIFXbp0sR+blpaGYRh89VX2lwTPPvsszz77rH374v8rFuv/HwvB39+fDRs2kJGRwbJly3jjjTeYMWOGwzH//ve/+fe//23fPn/+PL///rtDje2PP/5Iu3bt7Nv9+vWjQ4cOxMXF8eabb9KnTx9WrVqFt7e3/Zh7770XNzc3zp8/T2BgIG+++SZdunTh0UcfZffu3fj7+zvEkZKSwj///GPfbtSokUMiv2nTJpo1a2ZPuC+2fv161q1b51CbmpmZSUpKCsnJyfj6mhVbbdq0cTivTZs2BRo47eJ4YmNjadKkiT3hBrjhhhuw2Wzs2LHDnnQ3aNAANzc3+zFRUVFs3br1svfLqSjXyPrZX6xWrVqXPCc2NpbWrVs7vKutW7fOdUyvXr0cytq0aeOQdBf0Z9G4cWP7fj8/P/z9/YmPj8/3ua6Ekm4X5XbhhctKujONTFIyU/Bx9ym+m3SYBLt/BlsGrJoKjftA6KX/MoiIiIiUFPciJq3Ouu/gwYPtyeH7779f5Ps//fTTDBw4EF9fX6KiohySjpYtWzokZe+88w6HDx/mtddes5dlJYKhoaG4ubnlqtWOj4/PVftdGqxWq702uG7duhw9epS+ffuyfPly+zHDhg2jT58+9u1+/fpx55130rt3b3tZ5cqVHa4bGBhIYGAgtWrVonXr1lSsWJFvvvnGXmsJ8NZbb3HbbbcREBBAeHi4vdxms9GiRQs+++yzXPFm1TQDDsksgI9P/v//ttlsvPjiiw5xZ8n5ZUBeCvKFyMXx5PdFSs7yi5vyWywWe5P4girKNXL+7AvKMIxiOaagP4vi+GwKQ0m3i7L/fbFl/yU/m3a2eJPusDrQ9nGzljszDX4YDQ8szHFzERERkdJR2CbezpazH3CnTp2KfJ3Q0NBLJig+Pj4O+4KDg0lMTMzzeE9PT1q0aEFMTIxDbWBMTAw9evQocnzFZdSoUUyZMoVvvvnGHl9wcLBD7bGPjw/h4eGFStgMwyA1NdWhLGtguos1b96cefPmER4eTkBAQIHv0bhxY2bNmsWpU6fyrO1u3rw5O3bsuGzcv//+Ow888IDDdtbge1k12RePxJ6X+vXr89FHH5GUlGRPyFetWoXVarUPtFbW1K9fP9cAgjkHqss65vfff3cou3i7oD+L0qaB1FyUNaum25bdzzoxNbH4b3TjGAi8ML3F3uWwtWz9gyciIiLiDG5ubsTGxhIbG+vQ/PZiCQkJbNq0yWHJbwqvKzF69GhmzZrF7NmziY2NZdSoURw4cMBhXvFx48Y5JH6APa5z585x/PhxNm3axLZt2y57vx07duR6tktNaRUQEMDQoUOZMGFCgWosL7Znzx5eeeUV1q9fz4EDB1izZg19+vTBx8eHrl27Fuga/fr1IzQ0lB49erBixQr27t3LsmXLeOKJJzh06NAlz7v33nuJjIykZ8+erFq1ij179vD111/bpyEbP348H3/8MRMnTuTvv/8mNjaWefPm8fzzzztcZ/78+cyePZudO3cyYcIE1q5da28tER4ejo+PD4sXL+bYsWMkJCTk+xze3t4MGDCAv/76i99++43HH3+c/v37O6VVQ3EYNmwY//zzD6NHj2bHjh18/vnnfPLJJw7HjBgxgsWLF/P666+zc+dO3nvvPYem5VDwn0VpU9Ltoqz2mu7spDsh7dJ/+YrM0xe6vpG9vXgsJOUeel9EREREHAUEBFy2xnTp0qU0a9bMYRk/fnyJxNO3b1+mTp3KpEmTaNq0KcuXL2fRokVUrVrVfkxcXFyupL958+Y0a9aM9evX8/nnn9OsWbMCJbL33HNPrmc7cuTIJY9/4okniI2NdZgqq6C8vb1ZsWIFXbt2pWbNmvTp0wc/Pz9Wr17t0IQ8P76+vixfvpwqVarQu3dv6tWrx+DBgzl//ny+P0dPT0+WLFlCeHg4Xbt2pVGjRrz66qv2L1s6derE999/T0xMDK1ataJ169ZMmTLF4XMHc8TsL7/8ksaNG/PRRx/x2WefUb9+fQDc3d155513mDlzJpUqVcq3dYKvry8//fQTp06dolWrVtx11120b9+e9957r0CfgyuqUqUKX3/9Nd999x1NmjRh5syZvPTSSw7HtG7dmlmzZvHuu+/StGlTlixZkiuZLujPorRZjKJ81VSGJSYmEhgYSEJCQqGalZQmm83GdS//zImkdEIqLyctYBEA79zyDrdUuaVkbjrvfoj9zlxveCfcNbtk7iNlns1mIz4+nvDwcKxWfW8nrkPvprgqvZu5paSksHfvXqpXr37ZPq9ScgzDICMjA3d3d6cMtlaeWCwWvvnmG3r27OnsUMoEV3o38/t9VdDcUr/5XVSp1XRn6fp/4B1krv/1NWwr2QniRUREREREygMl3S4qq0+3kZk9cFpCagkm3f4Rjs3MfxitZuYiIiIiIiJXSEm3i7LXdGfmGEgtrQQGUsup0d1Q50L/naTj8P1IKF+9D0REREREip1hGGpaXo4p6XZRWTXdttKq6QZzqrDub4HPhakQYhfCps9L9p4iIiIiIiJXMSXdLiprnJWcSXeJTBl2Mf9IuP3t7O0fx8CpPSV/XxERESl3ytl4viJSBhXH7ykl3S7KwoU+3Rk5ku6Sbl6epf4d0Ox+cz3tHCx4GDIzSufeIiIictXLmmrpUnM6i4i4iuTkZAA8PDyKfA334gpGipebvabbCzcsGBgl37w8p86vwb5VcHovHFoLK/4Pbn6m9O4vIiIiVy13d3d8fX05fvw4Hh4emkrNSVxpWiaRnFzh3TQMg+TkZOLj4wkKCrJ/WVgUTk+6p02bxhtvvEFcXBwNGjRg6tSptGvX7pLHp6amMmnSJD799FOOHj3KNddcw3PPPcfgwYNLMeqSl/Vy2QwrQZ7+JKYlll5NN4BXBej9H5jdCYxMWPYaXHsrRLcqvRhERETkqmSxWIiKimLv3r3s37/f2eGUW4ZhYLPZsFqtSrrFpbjSuxkUFERkZOQVXcOpSfe8efMYOXIk06ZN44YbbmDmzJl06dKFbdu2UaVKlTzP6dOnD8eOHePDDz+kZs2axMfHk5Fx9TV9zvq+N9MwCPQKJDEtsWTn6c5LdCu4aQwsfcVMvBc8CMNWmgm5iIiIyBXw9PSkVq1aamLuRDabjZMnTxISEqLWBuJSXOXd9PDwuKIa7ixOTbqnTJnCkCFDGDp0KABTp07lp59+Yvr06bzyyiu5jl+8eDHLli1jz549BAebI2xXq1atNEMuNdYLc4YZhkGgZyAHOUhiaiI2w4bVUoovXrunYPfPcGid2dR88Vjo8V7p3V9ERESuWlarFW9vb2eHUW7ZbDY8PDzw9vZW0i0u5Wp7N52WdKelpbF+/XrGjh3rUN6xY0dWr16d5zkLFy6kZcuWvP7663zyySf4+flxxx138NJLL+Hj45PnOampqaSmptq3ExPNJto2mw2bzVZMT1O8bDZbdk23zcDf0x8AA4PE1EQCPANKLxiLFXrOxPLBjVjSzsHGT7DV6gh1u5deDOJSbDabvcmPiCvRuymuSu+muCq9m+Kqysq7WdD4nJZ0nzhxgszMTCIiIhzKIyIiOHr0aJ7n7Nmzh5UrV+Lt7c0333zDiRMnGD58OKdOnWL27Nl5nvPKK6/w4osv5io/fvw4KSkpV/4gJcD8QiDTXDfAy/Cy79t7ZC9RvlGlHJEfPm2fJXDps+bm/x7nhFdVbH4R+Z8mVyWbzUZCQgKGYVwV3zzK1UPvprgqvZviqvRuiqsqK+/m2bNnC3Sc0wdSu7hjvGEYl+wsb7PZsFgsfPbZZwQGBgJmE/W77rqL999/P8/a7nHjxjF69Gj7dmJiItHR0YSFhREQUIo1xoVgNqfYCZg19GH+YXDhewh3f3fCQ8JLP6iwYRhH12DZ/h3W1DOErZqI0W++WRMu5UrW38OwsDCX/iUo5Y/eTXFVejfFVendFFdVVt7NgnaPcVrSHRoaipubW65a7fj4+Fy131mioqKoXLmyPeEGqFevHoZhcOjQIWrVqpXrHC8vL7y8vHKVW61Wl/4BuuX44iHQK8i+fjb9rPPivuMdOPwnnI3DsudXLGs/gDbDnROLOJXFYnH5v0NSPundFFeld1Ncld5NcVVl4d0saGxOewJPT09atGhBTEyMQ3lMTAxt27bN85wbbriBI0eOcO7cOXvZzp07sVqtXHPNNSUab2mz5qjsr5CjD3diailOG3Yx32DoOS17O+YFOPCH8+IRERERERFxcU792mD06NHMmjWL2bNnExsby6hRozhw4ADDhg0DzKbhDzzwgP34++67j5CQEAYNGsS2bdtYvnw5Tz/9NIMHD77kQGpllTVHTbe/h799PSG1lKcNu9i1t0LbEea6LQP++wCcPebcmERERERERFyUU/t09+3bl5MnTzJp0iTi4uJo2LAhixYtomrVqgDExcVx4MAB+/EVKlQgJiaGxx9/nJYtWxISEkKfPn2YPHmysx6hxOTs1u7vkaOmO82JNd1Z2k+AIxth3wo4dxTmD4QBC8HNw9mRiYiIiIiIuBSnD6Q2fPhwhg/Pu1/w3Llzc5XVrVs3V5P0q9Glmpc7vaYbwM0d7poDH9wEiYfhwGpY8gJ0edXZkYmIiIiIiLgU1+2VXs7lbF5ewdVqugEqhEGfj8HN09z+Yzpsme/cmERERERERFyMkm4X5VDT7e5iNd1ZrmkJXV7P3l74OBz9y3nxiIiIiIiIuBgl3S7KmiPr9ssxkNqZ1DNOiCYfLQZCs/vN9YzzMO9+OH/GmRGJiIiIiIi4DCXdLirnD8bd6omfhx8Ap1JOOSegS7FYoOv/QVRTc/v0XljwENhsTg1LRERERETEFSjpdlE5a7oNA4K9gwEXTLoBPLyh7yfgY8bIrp/g10nOjUlERERERMQFKOl2UTn7dGfaDHvSnZiWSHpmupOiykdQFbhrNlguvFIr34I/5zg3JhERERERESdT0u2ico5ebjOyk26A06mnnRHS5V17i+PAaj88Cbt+dl48IiIiIiIiTqak20XlrOm22XBIul2yiXmW6x6ENo+Z60YmzB8AcVucG5OIiIiIiIiTXHHSnZmZyaZNmzh92kVrX8soSz413afOu3DSDdDhJah3h7medg4+7Q3x250bk4iIiIiIiBMUOukeOXIkH374IWAm3DfddBPNmzcnOjqapUuXFnd85ZZbzppuwyDEJ8S+fTLlpBMiKgSrFXp/ANe0MreTjsNH3ZV4i4iIiIhIuVPopPurr76iSZMmAHz33Xfs3buX7du3M3LkSJ577rliD7C8urimu6JXRfv26ZQy0KrAwwf6zc+eSizpOHx0Oxzf4dSwRERERERESlOhk+4TJ04QGRkJwKJFi7j77rupXbs2Q4YMYevWrcUeYHnl0KfbgGCf7OblLl/TncWnIjzwbY7EOx7mdlfiLSIiIiIi5Uahk+6IiAi2bdtGZmYmixcv5rbbbgMgOTkZNze3Yg+wvMo5enmmzSDMJ8y+HZ8c74yQisaeeJutI8zEuxsc/cupYYmIiIiIiJSGQifdgwYNok+fPjRs2BCLxUKHDh0A+OOPP6hbt26xB1heWS/q0x3pF2nfPpp01AkRXQGfitD/W4hsbG4nHYe5XeHgWqeGJSIiIiIiUtIKnXRPnDiRWbNm8dBDD7Fq1Sq8vLwAcHNzY+zYscUeYHnlME+3Dfw8/PD38AfKYNIN4BsMAxZmD66WkgAf94B/fnNuXCIiIiIiIiXIvSgn3XXXXQ7bZ86cYcCAAcUSkJisOb4OsRkGABF+EZw9c5ZjycewGTasljI2zXpWjfeX98HeZZCeDJ/3gbtmQ73bnR2diIiIiIhIsSt01vbaa68xb948+3afPn0ICQnhmmuuYcuWLcUaXHlmJUef7gtJd1YT83RbOqdSXHyu7kvxqgD3/RfqdDO3M9Pgvw/AulnOjUtERERERKQEFDrpnjlzJtHR0QDExMQQExPDjz/+SOfOnXnqqaeKPcDyKmdNt3FR0g1wLOlYaYdUfDy8oc/H0Pgec9uwwQ9PwpIXzLb0IiIiIiIiV4lCNy+Pi4uzJ93ff/89ffr0oWPHjlSrVo3rr7++2AMsrxxHLzf/jPR1HEytQWiD0g6r+Li5Q8/p4B8Jq6aaZavfgYSD0HOGmZiLiIiIiIiUcYWu6a5YsSIHDx4EcJgyzDAMMjMzize6cizH4OX2Pt1RFaLsZUeTy+BgahezWqHDi9Dt/yCrf/rf38AnPSG5jDafFxERERERyaHQSXfv3r2577776NChAydPnqRLly4AbNq0iZo1axZ7gOWVW445w7KS7otruq8arYbCPV+Ah6+5fWANfNgBTu11blwiIiIiIiJXqNBJ91tvvcVjjz1G/fr1iYmJoUKFCoDZ7Hz48OHFHmB5Zck5T3dW8/KyPFf35dTpDAN/AL9wc/vkbph1Gxz607lxiYiIiIiIXIFC9+n28PDIc8C0kSNHFkc8coGbJXdNd4RfhL3sqku6ASo3h6E/w2d3w4kdkHwC5nSF26dC0/ucHZ2IiIiIiEihFWmi53/++YfHH3+c2267jQ4dOjBixAj27NlT3LGVazlrurOmDPNy8yLYOxi4Svp056ViVRjyE1T9l7mdmQrfPgKLx0FmhnNjExERERERKaRCJ90//fQT9evXZ+3atTRu3JiGDRvyxx9/2JubS/HIOXp51pRhABG+Zm338eTjZNqu0oHrfCpC/2+gxaDsst+nwae9NMCaiIiIiIiUKYVuXj527FhGjRrFq6++mqv8mWeeoUOHDsUWXHlmzVnTnWPq6ki/SGJPxZJpZHL8/HGHft5XFXdPs1l5VGNYNAZs6bB3Ocy8CfrMhcotnB2hiIiIiIjIZRW6pjs2NpYhQ4bkKh88eDDbtm0rlqDEsabblqOmO8ove9qwuKS4Uo3JKVoOhoHfZw+wlnAAPuwEf3wAOT4XERERERERV1TopDssLIxNmzblKt+0aRPh4eHFEZPgWNNts2Unl9f4X2NfP5B4oDRDcp4qreGhpRB9vbltS4cfn4avBkFKolNDExERERERyU+hm5c/+OCDPPTQQ+zZs4e2bdtisVhYuXIlr732Gk8++WRJxFguWRxqurPLqwZUta/vT9xfmiE5V2Blc0qxnyfCmvfMsr+/gbgt0OcjiGzk1PBERERERETyUuik+4UXXsDf35//+7//Y9y4cQBUqlSJiRMn8sQTTxR7gOVRZkICYbv/otXROOL8Qhyal1fxr2JfP3C2nNR0Z3HzgE4vQ5U28O1wSE2AU//Af26F9hOg9XCwFmlAfhERERERkRJR6AzFYrEwatQoDh06REJCAgkJCRw6dIihQ4eyfPnykoix3EndsYNW0ycz6fcPue3Anw5Jd2X/yrhZ3IBy1Lz8YvW6w8PLIKqpuZ2ZBkueg4/vgIRDTg1NREREREQkpyuqFvT398ff3x+A3bt3c8sttxRLUOWdxdPTvu5hy3BIuj2sHlSqUAkwm5cb5XUwseDqMGQJtHkMuNAUf98KmNYWtsx3amgiIiIiIiJZ1BbXBeVMuj1t6Q5ThgFUCTCbmCdnJHMy5WRphuZa3L3M5uYDFkLAhQHmUhNgwVCYPwiSyvFnIyIiIiIiLkFJtwtyqOnOzHQYvRygqn85HUztUqrfCI+sgkZ9ssv+XgDvXwd/LdDUYiIiIiIi4jRKul3Qxc3LMy5KurNquqEc9+u+mE8Q3PkfuPND8KloliWfMKcV+29/OBfv1PBERERERKR8KvDo5QsXLsx3/969e684GDFZvLzs6562dDJsju3Ly+20YQXR6C6o1g5+GA3bvzfLYr+DfSuh86vQuC/kmI5NRERERESkJBU46e7Zs+dlj7EomSkWjs3LM8jIvHTz8nI3bVhB+EdA30/NebwXPQXJJ+H8afjmYdj4KXR9E8LrOjtKEREREREpBwrcvNxms112yczMLMlYyw1rrubljjXdURWicLeY35eopvsSLBZo2BseXQsN78wu37cCZtwAS16A1HPOi09ERERERMoF9el2QRf36U6/qKbb3epOdEA0APsS9pFhyyjV+MoUv1C4azbcNx8qVjPLbBmw+h1zoLW/v9FAayIiIiIiUmKUdLsid3eMC031PfNoXg5QM6gmAGm2NDUxL4jaHWH473DTWHC70Gc+8TDMHwhzusKRTc6MTkRERERErlJOT7qnTZtG9erV8fb2pkWLFqxYseKSxy5duhSLxZJr2b59eylGXPIsFguGuwdg1nRnXtS8HKBWxVr29V2nd5VabGWahw/cMg4e/R1qdsguP7AaPrgZvn0Uzh51WngiIiIiInL1cWrSPW/ePEaOHMlzzz3Hxo0badeuHV26dOHAgfxrbnfs2EFcXJx9qVWrVr7Hl0WGR3bSnW7LXdNdO6i2fX33md2lFtdVIbgG9JsP934JwddeKDRg06fwTnNY/iakn3dqiCIiIiIicnVwatI9ZcoUhgwZwtChQ6lXrx5Tp04lOjqa6dOn53teeHg4kZGR9sXNza2UIi5FF5Jus3m5arqLncUCdbqYTc47vQLegWZ5ehL8+hK8dx38tUD9vUVERERE5IoUeMqwLAMHDmTw4MHceOONV3TjtLQ01q9fz9ixYx3KO3bsyOrVq/M9t1mzZqSkpFC/fn2ef/55brnllksem5qaSmpqqn07MTERyB6N3RXZbDYMD3MwNQ9bBukZuWOt5FcJbzdvUjJT2HV6l8s+i8uzusP1w6DR3ViWvQp/zsFiZELCAfhqEMYfMzA6vQKVmjk7Updhs9kwDEPvnLgcvZviqvRuiqvSuymuqqy8mwWNr9BJ99mzZ+nYsSPR0dEMGjSIAQMGULly5UIHeOLECTIzM4mIiHAoj4iI4OjRvPvVRkVF8cEHH9CiRQtSU1P55JNPaN++PUuXLr3klwCvvPIKL774Yq7y48ePk5KSUui4S4PNZsN2ofbew5ZBYlIy8fHxuY6r6leVHYk7OHj2IPuP7MfH3ae0Q726tHga9+o98V/9Kl6HVgJgOfgHllm3cr52L85ePwqbX8RlLnL1s9lsJCQkYBgGVqvTh4UQsdO7Ka5K76a4Kr2b4qrKyrt59uzZAh1X6KT766+/5uTJk3z66afMnTuXCRMmcNtttzFkyBB69OiBx4Vm0QVluTBKdxbDMHKVZalTpw516tSxb7dp04aDBw/y5ptvXjLpHjduHKNHj7ZvJyYmEh0dTVhYGAEBAYWKtbTYbDZOepkjbHvYMvD08iY8PDzXcfXC6rEjcQcGBmc9z1I1tGpph3r1CQ+HOgux7Y7BsuR5LCfNpvs+O7/Be+9PGDeMgjaPmoOylVM2mw2LxUJYWJhL/xKU8kfvprgqvZviqvRuiqsqK++mt7d3gY4rdNINEBISwhNPPMETTzzBxo0bmT17Nv3796dChQrcf//9DB8+/LKDm4WGhuLm5parVjs+Pj5X7Xd+WrduzaeffnrJ/V5eXnhdSGBzslqtLv0D5MJc3Vl9uvOKtXZwbfjHXP8n4R8ahzcuzQivbnU6Q832sO5DWPoKpJzBkp6MZenLsOEjuOVZaHIPWK/C8QQKwGKxuP7fISmX9G6Kq9K7Ka5K76a4qrLwbhY0tit6gri4OJYsWcKSJUtwc3Oja9eu/P3339SvX5+33nor33M9PT1p0aIFMTExDuUxMTG0bdu2wDFs3LiRqKioIsXvyiwXWgxYMbClp+d5TM7B1Hae3lkqcZUrbh7QehiM2AjXPQyWCwl24iH433CY1ga2/U+DrYmIiIiIyCUVuqY7PT2dhQsXMmfOHJYsWULjxo0ZNWoU/fr1w9/fH4Avv/ySRx55hFGjRuV7rdGjR9O/f39atmxJmzZt+OCDDzhw4ADDhg0DzKbhhw8f5uOPPwZg6tSpVKtWjQYNGpCWlsann37K119/zddff13Yx3B9ntnN9C+VdNepmN3UftvJbSUeUrnlGwxdX4dWQ2DJ87BriVl+Ygf89wFzkLX246HGLeao6CIiIiIiIhcUOumOiorCZrNx7733snbtWpo2bZrrmE6dOhEUFHTZa/Xt25eTJ08yadIk4uLiaNiwIYsWLaJqVbNvclxcnMOc3WlpaTz11FMcPnwYHx8fGjRowA8//EDXrl0L+xguz3KheTmAkWP09Zwqelckyi+KuKQ4tp/ajs2wYbW4bvOLMi+sjjm/9/7V8POLcPB3s/zIRvikF1RrZybf0dc5N04REREREXEZhU6633rrLe6+++58O41XrFiRvXv3Fuh6w4cPZ/jw4Xnumzt3rsP2mDFjGDNmTIFjLcssHtlJtyU97ZLH1Q+pT1xSHMkZyexL3EeNwBqlEV75VrUtDF5s1nj/8hIc22qW71sBH3aAa2+Fm8ZCleudG6eIiIiIiDhdoatF+/fvb0+4Dx48yKFDh4o9KAGrV86a7vyT7ixqYl6KLBao3QkeXg53fgjBOb7s+OdXmN0RPu4B+9c4L0YREREREXG6QifdGRkZvPDCCwQGBlKtWjWqVq1KYGAgzz//POmX6HsshWfJ0aeby9R0Z4k9GVuSIUlerFZodBc8uhZufweCqmTv27MU5nSGj26HfaucFqKIiIiIiDhPoZuXP/bYY3zzzTe8/vrrtGnTBoA1a9YwceJETpw4wYwZM4o9yPLI6uGJ7cK6kc+XGfWC69nXVdPtRG4e0GIANL0PtsyD5W/C6QtdLPYuN5eq/4Kbx0L1ds6NVURERERESk2hk+4vvviCL7/8ki5dutjLGjduTJUqVbjnnnuUdBeTnM3LrfnUdIf4hBDhG8Gx5GPEnorVYGrO5uYBze6HxvfA1v/C8jfg1B5z3/6V8FF3qHoD3DQGqt+k0c5FRERERK5yhc7OvL29qVatWq7yatWq4ZljxG25MjlHL7ek5d9sP6uJeVJ6EgcSD+R7rJQSN3ez1vvRddBrJoTUzN63f5XZ3/vDjrDjR7DZLn0dEREREREp0wqddD/66KO89NJLpOaYxio1NZWXX36Zxx57rFiDK89yJt359ekGaBDSwL6+9cTWkgpJisLNHZrcY/b57j0LQmtn7zu0Fr64B6a3hc1fQqbGRBARERERudoUunn5xo0b+eWXX7jmmmto0qQJAJs3byYtLY327dvTu3dv+7ELFiwovkjLmxwDqVky8k+6G4c1tq9vPr6Z26+9vcTCkiKyukHju6Fhb/j7G7PP9/ELA98dj4VvHoZfJ0Pbx6FZf/D0dW68IiIiIiJSLAqddAcFBXHnnXc6lEVHRxdbQGKyeGQn3dbLNC9vFNoICxYMDDYf31zSocmVsLqZo5036G3O871yChz8w9yXcBB+HAPLXoPrh0GroeAb7Nx4RURERETkihQ66Z4zZ05JxCEXy9G83JqRf9JdwbMCNSvWZNfpXew8vZPk9GR8PVRT6tKsVqjT2Vz2r4GVb8Gun8x9ySfht5dh5VRo1s9MwEOudWq4IiIiIiJSNEUe5vr48eOsXLmSVatWcfz48eKMSXDs0+12mT7dAE3CzKb+NsPGXyf+KrG4pARUbQP9/gvDVkGjuyFr9Pn0JFj7AbzbAr7sB/tXg2E4N1YRERERESmUQifdSUlJDB48mKioKG688UbatWtHpUqVGDJkCMnJySURY7lk8fKyr7tdpk83ZCfdgJqYl1WRDeHOWTBiI7R6EOytFQzY/j3M6QL/uQW2fqVB10REREREyohCJ92jR49m2bJlfPfdd5w5c4YzZ87wv//9j2XLlvHkk0+WRIzlU86kuwA13U3DmtrXlXSXcRWrQbc3YdTf0H4C+Edl7zuyEb4eAm83gVVvw/kzzopSREREREQKoNBJ99dff82HH35Ily5dCAgIICAggK5du/Kf//yHr776qiRiLJdy1nR7FCDprhpQlSCvIAC2HN+CoWbIZZ9vMLQbDU9sMef6jmyUvS/xMMSMhyn14cdn4NRe58UpIiIiIiKXVOikOzk5mYiIiFzl4eHhal5ejCxe3vZ19wI0L7dYLPapw06nnubA2QMlFpuUMndPc67vh1fAgO+gdpfsfelJ8McMeLc5zLtf/b5FRERERFxMoZPuNm3aMGHCBFJSUuxl58+f58UXX6RNmzbFGly55pU9kJpHAZJuUL/uq57FAtVvhPu+hMf+hJZDwN3H3GfYIPY7s9/3jH/B+rmQluTUcEVEREREpAhJ99SpU1m9ejXXXHMN7du357bbbiM6OprVq1fz9ttvl0SM5ZLF27GmuyDNxR2S7ngl3Ve10FrQfQqM3ga3vgAVcrQ+OfYXfPcETKkHPz0HJ/9xXpwiIiIiIuVcoefpbtSoEbt27eLTTz9l+/btGIbBPffcQ79+/fDx8SmJGMsnz+w+3V6ZGWTaDNzdLPme0ii0EVaLFZthY0P8hpKOUFyBbzDc+BS0fRz+/hbW/QcOrTP3pSTAmvdgzftQ8za47iHzT2uRZwoUEREREZFCKlTSnZ6eTp06dfj+++958MEHSyomASzeOZPuNDJsBu5u+Z/j6+FL/eD6/HXyL3af2c3plNNU9K5YwpGKS3D3giZ9zeXwBlg368LUYqmAAbtjzKVidWg1FJr1Ax+9GyIiIiIiJa1QVV4eHh6kpqZiseRf4ypXLufo5Z6Z6WTYCjY4VsvIlvb19cfWF3tcUgZUbg49p8HoWLjtRQiskr3v9F5Y8hz8Xz3436NwaL0GXhMRERERKUGFbmf6+OOP89prr5GRkVES8UgWh+bl6WRk2gp0WqvIVvb1dUfXFXtYUob4hcC/RsITm+CeL6DGLdn7Ms7Dxk9h1q0wsx2s+xBSzzorUhERERGRq1ah+3T/8ccf/PLLLyxZsoRGjRrh5+fnsH/BggXFFlx5ZnF3I9PqhpstE6/MdNIzC1Yb2Sy8mb1f95/H/izhKKVMsLpB3a7mcmIXrP0PbP4CUhPN/Ue3wg+jYckL0OguaDkIKjVzbswiIiIiIleJQifdQUFB3HnnnSURi1wkw8MLt9TkC326C1bT7e/pT93gumw7uY2dp3dyJuUMQd5BJRuolB2htaDr63DbBPhrAayfA4cvdENIT4INH5lLpWbQYhA0vBO8Kjg3ZhERERGRMqzQSfecOXNKIg7JQ4aHJ16pyWaf7gLWdAO0imjFtpPbALNfd/uq7UsqRCmrPP2geX9zidtiJt9b/gtp58z9Rzaay0/PQeM+Zu13ZCPnxiwiIiIiUgYVuk/3rbfeypkzZ3KVJyYmcuuttxZHTHJBhocnAN6FGEgNHAdTUxNzuayoxtD9LXhyO3SfClHZ872Tdhb+/BBm/Av+097sB56e7LRQRURERETKmkIn3UuXLiUtLS1XeUpKCitWrCiWoMRku5B0exZiIDWA5hHNsWCOMK/B1KTAvPzNGu2Hl8ODv0HzB8Ajx5gNh/+E/z2KZUo9/Fe+BPHbnBeriIiIiEgZUeDm5Vu2bLGvb9u2jaNHj9q3MzMzWbx4MZUrVy7e6Mq5TA9zBHPPzHTSMwqedAd4BlA3uC6xp2LZeXonCakJBHoFllSYcjWq3NxcOr4MW/8Lf86FY1sBsKQm4vfXp/DXp1C5BTTrb/b99g5wbswiIiIiIi6owEl306ZNsVgsWCyWPJuR+/j48O677xZrcOVdpqdZ0+2GQUZq7tYF+WkZ2ZLYU7EYGKw/tp5bq6jpvxSBdwC0Ggoth8ChP2H9HIy/FmDJOG/uP7zeXBaPgwY9zQS8aluwWJwatoiIiIiIqyhw0r13714Mw6BGjRqsXbuWsLAw+z5PT0/Cw8Nxc3MrkSDLK1uOubrTzp8v1LmtIlrxybZPALOJuZJuuSIWC0S3guhWGB1f5uyqWfjv/hbLsb/M/RnnzWnINn8BwTWg2f3Q5F4IqOTcuEVEREREnKzASXfVqlUBsBVw6iq5ckaOpDs9qXBJd1a/bgNDg6lJ8fIOJLlRfyrcOhrLsa2w8RPYMh9SE8z9p/bAL5Pg18lQs4OZgNfuDO6ezo1bRERERMQJCj1lGMDOnTtZunQp8fHxuZLw8ePHF0tgAoaXt309PblwI0YHegVSJ7gO209tZ8epHerXLcXPYoFKTc2l42SI/d5MwPcuM/cbNtj1k7n4hkKTe8zm5+F1nRm1iIiIiEipKnTS/Z///IdHHnmE0NBQIiMjseTou2mxWJR0FyOLV46a7uTC1XQDXBd5HdtPbcfA4Pe43+lUrVNxhieSzcMHGt9tLqf3wcbPYNPnkHjI3J98Ata8Zy7XtDJrvxv01uBrIiIiInLVK/SUYZMnT+bll1/m6NGjbNq0iY0bN9qXDRs2lESM5Zd3dtKdUYSku22ltvb1NUfWFEtIIpdVsRrc+hyM3AL3fw0NeoFbjqblh9bBd0/Am7Xh66Gw62fIzHBauCIiIiIiJanQNd2nT5/m7rvvLolY5CIWb1/7emYhm5eD2a/b0+pJmi2N1UdWYxiGQ8sEkRJldYOat5lL8inY8l+z+XnOwde2zjeXChHQ6G6zCXpkI+fGLSIiIiJSjApd03333XezZMmSkohFLubrY18tStLt4+5Di4gWAMQlxbE3cW+xhSZSKL7B0HoYDFsJDy01pyHzDsref+6Y2fR8xr9gWltY9TYkHnFWtCIiIiIixabQNd01a9bkhRde4Pfff6dRo0Z4eHg47B8xYkSxBVfeWX1z1HQnFT7pBrOJ+Zo4s2n5miNrqBFYo1hiEykSiwUqNTOXTv+GXUtg85ew8yewpZvHxP8NMeMhZgLUuNms/a7bHbwqODV0EREREZGiKHTS/cEHH1ChQgWWLVvGsmXLHPZZLBYl3cXIzSe7ptsoQk03QNvKbfm/9f8HwKrDq+hXr1+xxCZyxdy9oN7t5pJ8Cv7+xkzAD629cIABe34zFw8/qNfdTMCr32Q2XRcRERERKQMKnXTv3asmyqXFzc/Pvm6cL1rSXSuoFqE+oZw4f4I/j/1JWmYanm6aL1lcjG8wtBpiLif/Mft/b/nSHAkdID0JtswzF/8oaHQXNLkXIho4NWwRERERkcspdJ9uKT3uOZJuzhd+9HIwWx9kjWJ+PuM8m+I3FUNkIiUo5Fq4ZRyM2ASDf4IWg8A7xxzzZ+Ng9bswvS1M/5e5fvao08IVEREREclPgZPu+vXrc+rUKfv2Qw89xPHjx+3b8fHx+ObogyxXzs0v+/O0FDHpBsepw1YdWXVFMYmUGosFqrSG26fCkzuhz8dQpxtYc4wjcWwrLHkeptSDT3qbNeRpSU4LWURERETkYgVOurdv305GRvZcul9++SVnz561bxuGQUpKSqEDmDZtGtWrV8fb25sWLVqwYsWKAp23atUq3N3dadq0aaHvWVZ4VMhR051a9KS7dVRr+7rm65YyycMb6veAez+HJ3dA1zehcsvs/YYN/vkFFjxozv/9zTDY/TNkpjsvZhERERERrqB5uWEYucoKOwf0vHnzGDlyJM899xwbN26kXbt2dOnShQMHDuR7XkJCAg888ADt27cv1P3KGs8cSbc1pehJd4hPCPWC6wEQeyqWE+dPXHFsIk7jFwLXPQgP/gKPrYcbn4agKtn7087B5i/g0zvhzVqw8HH451fIzLj0NUVERERESohT+3RPmTKFIUOGMHToUOrVq8fUqVOJjo5m+vTp+Z738MMPc99999GmTZtSitQ5vPxzJt2Fb0WQU84m5qrtlqtGaE249XkYsRkG/QjNB4BXjv7f50/Dho/hk17wf7Xhuydgz1Il4CIiIiJSagqcdFssllw12YWt2c4pLS2N9evX07FjR4fyjh07snr16kueN2fOHP755x8mTJhQ5HuXFZ7+2fMSu11B83KAGyrfYF9fdmhZPkeKlEFWK1RtC3e8A0/thD6fQMM7zanGsiSfhPVz4eMe8H914PtRsHc52DKdFraIiIiIXP0KPGWYYRi0b98ed3fzlPPnz3P77bfj6WlOP5Wzv3dBnDhxgszMTCIiIhzKIyIiOHo075GId+3axdixY1mxYoU9jstJTU0lNTXVvp2YmAiAzWbDZrMVKubSYrPZMAwDT7/sebrdU1OuKN4moU0I9AwkIS2BlYdXkpKeoqnDpEiy3k9X/fuDmyfU7W4u6cmw+2csf38Lu37Ckn5h6r3kE/DnbPhzNoZfONS7HaN+L3PgNs0BXma5/Lsp5ZbeTXFVejfFVZWVd7Og8RU46b64ZrlHjx65jrnzzjsLejm7i2vLDcPIswY9MzOT++67jxdffJHatWsX+PqvvPIKL774Yq7y48ePF2ngt9Jgs9lISEjA0zeDNKs7nrYM3FLPEx8ff0XXbRXaip+P/ExSehIxO2JoFdqqmCKW8iTr/TQMA6u1DMw6GNIabmyNpc14PA8sw+efH/E6sAxLhvn335IUD39+iOXPD8n0CSW1WntSanQgrdL1ZgIvZUaZezel3NC7Ka5K76a4qrLybuYcWDw/FiOvEdFKQVpaGr6+vsyfP59evXrZy5944gk2bdrEsmWOTaDPnDlDxYoVcXPLroXK+gbEzc2NJUuWcOutt+a6T1413dHR0Zw+fZqAgIASeLIrZ7PZOH78OBWDQ9jUqg2BacmcCAznhjW/XdF1f97/M08ufxKAPrX78Nz1zxVHuFLOZL2fYWFhLv1LMF9p52DXErMGfHeMPQHPyfAKgNqdMOreDtfeCp5+ua8jLuWqeDflqqR3U1yV3k1xVWXl3UxMTKRixYokJCTkm1sWuKY7L6+++irDhg0jKCio0Od6enrSokULYmJiHJLumJiYPGvRAwIC2Lp1q0PZtGnT+PXXX/nqq6+oXr16nvfx8vLCy8srV7nVanXpH6DFYsHTw51UNy8gGc/0lCuO91/X/AtPqydptjSWHlzKc62fw2px3c9AXJfFYnH5v0P58g6ARneZS+pZ2PkT/P2NOc1YVg14aiJsnY9l63xw94Ga7aHe7VC7E/hUdPIDyKWU+XdTrlp6N8VV6d0UV1UW3s2CxnZFSfe///1v+vTpU6SkG2D06NH079+fli1b0qZNGz744AMOHDjAsGHDABg3bhyHDx/m448/xmq10rBhQ4fzw8PD8fb2zlV+NUn18ITz4JmeevmDL8PXw5fro65nxeEVxJ+PZ9vJbTQMvXo/O5EC8fLPTsDTkmD3LxD7nZmIpyaYx2Sch+3fm4vVHaq1g3oX+o37Rzo3fhERERFxaVeUdF9py/S+ffty8uRJJk2aRFxcHA0bNmTRokVUrVoVgLi4uMvO2X21S/XwBsAzPQ3DZsNyhd/03FLlFlYcXgHArwd+VdItkpOnH9S/w1wy0mDfcoj9Hrb/AEkXxlSwZcCe38zlh6fgmlZQpzPU7gLh9eAKZnUQERERkavPFfXp9vf3Z/PmzdSoUaM4YypRiYmJBAYGXrbdvTPZbDbi4+MJDw/nm1t7Uf/oTgDqrP8Tq9+V9Ss9cf4Et/73VgwMagbV5Jse3xRHyFKO5Hw/Xbm5T7GyZcKhdWYNeOxCOHOJLwODqpjJd53OUPUGcM/dtUVKTrl8N6VM0LsprkrvpriqsvJuFjS3vKKa7m3btlG5cuUruYRcRpqnt309MynpipPuUJ9QGoc1ZvPxzew+s5t/zvzDtUHXXmmYIlc3q5s5nViV1tBxMhzdaibg27+H+G3Zx505AGtnmounP1x7C9TpArU6gl+o8+IXEREREacp9NcGBw8e5NChQwBER0fz559/MnLkSD744INiD04gzdvXvm47d65Yrtm5Wmf7+uJ9i4vlmiLlhsUCUY3h1udg+BoYsQk6vwY1bgarR/ZxaWfNWvFvH4E3asKHHWHFFDi2DZwzaYSIiIiIOEGhk+777ruP334zp646evQoHTp0YO3atTz77LNMmjSp2AMs79K9fOzrxZV0d6zWEQtmv9PFexdfcd98kXItuDq0HgYP/A/G7IG750Lje8AnOMdBBhz8A355Eaa3gbebmP3Bd/5kDt4mIiIiIletQifdf/31F9dddx0A//3vf2nYsCGrV6/m888/Z+7cucUdX7mXnqOmOy0hsViuGe4bTsvIlgDsS9zH9lPbi+W6IuWedwA06AW9Z8LTu2HQYrhhJITVdTzuzH5Y9x/4vA+8Vh0+7gmr34PjO1QLLiIiInKVKXSf7vT0dPu81z///DN33HEHAHXr1iUuLq54oxMyfXIk3Ylni+26nat1Zt3RdQD8uPdH6oXUK7ZriwhmP/Cqbcylw4twao9Zs73jR9i/yhwFHSAzNXs09CXPQWAVc07wmrdBjZvMKc1EREREpMwqdE13gwYNmDFjBitWrCAmJobOnc3+wUeOHCEkJKTYAyzvMn2yB05LOVM8Nd0AHap2wN1ifueyeN9ibIat2K4tInkIrgGtH4EBC2HMXuj7GbQYBIHRjsclHID1c2BeP7MWfG53WDkVjv2tWnARERGRMqjQNd2vvfYavXr14o033mDAgAE0adIEgIULF9qbnUsxqlDBvlqcNd0VvSvSulJrVh5eSVxSHJuPb6ZZeLNiu76I5MM7AOp1NxfDgBM7YVcM7P7ZrAXPTDOPs6XDvhXm8vME8I8ya8GvbW8O3OYbnO9tRERERMT5Cp1033zzzZw4cYLExEQqVqxoL3/ooYfw9fXN50wpCotvjqS7mPp0Z+lSvQsrD68E4Pt/vlfSLeIMFguE1TGXto+ZA6vtW3khCY+B0/uyjz0bBxs/NRcsENXEnJasxs0Q3Ro8vC9xExERERFxlkIn3efPn8cwDHvCvX//fr755hvq1atHp06dij3A8s7qn510ZyQWb9Ldvkp7JrtP5nzGeX7c+yNjrhuDl5tXsd5DRArJ0w9qdzIXgJP/mDXgu2LMGu+MlAsHGhC3yVxWvgXuPmb/8Rq3mIl4eAOwFroHkYiIiIgUs0In3T169KB3794MGzaMM2fOcP311+Ph4cGJEyeYMmUKjzzySEnEWW6550y6zxbPlGFZ/Dz86FC1Awv/WcjZ9LP8duA3OlfvfPkTRaT0hFxrLtc/DOnnYf9qc9C1f5bCsa3Zx2Wch39+NZcYwC/MrAGvcbOZiAdWdk78IiIiIuVcoatBNmzYQLt27QD46quviIiIYP/+/Xz88ce88847xR5geecekD1ycXHN051Tj2t72Ne//efbYr++iBQjDx+zT3fHyfDISnhqN9z5ITS9HwIuSqqTjsPW+fC/R+Gt+vBeK1g0xhw9PaV4W82IiIiIyKUVuqY7OTkZf38zEVyyZAm9e/fGarXSunVr9u/fX+wBlneeOZJuI6n4k+6WkS2p5FeJI0lHWHNkDceSjhHhF1Hs9xGRElAhDBrdZS6GASd2wZ6lZk343hWQlmPwxRM7zWXtTLC4QaWmUP1GqNYOqrQ2m7WLiIiISLErdE13zZo1+fbbbzl48CA//fQTHTt2BCA+Pp6AgIBiD7C88wrIbl5OUlKxX99qsXJHTXOudZth4/s93xf7PUSkFFgsEFYbrn8I7v0CntkLg3+Cm8aag6xZ3LKPNTLh8HqzL/inveHVqjC7M/z2bzNZT0+59H1EREREpFAKnXSPHz+ep556imrVqnHdddfRpk0bwKz1btZMo18XNx9vL5LdzcHNLMnFn3QD3FHjDvv6t7u/xdBcwCJln5uHWYN9yzgY8hM8sw/u+QKuexjC6zsea0uHA2tg2WvwUXd4rSp8dDssfwMO/AGZ6U55BBEREZGrQaGbl991113861//Ii4uzj5HN0D79u3p1atXsQYn4OfpRrK7N74ZqbiVUNIdHRBNi4gWrD+2nn2J+1h/bD0tI1uWyL1ExEm8A6BuV3MBOHfcHA1973JzOfVP9rEZKdnlAB5+ULUtVG9nNkmPbAxWt9z3EBEREZFcCp10A0RGRhIZGcmhQ4ewWCxUrlyZ6667rrhjE8DH040znr6EpiTgnlz8fbqz3FX7LtYfWw/A/J3zlXSLXO0qhEHD3uYCkHD4QhK+AvYug4SD2cemJ5lzhu+OMbe9A6Hqv8wkvNq/ND2ZiIiISD4K/b8km83GpEmTCAwMpGrVqlSpUoWgoCBeeuklbDZbScRYrvl6unPWwwcAt/Q0bCkl09eyQ9UOBHkFARCzP4bTKadL5D4i4qICK0OTe6Dn+zByK4zYBHe8C43uhgqRjsemJMCOH2DxWJjxL3i9Gnx+D6x6Bw6tV3N0ERERkRwKXdP93HPP8eGHH/Lqq69yww03YBgGq1atYuLEiaSkpPDyyy+XRJzllp+nG+c8fe3bmQkJWL29i/0+Xm5e3HHtHXy87WPSbeks/GchAxoMKPb7iEgZYLFAcHVzaf5A9sjo+y40Od+7As6fyj4+JQF2/mguYDZHj74Oqt5gNkuv3AI8iv/3loiIiEhZUOik+6OPPmLWrFnccUf24FtNmjShcuXKDB8+XEl3MfPxdOOsh2PS7RFRMlN63VX7Lj7e9jEAX+38igfqP4DFYimRe4lIGZI1MnpYbWg1FGw2iN9mJuAHVsP+1ZB8Mvv49CRz2rI9v5nbbp5QuaWZgFdtaybkXv5530tERETkKlPopPvUqVPUrVs3V3ndunU5depUHmfIlfD1dOesp49925aQUGL3qh5YnVaRrVh3dB37Evex7ug6rotSX30RuYjVCpENzaXN8As14Tth/yozAd+3Cs4eyT4+M81Mzg+shhWY05dFNbmQhN9gjrLuG+y0xxEREREpSYVOups0acJ7773HO++841D+3nvvOYxmLsXD19ONsxc1Ly9JfWr3Yd3RdQB8vv1zJd0icnkWC4TVMZeWg80k/Mx+M/nev9pMxk/vzT7eyIQjG8xlzXtmWVhdiL7eXKq0huAa5nVFREREyrhCJ92vv/463bp14+eff6ZNmzZYLBZWr17NwYMHWbRoUUnEWK55uVtz9ekuSe2rtCfcJ5z48/H8euBXDp49SLR/dIneU0SuMhYLVKxmLs36mWWJRy4k4BeW47GO5xzfbi4bPjK3fUMvJOHXmUl4VFP1CxcREZEyqdBJ90033cTOnTt5//332b59O4Zh0Lt3b4YPH06lSpVKIsZyzWKxkOFTwb6deaZkk24PNw/uqXsP72x8BwODL7Z/wZhWY0r0niJSDgRUgkZ3mQtA0kk4sMasBT/wOxzdAraM7OOTT5gjpO/4wdx28zQT7yrXZ9eIVwgv9ccQERERKaxCJd3p6el07NiRmTNnasC0UpTmlyPpTkws8fvdVfsuZm6ZSWpmKgt2LWB4k+FU8Kxw+RNFRArKLwTqdTcXgLRks7n5gd/h4B9wcC2knMk+PjMNDq01F941yypWN2vBo68zk/CQ2qX9FCIiIiKXVaik28PDg7/++ksjWpcywz/Avp6ZcKbE71fRuyLda3Tn611fk5SexP/++R/96vUr8fuKSDnm6QvV/mUuYI6QfmLnhQT8wnJyt+M5p/eay+YvALB4VqBiWEMs1dqYifg1LVUbLiIiIk5X6OblDzzwgH2ebikdloBA+3pGCTcvz3J/vfv5etfXAHy67VP61umLu7XQr4uISNFYrRBe11xaDDDLkk6YNeAHfzf/PLwBMlPtp1jSzuF1+Hc4/Hv2dYKqwDWtzCnLrmkFUY3B3auUH0ZERETKs0JnUWlpacyaNYuYmBhatmyJn5+fw/4pU6YUW3BicgvMTrrTTp0ulXvWrFiTNlFtWBO3hkPnDhGzP4Yu1buUyr1FRPLkFwp1u5oLQEYqxG0xk/BD6zAOrcOSeMTxnDMHzOUv80tErB5m4p2VhF/T0hzwTS24REREpIQUOun+66+/aN68OQA7d+502Kdm5yXDO9CfdIsbHkYmGadLJ+kGGNJoCGvi1gAwa+ssOlfrrJ+xiLgOdy+IbmUugGGzcXzPVkJT92E9sh4O/QlHNkJ6cvY5tnQ4vN5c1s40y3xDctSGt4TKzcE7MI8bioiIiBReoZPu3377rSTikHwE+HiQ4OVHaEoimadOldp9r4u8jkahjdh6Yis7T+9kxeEV3HjNjaV2fxGRwrJViIAajaBBD7MgMwPit8GhdWaifWid2Vc8p+STsHOxuWQJqWUm35WaQ6VmZu24h0/pPYiIiIhcNQqcdGdmZvL3339Tq1YtfHwc/+ORnJzM7t27adiwIVartdiDLO/8vT1I8KxAaEoiJJzBMIxSqXG2WCwMbTSUJ357AoD/bPkP7Sq3U223iJQdbu5mwhzVGFoNMcvOnzb7gx/6Ew7/aSbi5y9qRXRyl7lsmWduW9wgvD5UapqdjIfXB3fPUn0cERERKXsKnHR/8sknvPfee/zxxx+59nl5eTF48GBGjhzJ/fffX6wBCgR4u5PgZfadt2RkYDt7FreAgMucVTxujr6ZawOv5Z+Ef9h0fBPrj62nZWTLUrm3iEiJ8KkINdubC4BhwKk9ZhJ+aJ05ddnRreY0ZVmMTDi21Vw2fmKWuXlBZMPs2vDKzSG0NljdSv+ZRERExGUVOOn+8MMPeeqpp3Bzy/2fCTc3N8aMGcN7772npLsE+Ht7kOCVY67uU6dKLem2WqwMaTSEZ1c+C8Csv2Yp6RaRq4vFAiHXmkuTvmZZRhrE/23WiB/ZaC7xsWbynSUzNbt/eBYPP4hqcqE2vJm5BNfQQG0iIiLlWIGT7h07dtC6detL7m/VqhWxsbHFEpQ48vd254hn9ijxGadO41mtWqndv0v1Lry/6X0OnzvMqsOr+Pvk3zQIaVBq9xcRKXXuntlJc5a0ZLMG/MiGC8n4htxzh6cnwYHV5pLFO9BMxKOaQFRT88/ga81p0UREROSqV+CkOykpicTExEvuP3v2LMnJyZfcL0UXcHFN9+nSG0wNwN3qzqAGg5j8x2QA3tv4HtNvm16qMYiIOJ2nL1S53lyypCTAkU1mAn5kIxzeCAkHHM9LSYC9y83Ffq0KENkoRzLeBELrmH3QRURE5KpS4H/da9WqxerVq2ncuHGe+1euXEmtWrWKLTDJ5u/t7pB0Z5TiCOZZetXqxYd/fUhcUhwrD69kY/xGmoU3u/yJIiJXM+9AqHGTuWQ5dxziNmXXhh/ZCOeOOZ6Xdg4OrDGXLO7e5uBsORPx8Prg4V0qjyIiIiIlo8BJ93333cfzzz9P27ZtcyXemzdvZvz48YwZM6bYA5Ss0cuzm5dnniq9ubqzeLp58kiTRxi/ejwA72x4h9mdZmskcxGRi1UIg1odzCXL2aMQtwWOboa4C8uZi2rEM1IuJOkbssus7hBWzzERj2wIOf5NEBEREddW4KR71KhR/Pjjj7Ro0YLbbruNunXrYrFYiI2N5eeff+aGG25g1KhRJRlruRXg437RQGonnRLH7dfezuy/ZrMvcR9/HvuTNXFraFuprVNiEREpU/wjzaV2x+yy5FNwdEt2Eh63GU7+AxjZx9gyskdN3/TphUILhNaCiIZmAh7RyPzTP0oDtomIiLigAifdHh4eLFmyhLfeeovPP/+c5cuXYxgGtWvX5uWXX2bkyJF4eHiUZKzlVoCPB6e8/O3bGSeck3S7W90Z3nQ4Y5abLRre3fAubaLaqLZbRKQofIOhxs3mkiX1LBz9yzERP77dcdR0DDix01z+XpBd7BNs9hOPbJSdkIfW0VziIiIiTlaoEVs8PDwYM2aMmpGXMn8vdxJ9s6cIyzh+3GmxdKrWiVlbZ7Hz9E7+OvkXvx38jVur3Oq0eEREripe/lC1jblkST8Px7aZ/cSPbjEHbouPNacsy+n8Kdi7zFyyWD0grE6OWvGGZlLuF1oaTyMiIiIUMukW57BYLPgEVCDZ3QvfjFQyTpxwWixWi5XHmj7GiN9GADB1w1TaXdMOD6taOYiIlAgPH7imhblkyUw3pys7utVcjv1l1pAnxTuea0s39x37C7bkKPePyp2Ih9QEq1upPJKIiEh54vSke9q0abzxxhvExcXRoEEDpk6dSrt27fI8duXKlTzzzDNs376d5ORkqlatysMPP1wu+pJX9PXktJe/mXQ7saYb4Obom2ka1pRNxzexN2Ev83fM57569zk1JhGRcsXNA8LrmUvjPtnl5+Idk/CjW81m6A7N04GzceayOya7zN3brBUPr5+9RNRXX3EREZEr5NSke968eYwcOZJp06Zxww03MHPmTLp06cK2bduoUqVKruP9/Px47LHHaNy4MX5+fqxcuZKHH34YPz8/HnroISc8Qemp6OfJKW9/KiedwHb2LLaUFKzezplGxmKxMKbVGO5bZCba0zZPo1uNbgR6BTolHhERuaBCONRsby5Z0lPMfuFZifixC8l4yhnHczNSsvuR5+QddCEJr2cm4VnrPhVL+mlERESuCk5NuqdMmcKQIUMYOnQoAFOnTuWnn35i+vTpvPLKK7mOb9asGc2aZc8NXa1aNRYsWMCKFSuu+qQ7+EJNd5aMEyfwvOYap8XTKKwR3Wt05/s935OQmsCMzTN45rpnnBaPiIhcgoc3VGpqLlkMAxIO5UjEt5r9xk/9A4bN8fyUM3Bgtbnk5F/pQhJeD8IbmH+G1TGbw4uIiIhdoZPupUuXcvPNN1/xjdPS0li/fj1jx451KO/YsSOrV6++xFmONm7cyOrVq5k8efIlj0lNTSU1NXuwmcTERABsNhs2m+1SpzmVzWbDMAyH+IJ8PTjlnT2YWvqxY7hXquSM8OxGNB3Bz/t/JiUzhS+3f8ndte+mWkA1p8YkJS+v91PEFejdLKSAyuZSq1N2Wfp5OLEL4rdhid8G8dvgeCyWxCO5zz97xFx2/2wvMixWCK5h1oaH1cOIMP8kuLo553g5pXdTXJXeTXFVZeXdLGh8hf4XsHPnzlSuXJlBgwYxYMAAoqOjCx0cwIkTJ8jMzCQiIsKhPCIigqNHj+Z77jXXXMPx48fJyMhg4sSJ9pryvLzyyiu8+OKLucqPHz9OSkpKkWIvaTabjYSEBAzDwGq1AuBFukPSffKff/CsXNlZIQJgwcJd1e7i038+JcPI4JXVr/BS85ecGpOUvLzeTxFXoHezmLhFQlQkRGXPTGFJTcD91C7cT+3E/dQuPE7txP3kTqxpiQ6nWgybOcDbyd0Qu5CsnuCGmycZQdXJqFiTjIrXXvizJpkBVcz+6Vc5vZviqvRuiqsqK+/m2bNnC3RcoZPuI0eO8OmnnzJ37lwmTpxI+/btGTJkCD179sTTs/BzgV48x7NhGJed93nFihWcO3eO33//nbFjx1KzZk3uvffePI8dN24co0ePtm8nJiYSHR1NWFgYAQEBeZ7jbDabDYvFQlhYmP0lqxyWxFrv7OblfqmpVAwPd1aIdo9WfJSfjvzE8fPH+f3478SmxXLTNTc5OywpQXm9nyKuQO9mSQqH6FpA1+wiw8B2Ns6cvix+G5bjsRdqxndgyXD8UtuSmYbHyR14nNzhUG5Y3c1R00PrQFgdjLC6ZhP14GvB3asUnqt06N0UV6V3U1xVWXk3vQs4xlahk+7g4GBGjBjBiBEj2LRpE7Nnz+bRRx/lkUceoV+/fgwZMoQmTZpc9jqhoaG4ubnlqtWOj4/PVft9serVqwPQqFEjjh07xsSJEy+ZdHt5eeHllfsfbqvV6tI/QIvF4hBjiJ8XJ3yC7Psz4+NdIv4KXhV4suWTjF1hdhN4de2rtK7UGh939em7ml38foq4Cr2bpSzoGnOp3SG7zJYJp/aaCXj8Njj2tzmC+sndYMtwON1iyzAHeTu+HWKx14xjcTObqYfVgbC6F5Y6EFqrzPYZ17sprkrvpriqsvBuFjS2K+pg1bRpU8aOHUtwcDCvvvoqs2fPZtq0abRp04YZM2bQoEGDS57r6elJixYtiImJoVevXvbymJgYevToUeAYDMNw6LN9tQr28+S4d/bo4Olx+TfBL01dq3flm93f8EfcHxxJOsLMzTMZ2WKks8MSERFnsLpBaE1zqX9HdnlmOpzaY9aMH99xIdneASd3QWaa4zWMTLP85C7Y/n2OHRaoWC170LawuhBa20zGcww2KiIi4kqKlHSnp6fzv//9j9mzZxMTE0PLli157733uPfeezl16hTPPPMMd999N9u2bcv3OqNHj6Z///60bNmSNm3a8MEHH3DgwAGGDRsGmE3DDx8+zMcffwzA+++/T5UqVahbty5gztv95ptv8vjjjxflMcqUin6enPDJTrozLtPvvTRZLBaev/55ei/sTbotnY/+/ojuNbpTs2JNZ4cmIiKuws3jQqJcx7E8MwNO78uu8T6+A47HmgO6ZVw89ooBp/eay45Fjrv8K5nJd2it7EQ8pJY5WJwL15KIiMjVr9BJ9+OPP84XX3wBwP3338/rr79Ow4YN7fv9/Px49dVXqVat2mWv1bdvX06ePMmkSZOIi4ujYcOGLFq0iKpVqwIQFxfHgQMH7MfbbDbGjRvH3r17cXd359prr+XVV1/l4YcfLuxjlDlh/l6kuntx1sMH//TzpB875uyQHFQLrMbQRkOZvnk6GUYGL/3+EnM7z71s/3wRESnn3Nyza8brdc8ut2XCmf2OteJZf6Yn575O1mjqe5c5lnv4Xug3XtsxKQ++Fjx9S/bZREREAIthGEZhTmjfvj1Dhw7lzjvvvOTAaRkZGaxatYqbbnK9AbUSExMJDAwkISHBpQdSi4+PJzw83N5PIDUjkzrPL2bar/9H9cQ4LB4e1Nm8CYsLfXufmplK7//15sBZ84uS8W3Gc3ftu50clRS3vN5PEVegd7OcsNkg8ZCZfMfHmv3FT+wy/zx/qhAXskBgtGPNeNZ6hQgoxi+N9W6Kq9K7Ka6qrLybBc0tC1XTnZ6eTpUqVbj++uvzHanc3d3dJRPusszL3Y0gXw9O+ARSPTEOIz2dzNOncQ8JcXZodl5uXjzX+jkejjFbHry57k1uqHQDlSo4dz5xERG5ilitEFTFXGp1cNyXdNLsB35i54Vlt/nn6X1mP3EHBiQcMJd/fnHc5RWQ3Tw9KxkPqWkO7lZGB3ITERHnKVTS7eHhwTfffMMLL7xQUvFIPsIqeHE8xwjm6XFHXSrpBmhbqS29a/Vmwa4FJGckM3H1RGZ2mKlm5iIiUvL8QsylSmvH8oxUc0R1e0Ke48/UxNzXSU2Ew+vN5WIB10BIjQtJ+LXmnyHXQlBVcC/81KkiInL1K3Sf7l69evHtt986zH0tpSM8wIt4n4r27fRDB/FpeOkR4p3lqZZPserwKo4lH2NN3Bq+2vWVmpmLiIjzuHtBeF1zyckw4Fz8hSnNduVIxnfCmYNAHj3wEg+Zy97ljuUWN7P2PaRmdiIecq2ZmPurxZeISHlW6KS7Zs2avPTSS6xevZoWLVrg5+fnsH/EiBHFFpw4CqvgRZxfds122oGDTozm0vw9/Xmx7YsM+9kchV7NzEVExCVZLOAfYS7V2znuSz8PJ/+5kJD/Y84zfurCn+dP576WkZk9svruGMfbuHkREhCNJbzOhWS8Zvafxdx/XEREXE+hk+5Zs2YRFBTE+vXrWb/esdmVxWJR0l2CwgO8+TtH0p1+8EA+RzvXDZVvcGhm/sKqF/igwwe4Wd2cHZqIiMjlefhAZENzuVjyqdyJ+Ml/zCU9KdfhlsxUPE7vhtO7c1/Ls4LZVzwrCQ++1twOrgF+oUrIRUSuAoVOuvfu3VsScUgBhPuXjZruLE+1fIrVR1ZzNOkoa4+uZe7fcxnSaIizwxIREbkyvsHmEt3Ksdww4NyxHEn4bji1B+PCn5bMtNzXSjsHR7eYy8W8AiC4+oUkPEcyHnIt+IUpIRcRKSMKnXSL84T5/397dx4fRXn4D/wzM3tmc98JhBvklktRBOVbAQUvag9rPWuPn61aEf1arbWiraJtQawWr1qrtR5fFa1ataIFPEBADkVBOUM4EnJnd5O95/n9MbuzZy5Ispvk83695jUzzzwz+2x8iPk8c5nhNKXp7+r2pvCZbkC7zPy+mffhx//5MQQEHtn2CKaXTMf4/ARnDYiIiHo7SQIyirVpyEy9WKgqqqsqUWjxQW7YHz4rXrdXmxorEjxdHdoD3So/16ZYpvRgII8J47nDeMk6EVGKOa7QffjwYbzxxhuoqKiA1xs9art8+fIuaRjFG5Ctvaak0paHjMbD8FdWQfV6Ibfx+rZkO6X4FPxkwk/w5I4n4Rd+/OrDX+HlC15GmjEt2U0jIiLqObICZJcAuUOA4d+K3ub3Ao0HtSBevw+o369NdfuApkOAUOOP53UCVTu0KZbRFgziQ6PDeO4wIKOEgZyIqId1OnR/8MEHuPDCCzF06FB88803GD9+PMrLyyGEwJQpU7qjjRRUoofufIxqPAwIAd/hIzAPG5rklrXt55N+jk8rP8WO2h2ocFRg6aal+N0Zv0t2s4iIiFKDwRR+H3gsv1c7Ex4bxuv3t36G3NcMHNuhTXGfZQ0H8rzhQM4QIGeoNs8qAxReBElE1NU6/Zv19ttvx80334x77rkHGRkZePXVV1FYWIjLLrsM5557bne0kYKKMsyQJUTd1+07VJHyodsoG3H/rPvxvTe/hxZ/C17f+zpmDpiJc4ack+ymERERpTaDCcgfoU2xAr5gII8J4/X7tTPnqj9+H78LqP5Km2JJCpBdFh3Ec4PznCGAJatrvxsRUT/R6dC9a9cuvPDCC9rOBgNcLhfS09Nxzz334KKLLsLPf/7zLm8kaQyKjKJMS696mFrIoMxB+PX0X+M3n/wGAHD3hrsxMX8iStJLktwyIiKiXkoxht8HHivg0y5Nr9sfDuKhs+UN5YkDuQho2xrKAayN327NDQdwPYwH55ml2iX0REQUp9Oh22azwePxAABKS0uxb98+jBs3DgBQW1vbta2jOKXZ1ujQneIPU4t04fAL8fGRj/Fu+btweB343w//F0+f8zSMijHZTSMiIupbFGP4Pu5YAb8WyEMBvKE8+I7xcqC+HPA6Eh/TVa9NR7cm+DwTkD0oOoiHwnn2YMCc3kVfjIio9+l06D7ttNPwySefYOzYsTjvvPNw8803Y8eOHVi1ahVOO+207mgjRSjJsuDTyMvLe8mZbkB7j/udp9+JL2q+wNHmo/i85nP86bM/4fbptye7aURERP2HYgg+ZC3B7WlCaO8h14N4RBhvKAfsRwCI+P0C3vDT2BOxFSY4Sz5EC+QZJYAsd8lXIyJKRZ0O3cuXL4fT6QQALFmyBE6nEy+99BJGjBiBBx98sMsbSNEGZFtRZ8mEVzbApPrhPdR7QjcAZJoysWz2Mlz5zpXwqT48//XzOLngZCwYtiDZTSMiIiJJAmx52jRwavx2v0e7jzx0hrz+QPSZcl9L4uM2V2vT4U3x22Sjdi959uDg2fLBweXB2jLfSU5EvVynQ/ewYeHLlNLS0rBy5coubRC1rSw3DUKScdSWhyGOY/BWVED4fJCMvecS7fH543H79Ntxz4Z7AABLNizByJyRGJmT4KmtRERElDoM5taftC4E0FwTEcTLw2G8oRxwVCY+puoL33ee8DOtEWF8UHw4t+YwlBNRSjvu90J4vV5UV1dDVaPfHTlo0KATbhS1bnCe9n7rg5klGOI4Bvh88B48CPOIBE81TWHfHfldfFHzBV7f+zpcfhcWr12MF857Aekm3vNFRETUK0kSkF6oTYOmx2/3tkScJQ+G8cYKoOGg9rR1rzPxcf0uoPYbbUrElBFxdjwmnOcMBswZXfUNiYiOS6dD9+7du/HjH/8Y69evjyoXQkCSJAQCCd4XSV1mcK4NAFCeWYyzjmhlnj17el3oliQJd0y/A9/Uf4Nd9btQbi/HnZ/cieWzl0PiaDUREVHfY0oDCkdrUywhAFdDOIg3HgyG8eByYwXgdyc+rtcBHPtSmxKx5sSfHc8q0y5pzyoDLJld9hWJiBLpdOj+0Y9+BIPBgLfeegslJSUMSD2sNNsCgyzhYEaxXubZsweYPz+JrTo+FoMFy2YvwyVvXQKH14H3K97HU18+hZ9M+Emym0ZEREQ9SZKAtFxtGjAlfrsQgLM6HMAbyiOWDwJNh7XL1BNxNWhT5fbE281Z4QCeXQZkDQwuD9KWbYV80BsRnZBOh+7t27djy5YtGD06wSgldTuDImNAjhXl9sjQ3cqTQnuBsowy3D/rflz3wXUAgD9v/TOGZA7BnMFzktwyIiIiShmSBGQUaVPZqfHb1YB2z3jk2fHIZfsRQKjx+wGApwk41tT6mXLFFA7ikWfIswZqy5kDAYOp674rEfU5nQ7dY8eO5fu4k2xwng0f1+bCrRhhCfi0M9292JkDz8T1k67HI9sfgYDA7R/djhJbCcblj0t204iIiKg3kJVgMB4I4Iz47QGfdjY8dHa86TDQeEh7X3ljhRbKVX/iYwe8bT/oDRKQURxxhrwsPqDzEnaifq3TofuBBx7Arbfeivvuuw8TJkyAMeap2ZmZ/KXS3UYUpOPD3TWoyCjCqMbD8FZUQHW7IVssyW7acfvZxJ+h3F6Ot/a/BXfAjRv+ewOeP+95FNuK29+ZiIiIqC2KsfV3kwPamXLnsXAQbzoUXD4cXvY6Wjm40M6yOyqBw5sTVwldwp45AMgaEJwPBDJLteXMAYCx9/4dR0Rt63TonjNHu+z37LPPjirng9R6zqgi7QnfBzOKMarxMKCq8O7fD8vYsUlu2fGTJAl3z7gbR5xHsK16G2pcNbj+g+vx7PxnkWZMS3bziIiIqC+TlWAALgWQ4MnrQgDuppgwXhER0g9rob017V3CDgBp+cFAPjAczCNDemapNnhARL1Op0P3mjVruqMd1AmjirVXXxzMjH6YWm8O3QBgUkxY8T8rcNm/L8Nh52F80/ANbll3Cx761kMwyvyfDBERESWJJAHWbG0qnpC4js+tXaauB/NgGG+sCC4faf1hbwDQUqtNlZ+31gggvSjxmfKsgdo8o1gbQCCilNLp0H3WWWd1RzuoE0YWame6yyNCt3v3bmQlq0FdKNeSi7+c/Rdc/vblcPgc+OjIR7jrk7vw+5m/hyzxyaFERESUoowWIG+4NiWiqkBzDWA/rAVw+xEtlNuPAPajWpmjEhCtXTUqAGeVNh3ZkriKpAAZJdFnx0OBPFRmK2AwJ+phHQrdX3zxBcaPHw9ZlvHFF1+0WXfixIld0jBqXYbFiNIsC/a7S/Uy95dfJbFFXWtY9jCs+J8VuPb9a+FTfXhz/5vIseTglmm38BV1RERE1DvJcvgJ7AOmJq4T8GuXqUcG8qYjWlAPBXPnMQAi8f4iEKx7uI12GLQz5pmlkDJKkKFkAUXDtVCeUaIF9YwS7b3qRNQlOhS6J02ahKqqKhQWFmLSpEmQJAlCxP9j5z3dPWdUcQbWNrlRbc1GoasR7i+/hAgEICl9Y+Ty1JJT8Ycz/4Cb190MVah4duezyLHk8B3eRERE1HcpBi38Zg1I/Go0APB7tTPikYE8dOY8VNbSxpuGVL9eVwJgA4BEt5pbsoCMUiCzJGJeEjyDHixLy+M7zIk6oEOh+8CBAygoKNCXKflGFWVg7Tc12J1ThkJXI9TmZnj374d55MhkN63LzBk8B3eedifu3nA3AOChrQ8h25yN7476bpJbRkRERJQkBhOQM1ibWhO6vzw2mDsqtTPm9qNtB3NAe3Ccuwmo2dV6HdkYDOIl0WfJ9XkwnPPJ7NTPdSh0Dx48OOEyJU/ovu5vcgZh5tEdAADXFzv6VOgGgO+O+i4aPY14aOtDAIB7NtwDo2zERSMuSnLLiIiIiFJUe/eXA4DfA9VeiYaDXyHH4IbsrNLCuKMSsFcCjqPaPOBp/RiqT3uKe1NF2+2x5mjhO6NIC+PpRdpD3zKKgfRirTy9mOGc+qxOP0itrq4OeXl5AIBDhw7hySefhMvlwoUXXohZs2Z1eQMpsZOCTzD/JrtML3Pt+ALZ37k4WU3qNj8e/2M0uhvxzM5nICBw5yd3AgCDNxEREdHxMpiB7EHweS1AYWHiy8SFAFrqwwE8bh48c+6qb/uzXA3aVN3OM4gs2fFhPFFI5/3m1Mt0OHTv2LEDF1xwAQ4dOoSRI0fixRdfxLnnnovm5mbIsowHH3wQr7zyChYuXNiNzaWQEYXpkCRgb/ZAqJAgQ8DVzkPueitJknDztJvhVb144esXGLyJiIiIeoIkAbY8bWrtVWmAdjl7KIAnnFdqT10PeNv+PHejNtV83XY9c2YwgBdFzEtilosAc0ZnvzFRt+hw6L711lsxYcIEPPfcc3juuedw/vnnY8GCBfjrX/8KALjhhhtw//33M3T3kDSTAaMKM/DNMaAiswhD7FXwfLMbqtsN2dL3Ls2RJAm3n3o7ADB4ExEREaUSowXIHapNrRFCO9vtqNKCuPNYcDn4GjTHsXC5393253ns2lS7u5122SLOkEeE8fRiIL1QK0sv0i5/5wPhqBt1OHRv3rwZ//3vfzFx4kRMmjQJTzzxBH7xi19ADnbQG264Aaeddlq3NZTiTR6UjW+OOfBN9iAMsVcBgQDcX32FtKmtvIail2steLv9blwy+pIkt46IiIiIWiVJQFquNhWNbb2eENoD3BKFcUdVcF6plfua2/5MXzNQv0+b2mybEgzhwSBui1hOj1k2Z2rfhagTOhy66+vrUVxcDABIT0+HzWZDbm6uvj0nJwcOh6PrW0itmjwoGy9uPoSdeUNwTsUmAEDLpk19NnQDiYP37zf+Ho2eRvxs4s/4Hm8iIiKi3kySAGu2NhWObruuxxFxtjwUxqtizqIf086Kt0UEgvtWtt8+g0UL37bIUF4EpBeEz5yHtvPecwrq1IPUYgMNA05yTR6UAwD4PD/8ZMrmTzci/+c/T1aTekQoeFsNVvzty78BAB7Z/ggaPY3431P+F7LEy4OIiIiI+jxzhjblt/P2Hm9zdDB3VkdMx4JTNdBcowXwtvjdQGOFNrXHlJEglEdc1m4rCM8Npo5/b+p1OhW6r776apjNZgCA2+3GtddeC5vNBgDweNp4nQB1i+EF6Ug3G3AMeai15SK/uR6ubdugejyQg/+d+ipJknDT1JuQZc7Cg1seBAA8t+s52L12LJmxBEbZmOQWEhEREVFKMNnaf4UaAKiq9iR2PYjXxITyiKDeUtf+53odQL2j/cvbAe2+cltBcMrXzpTry8Hy9EJtnZe49zodDt1XXXVV1Prll18eV+fKK6888RZRhymyhJPLsvDJ3jpszRuOec31EF4vXNs/h236qcluXo+4Zvw1yDZn4+4Nd0MVKt7Y9wZqWmqwbPYyZJj4xEoiIiIi6iBZDobcfKBoXNt1Az6guTYcyJ3HokN55Nl0T1P7nx16rVp7D4cDAMUUEdALosN5KJiHytPyeRY9BXQ4dD/99NPd2Q46TpPLcvDJ3jp8nj8C8yo2AwBaNm7sN6EbAC4eeTEyTZm49cNb4VN92FC5AVe+cyVWnr0SJeklyW4eEREREfU1ihHILNGm9vhc4cvXI8+cR17e3lyjhXivs/3jBbyA/Yg2dYQlOzqcpyc4i24LhnVLFs+id4NOXV5OqWfqYO2+7i8KIu7r3rgRBbghWU1KijmD5+DJeU/ixjU3osnThL2Ne/HDt3+IR771CMbltzNSSURERETUXYxWIGewNrXH2wK01GqXtjdHTrXamXR9OThv7x50IPz+87o97deVjdHhPC145j8tLziPWeel7h3C0N3LnTo0F0ZFQq01G8cyC1Fkr4br888RsNuhZGYmu3k9amrRVPxzwT/xi/d/gQpHBWpdtbj63atx78x7MW/IvGQ3j4iIiIiobaY0wDQIyB7Ufl1V1S5J14N5dUQgD4ZyZ0RQ93bgTVOqD3Ac1aaOUExaAE/LB2x5EaE80Xq+dta9H74TnaG7l7OZDZg8KAebDtRjQ/5JWGivBvx+OD/6CFnnnZfs5vW4wZmD8c8F/8SNa27E1uqtcAfcuHndzfhJ/U9w/aTrochKsptIRERERHTiZFkLtrY8AO28Xg3QLnOPOnNeEwzltQnOqHfwLHrA2/HXrQHaO9HTctsP6tZcyC4Aai4g9/570pMeuleuXIk//vGPqKysxLhx47BixQrMmjUrYd1Vq1bh0Ucfxfbt2+HxeDBu3DgsWbIE55xzTg+3OrXMGpGPTQfqsbFkLBbu/wgA4Fyztl+GbgDItmTjyXlPYsn6JXhz/5sAgL/u+Cu+rv8a98+6H1nmrCS3kIiIiIiohxmt2hn0jp5FdzeGg3hLbXBe18p6nXaWvD0iEA74bZABFAIQI+cBl73ckW+X0pIaul966SUsWrQIK1euxBlnnIHHH38c8+fPx86dOzFoUHxn+PDDDzF37lzcd999yM7OxtNPP40LLrgAGzduxOTJk5PwDVLDzJH5WLZ6N3bkDYPHnAazpwXOjz6C8PshGZI+rpIUJsWEe2feizF5Y7Dss2UIiAA+PvIxLv33pVjxPyswKmdUsptIRERERJSaZDl4RjoXKDip/fpCAB57O8E8Zt3vav+4afkn/l1SgCSEEMn68OnTp2PKlCl49NFH9bIxY8Zg4cKFWLp0aYeOMW7cOFxyySX47W9/26H6drsdWVlZaGpqQmaK3vOsqiqqq6tRWFgIuQP3PARUgcn3vAe72487t/4TMyq2AQAGPfsMbKf2n6eYt2ZT5Sbcsu4WNHgaAABmxYzbTr0N3xn5HUh88EOndbZ/EvUU9k1KVeyblKrYNympQg+NSxDMRXMtPA1HYBq7APL0nyW7pa3qaLZM2mlQr9eLLVu24LbbbosqnzdvHtavX9+hY6iqCofDgdzc3FbreDweeDwefd1ut+v7qqp6HC3vfqqqQgjR4fZJAGYMz8O7Xx3DxwVj9NDt+OADWKdN68aW9g7Tiqbh+QXPY/G6xdhVvwuegAd3b7gbnx79FL897bdIN6Unu4m9Smf7J1FPYd+kVMW+SamKfZOSymABMgdqUwxVVdFQU4OCggLtUvcU1dF/O0kL3bW1tQgEAigqKooqLyoqQlVVVYeOsWzZMjQ3N+P73/9+q3WWLl2Ku+++O668pqYGbre7c43uIaqqoqmpCUKIDo86Th9oxbtfAZuLRkOVFchqAI3/fhviqqsgceQSBhjwpyl/wmPfPIY3D2n3ef/n4H/wRfUXuOPkO3BSVgcumyEAx9c/iXoC+yalKvZNSlXsm5SqekvfdDg68ER4pMCD1GIv7xVCdOiS3xdeeAFLlizBv/71LxQWFrZa7/bbb8fixYv1dbvdjrKyMhQUFKT05eWSJKGgoKDDnezbmTm47/2DcJrSsKN0NE4+/BVEbS0yDh9B2rSp3dzi3uP3Jb/H7IOzsWTDEjh8DlS6KrFo0yL8ctIvcfmYy/l08w44nv5J1BPYNylVsW9SqmLfpFTVW/qmxWLpUL2khe78/HwoihJ3Vru6ujru7Hesl156CT/+8Y/x8ssvY86cOW3WNZvNMJvNceWyLKf0f0BJkjrVxuw0M2aOyMeab2rwn6KJOPnwVwAA57vvIv3UU7qzqb3OvKHzMDZ/LG798FbsqN0Bv+rH8q3LsfbwWvz+jN+jLLMs2U1MeZ3tn0Q9hX2TUhX7JqUq9k1KVb2hb3a0bUn7BiaTCVOnTsXq1aujylevXo0ZM2a0ut8LL7yAq6++Gs8//zzO66evxGrN/PElAICNxeMQMGjvs7P/5z8Qfn8ym5WSBmYMxDPnPoMfjfsRJGhXVmyt3orvvPkd/N83/4ckPl+QiIiIiIj6kKQOGyxevBh//etf8be//Q27du3CTTfdhIqKClx77bUAtEvDr7zySr3+Cy+8gCuvvBLLli3DaaedhqqqKlRVVaGpqSlZXyGlzB1bBEWW0GK0YGvpWABAoK4OzRs2JLllqcmoGLF42mL87Zy/YUD6AACAy+/C7z79Ha59/1pUNXfs2QJEREREREStSWrovuSSS7BixQrcc889mDRpEj788EO8/fbbGDx4MACgsrISFRUVev3HH38cfr8f1113HUpKSvTpxhtvTNZXSCk5NhP+5yTt/vZ3iifp5Y0vv5KkFvUO04qn4dULX8X3Rn1PL1t/dD0uev0iPLfzOQTUQBJbR0REREREvVlS39OdDH3xPd2RVu88hp8++xkUNYCXPrgPtuYmwGDAyHVrYcjL66YW9x0fH/kYd31yF6pd1XrZ2LyxWHL6EozJG5PElqUOvtOTUhX7JqUq9k1KVeyblKp6S9/saLZM3W9Ax+V/TipAQYYZAVnBOwOmaIV+P5pe/1dyG9ZLzBwwE68tfC3qrPfOup34wb9/gD9u/iNafC1JbB0REREREfU2DN19jEGRcfEU7f7kdwadqpc3/t//QaTwi+VTSaYpE789/bd4dv6zGJ41HACgChXP7nwWF7x2Ad7c9yYftEZERERERB3C0N0HXXrKIEgScDS9ADuLRwIAvAcPovnjj5Pcst5lcuFkvHzBy/jl5F/CJGtPg692VePXH/8aV7xzBb6s/TLJLSQiIiIiolTH0N0HDcm3Ye4Y7V3nLw8+Qy+v//szyWpSr2VUjPjpxJ/itYtew+yBs/Xyz2s+x6X/vhR3fnInalpqktdAIiIiIiJKaQzdfdRPzxwGANhUPBa1mfkAgOb16+HZsyeZzeq1BmUOwsNnP4zH5jyGYVnD9PLX976OBasW4KGtD8HutSexhURERERElIoYuvuoaYNzcHJZNlRJjjrbXffU35LYqt7vjAFn4JULX8GvTvkVMowZAAB3wI2/7vgr5r86H09/+TTcfneSW0lERERERKmCobuPkiQJ183WHgK2etApaDanAQCa3nwT3oh3n1PnGWUjLh97Od66+C1cPuZyGGUjAMDutWP5luU477Xz8OruV+FX/UluKRERERERJRtDdx82d2wRJg7MgstowatDZ2mFgQBqH388uQ3rI3ItufjVqb/Cm99+ExcOvxASJABAdUs1lmxYgvNfOx+v7H4FvoAvyS0lIiIiIqJkYejuwyRJws3zTgIA/Gv4TLSYrACAptf/xbPdXWhA+gDcO/NevHrhq5hdNlsvP+I8grs33I35q+bj+V3P87JzIiIiIqJ+iKG7jztzZD6mD81Fi9GKV4eFz3ZXL1ue3Ib1QSNzRuLhbz2Mf8z/B2aUztDLj7Ucw9JNSzF/1Xw889UzaPY1J7GVRERERETUkxi6+zhJknDXBeMgS8Brw89EozkdAOD4z3/QsnVrklvXN00qnITH5z6O5xc8H/WasVpXLf702Z8w5+U5WPbZMlQ1VyWvkURERERE1CMYuvuBsaWZuGz6YLiMFjw7+hy9/NjS+yECgSS2rG+bUDABD5/9MF6+4GXMHTxXL3f6nPj7V3/Hua+ei1s/vBVf1X6VxFYSEREREVF3YujuJ26eNwr56Wb8Z/CpKM8oAgC4d+xAw/MvJLllfd/o3NFYPns5Xr/odVw88mL9aecBEcA7B97BD/79A1z1zlV458A7fOgaEREREVEfw9DdT2SnmfD7heOgygoeOfk7ennNgw/CV1mZxJb1H8Ozh+PuGXfjve++h2tPvhY55hx929bqrbj1w1sx55U5eGjrQzjiPJLElhIRERERUVdh6O5Hzh1fgvMmluCr/GF4e8hpAAC1pQVVd98DIUSSW9d/5Fvzcd2k6/Ded9/DXaffhWFZw/Rt9e56/HXHXzH/1fm47oPrsPbQWr7vm4iIiIioF2Po7mfuWzgBA7Kt+NvY81BvzgAAONeuhf3NN5Pcsv7HYrDgu6O+i9cveh1PzXsK8wbPg0EyAAAEBD48/CFu+O8NmPPyHPxp85+wp2FPkltMRERERESdxdDdz2SlGfGXy6bAa03DoxMX6uVVS+6G9+DB5DWsH5MkCaeWnIpls5fhve++h+snXY9iW7G+vc5dh2d2PoOL37gYl7x1CZ7f9Twa3Y3JazAREREREXUYQ3c/NKksG79eMAYfDzgZ75dNBaBdZn7k5lsgvN4kt65/K0grwP87+f/h3YvfxSPfegRzB8+FQTbo23fW7cTSTUvxrZe/hcVrF+ODig/gDfC/GRERERFRqjK0X4X6oqtnDMHnhxqx0vdtjK4/iIHNtXB/+SWqly1H0e23Jbt5/Z4iKzir7CycVXYWGt2NePvA2/jXvn9hZ91OAIBP9WH1wdVYfXA1MkwZmDt4LhYMXYBpRdOgyEqSW09ERERERCE8091PSZKEB747ESefVIr7T7kcvmBQq3/mGTS+9npyG0dRsi3Z+OGYH+Kl81/Cqxe+iqvGXoU8S56+3eF1YNWeVfjJez/B3Ffm4oFND2BHzQ4+HI+IiIiIKAUwdPdjZoOCx6+YBuWk0Xh8/IV6eeWdd6Jly5YktoxaMypnFG455Ras/t5qPDrnUVww7AKkGdL07TWuGjy36zn88O0fYu4rc3HfxvuwsXIjn4BORERERJQkvLy8n8uyGvHMNafiB34VbzqO4YID6wG/HxXXXY9hL70I0+DByW4iJWCUjZg5YCZmDpgJl9+FDw9/iLf3v42PjnwEn+oDABxrOYYXvn4BL3z9ArLMWThr4Fk4e9DZmFE6AxaDJcnfgIiIiIiof2DoJpRkWfHiz07D5ULFAGctptTshmhsxN4rrsbIF/8JY2lpsptIbbAarDhnyDk4Z8g5sHvt+ODgB1h9cDU+rfxUD+BNnia8se8NvLHvDZgVM04pPgWzBszCrAGzUJZZluRvQERERETUdzF0EwAteD9/7Rm4xu9Bzut/xFB7FaTqKnx92ZUY/dLzMBYWJruJ1AGZpkx8e+S38e2R34bT68THRz7GBxUf4MPDH6LF3wIA8AQ8+PjIx/j4yMdYiqUYkjkEMwfMxKwBszC1eCrMijnJ34KIiIiIqO9g6CZdUaYFz904BzeZJFz24lIMdNbAUHkEn3/vh5jw/LMwD+AZ794k3ZSOc4eei3OHngtvwItPKz/Ffyv+i4+OfITqlmq9Xrm9HOX2cjy36zlYFAumFE3B9JLpmF48HaNzR/Np6EREREREJ4Chm6Lk2Ex47MZ5uCvNgPlP3oUiVwNsx45g+8LvYdjfn0bBuFHJbiIdB5NiwpkDz8SZA8+EEAJ7Gvfgo8Mf4aMjH2F79XYERAAA4A64sf7oeqw/uh6Adub8lOJTtBBeMh1DM4cm82sQEREREfU6kuhn7xWy2+3IyspCU1MTMjMzk92chFRVRXV1NQoLCyHLyXnAvKoKPL1qPQYvvQ0DmmsBAHZzOrz3PIBZF30rKW2i7mH32vHp0U/x0ZGPsP7o+qiz4LEKrYWYWjQVI60jcebwMzEydyRkiS9BoNSQCr87iRJh36RUxb5Jqaq39M2OZkuG7hSUSp3sk0274bjxFxjccAQA4JMVfLrw/+HS316LDIsxqW2jrieEQLm9HBsrN2Jj5UZsqtoEu9feav1MUyYmF07G5MLJmFo0FWPzxsKkmHqwxURhqfS7kygS+yalKvZNSlW9pW8ydLeCobvzqo7U4PMf/QyDKr7Wy94d+y2MuPM2nD9pICRJSmLrqDsF1AC+afhGD+Fbq7fC5Xe1Wt+smDE+fzwmFUzChIIJmJA/AYVpfAgf9YxU+91JFMK+SamKfZNSVW/pmwzdrWDoPj6q14tPFt2B/P++pZdtzx+ONd/+BW65fBZGFWUksXXUU3yqDztrd+LD/R9iT8sebK/ejgZPQ5v7FKUVYUL+BIzPH4+JBRMxNm8sbEZbD7WY+pNU/N1JBLBvUupi36RU1Vv6ZkezJR+kRh0im0yYtfKP2P/X8WhZ/kcoagCTavdh+N9/g3s+/x5KL1yAG+eMwoBsa7KbSt3IKBsxIX8CitQiFBYWQpIkHLAfwNZjW7Gtehu2HtuKw87DUfscazmGYxXH8H7F+wAACRKGZw/HhPwJGJc3DqPzRmNUzihYDew7RERERNT38Ex3Ckr1kZ2Wzz7DvhsXw1BXo5e9N+gU/P3ki/DtM8fg2rOGoTDTksQWUndqr39Wt1RjR80O7KjVpi9rv9TfEd4aWZIxJHMIRueOxpjcMRidNxqjc0Yj25LdTd+C+qJU/91J/Rf7JqUq9k1KVb2lb/Ly8lYwdHeNQGMjDt95F1pWv6eX1Zsz8MSEC7Fh0BR8Z9pA/OzM4Riaz8uI+5rO9s+AGsCBpgNRIXx3w279NWVtKbYV60H8pJyTMCJnBAamD+S7wymh3vC7k/on9k1KVeyblKp6S99k6G4FQ3fXEUKgadVrqLrvPojmZr18c9FoPD7+QhzNLMS544pxzcyhmDY4hw9c6yO6on+6/C58Xf81dtbtxNf1X+Pr+q+xt3Ev/Kq/3X3NihnDsoZhRPYIjMgZoc2zR6DEVsI+1s/1lt+d1P+wb1KqYt+kVNVb+ibv6aZuJ0kSsr9zMWxnzMCxe++FY7V2z+4px77G5OrdeHvI6XjePRfvfFmFk4oycNlpg7Bw8gBk8lVj/Z7VYNVfNRbiC/iwr2kfdtXt0oP41/Vfx12a7gl4sKt+F3bV74oqtxltGJ49XA/hQ7OGYnDmYJTaSnlmnIiIiIiShme6U1BvGdmJ5Xj/fVT97vfwHzumlzUbLHjxpLPxxrCZ8CpGWI0KFkwowcLJpZgxPB+KzDOTvU1P9k9VqDjsOIxd9buwt3Ev9jXuw56GPahwVEAVaoeOYZJNGJQ5CEMyh2BI1pCoeZY5q1vbTz2rt/7upL6PfZNSFfsmpare0jd5eXkrGLq7l9rSgrqnn0bdU3+DaAmfoaw3Z2DViDPx76Ez4DaYAQCFGWZccHIpFk4agPEDMnlpcC+RCv3TE/DgQNMB7G3ci70Ne7V5414ccR7p1HFyzDl6AB+UOQhlGWUoyyjDwIyByDSl5u8Hal0q9E2iRNg3KVWxb1Kq6i19k6G7FQzdPcNXXY3ahx9G46urADV8RtJhSsOq4WfiraEz4DSl6eWD89Iwb2wR5o0rxpRBOTwDnsJSuX+2+Fqwr3Ef9jbuRbm9HOVN5ThoP4gKRwV8qq9Tx8oyZ6EsXQvgoSAeCuWFaYWQpdT67pTafZP6N/ZNSlXsm5SqekvfZOhuBUN3z3Lv3o3aRx+F493/ABFdzWc0YXXZNLw2dCYOZxRG7ZNnM2HOmCJ8a0whTh+ex3vAU0xv7J9+1Y9KZyUO2A+gvKlcC+T2chxsOohqV3Wnj2eUjRiQPgADMwai1FaKkvQSfV5iK0GBtYD3kSdBb+yb1D+wb1KqYt+kVNVb+mavCd0rV67EH//4R1RWVmLcuHFYsWIFZs2albBuZWUlbr75ZmzZsgV79uzBL3/5S6xYsaJTn8fQnRyefftQ98QTaHrr30Ag+lVRe4dMwD9LTsWmglFQY4KKIkuYMigbs0YWYNbIfEwcmM2z4EnW1/pns68Z5fZyHLIfwmHnYRxyHMJhhzavaq6CQOd/RRokA4psRSixlaA0vVSfF9uK9XBuVszd8G36t77WN6nvYN+kVMW+Samqt/TNXvH08pdeegmLFi3CypUrccYZZ+Dxxx/H/PnzsXPnTgwaNCiuvsfjQUFBAe644w48+OCDSWgxHS/z8OEofeAB5F93Her//gwaX39dv+d7RPkO3FW+A77sXGwfOwN/zxyP/ZZ8AEBAFdhc3oDN5Q1Yvno3Mi0GnDo0D6cMycEpQ3MxvjQLJkPq/kOk1Gcz2jAubxzG5Y2L2+YNeHHUeVQL4jGB/IjzCFx+V8Jj+oUfR5xHtHvMjyWsgjxLHgrTClGUVoTCtEJ90tdthcgwZvBZB0RERES9XFLPdE+fPh1TpkzBo48+qpeNGTMGCxcuxNKlS9vcd/bs2Zg0aRLPdPdSAbsdja+8iobnnoPv6NG47Z4xE/DF2Bl4OW0EdjhaDx0Wo4zJZVoAnzY4BycPzEZWGi9H7079oX92hBACde46VDorUdmsTUedR3G0+Siqmqtw1HkUdq/9hD7DarBGBfLIUF5gLUBBWgHyLHmwGCxd9K16N/ZNSlXsm5Sq2DcpVfWWvpnyZ7q9Xi+2bNmC2267Lap83rx5WL9+fZJaRT1FycxE3jU/Qu6VV8D54YdoXLUKzrXrAL8fAGDetQOn7NqBUwwGKNNOxYEJM/Bu9klYc6QFjS3hB2K5fSo27K/Dhv11etmQvDRMHJiNiQOzcHJZNsaVZiLNxFfSU9eSJAn51nzkW/MxoWBCwjrNvmYcdR7VQrmzEkebj0aF9FpXLQIikHBfAHD5XThoP4iD9oNttiXdmI58az7yrHl6m/IseXFlOZYcGGUOShERERH1pKQlkdraWgQCARQVFUWVFxUVoaqqqss+x+PxwOPx6Ot2u3bmSVVVqGrH3vPb01RVhRAiZdvXpWQZttmzYZs9G/7aWtjffAtNq1bBu2+ftt3vR+DT9Rj06Xr8P6MRN82cieYZs7GtdBw21nixubweRxvdUYcsr2tBeV0L3vhcO4MuS8CIwnSML83CScXpGF2cgZOKMlCQYealu8ehX/XPE2RVrBieNRzDs4Yn3B5QA6h316PaVY3qlvB0rOVYeN1VjWZfc5uf4/Q54fQ5UW4vb7OeBAnZ5mwtiFu0QJ5ryUWOJQc55hx9OducjRxLTq+7vJ19k1IV+yalKvZNSlW9pW92tH1JP/0X+wedEKJL/8hbunQp7r777rjympoauN3uBHskn6qqaGpqghAipS+n6BbnLUDagvkwf7Mb3jVr4F2zBqJae7q08PnQvGYNsGYNJisKTpk4EcbTT0PT9GnYJmXhy8pm7DrWgj01LfAGwndNqALYfcyJ3cecUR+VbTVgeJ4VI/KtGJ5vxbA8K4bkWpBu5lOn29Kv+2c3KUABCswFGGceB+TEb2/xt6DWXYtaTy3q3HXa3FOHBk8DGrwNqPfUo8HTgJZAS5ufIyC0fTwN2Iu97bbLIBmQacpEtjEbWaYsZJmykG2KWI4ozzJlIcOYAUVK3r8f9k1KVeyblKrYNylV9Za+6XA4OlQvaaE7Pz8fiqLEndWurq6OO/t9Im6//XYsXrxYX7fb7SgrK0NBQUFK39MtSRIKCgpSupN1q6Ii4MxZEHf+Bu7PP4f9nXfh+M9/EKip0bYHAvBv2wb/tm0wAZg5aBDOPetMpJ1+OgyTZmBvs8COI0344nATvjjShN3HnAio0Y8vaHT5seWwA1sOR/9jyU83YVi+DcML0jG0wBZctmFgThqfnA72z2QZgiHt1nH5Xahz1WnB3FWLOledFtaDy3XuOq3MVQuv6m33eH7hR72nHvWe+g61UYKEDFMGsszBIB45N2ch05SJLHMWss3ZUcvpxvQuecUa+yalKvZNSlXsm5SqekvftFg69lydpIVuk8mEqVOnYvXq1fj2t7+tl69evRoXXXRRl32O2WyG2Rz/ah5ZllP6P6AkSSnfxh4hy7BNnQrb1Kkovv02tHy2BY4P3odzzVr4Dh3Sq/kqKtDwj+fQ8I/nAEVB5vjxmHvaaVh42nRYzz8VXsWIvdVOfF3lwNeVdnxzzIFdlQ7UOj1xH1nr9KLW6cWm8oaocpMiY3BeGgblpqEsN3Zu7Vf3jbN/piabyQabyYZBWfFvf4gkhIDD50Ctqxb1Lu0seYM7OHkaUO+uD6+7tTPpftXf7ucLCNi9dti9dhzCoXbrh0SG9WxzNjLNmXpYDwX0dFM6MkwZyDRlIsOUgQxTBtKN6XGBnX2TUhX7JqUq9k1KVb2hb3a0bUlNCYsXL8YVV1yBadOm4fTTT8cTTzyBiooKXHvttQC0s9RHjhzBs88+q++zfft2AIDT6URNTQ22b98Ok8mEsWPHJuMrUA+SFAW26afCNv1UiNtvh/fAATjXroNz7Vq0bNkSfv93IADX55/D9fnnqHv8cUhGIywnT0ThpEkYPHkyFp4+CYY8rb/UOj34psqBXZV27KtxYl9NM/bXOFHrjD8L6A2o2FPtxJ5qZ9w2AMhPN2NQrjUcxHPSUJJtQUmWBSVZVtjM/SeUU2qTJAmZpkxkmjIxLGtYu/WFEHD6nHoo18N4KJwHg3qTpwlNniY0ehrh8Do6/I7zqLDu6HhYD0k3putB3Awzcm25UeE80ZRp1IJ8uimdD5cjIiKibpXUV4YBwMqVK/GHP/wBlZWVGD9+PB588EGceeaZAICrr74a5eXlWLt2rV4/0f3egwcPRnl5eYc+j68M65sCdjtaNm1C86cb0bLxU3j2tH2/qrGsDNbJk2CdNAnWCRNgHjUKcsQVEU0uH/bXOLG/phn7gvP9tU6U17XA6z++BzpkWgwozbZqITzbitIsC4qztHlJthXFmRZYTal/Pzn7J3VEQA3A6XPqIbzJ04Qmb5MezFtbt3vsHQ7rXcVqsMJmtMVN6cb0dssi19OMaZAl/pugePy9SamKfZNSVW/pmx3NlkkP3T2Nobt/8NfUoHnjJrRs/BTNn26MuhQ9IYMB5hEjYBk7FpZxY7X56NGQrdaoaqoqUO3w4FBDCyrqWlBR34JD9cF5QwuO2eMvV++MDLMBBRlmFGSYUZhpQUG6GYWZZhSGyjIsKMgwIyfNmLSnSrN/UndShQqH1xF31tzhdcDhc4SXI6eIck/gxP4Nnqg0Q5oWxE022Aw2fZ5uSkeaIQ1WoxVphrSOLRvTYJJNveoJ8pQYf29SqmLfpFTVW/omQ3crGLr7J39NDVq2b4dr+3a4tn8O944dEN52HiQlyzAPHwbL2LEwnzQa5pEjYR41EobCwlb/CHb7AjjcoIXwIw0uHG1yo7IxOG9yoarJDV/gxP/JGRUJBelaEM+1mZBrMyMv3YScNBPybCatLD28nG42dNkf7uyflKpUVcXhysOwZlvh9Dvh9Drh8Dpg99kTh3WvA3avHc2+ZjT7muH0OdHia2nz3ek9TZbkuDBuNVijQ7rBijRj68tWgxUWgwVWRZuHJl5W33P4e5NSFfsmpare0jc7mi15kyn1C4aCAmTOnYvMuXMBAMLrhfvrr7UAvnMn3F99Bc++fUDku/ZUFZ49e4OXqr+hF8sZGVoAHzkS5hEjwmE8NxcWo4IRhRkYUZiRsB2qKlDX7EVlkwtHG8NB/GiTG8fsbtQ6PKh2eOD0tP3QKl9A4Ghwv44wKXIwnJuQl27Sl3PTTMhOMyLTakR2mglZViOyrUZkWbUyPq2dehuTYkKeNQ8FcsFx7S+EgDvgjgviTq8zvOxz6tvj6sVsO9FL5VWh6u9hh+uEDhXHIBlgNphhUbQQbjVY9eXYdbNi1sN7VP2YdbNijtvXIPNPDSIi6t/4f0LqlySTCdaJE2GdOFEvU10ueL75Bq6dO4NBfCc8e/YA/ugArDoccG3dCtfWrVHlSm4uzMOHwzR0KExDhsA0dAjMQ4fCOHAgJIP2T02WJf3y8YkDW29fi9ePmmAAr7Z7UONwa8sOj15e4/CgrtmDjlyr4g2oqLK7UWXv3LvpMy0GZKUZkW3VAnlWmhGZFgNMwofSfGcwqGvbMiwGZFq0ebrFAKOSuqOSRK2RJEk7k2ywIt+af0LHUoUKt9+th2aXz4UWfwtcfhdafMG5v6XV5VC92OWOPEm+I/zCD7/Pj2Zfc5ccrzUG2QCLYtEDuUkxwayY9cmkmGBRWikPBv7W6pgNiY9hUbSwz0vziYgoFTB0EwXJVqv2YLVJk/Qy1euFZ/ceePZETHv3wl9ZGbd/oL4eLfX1aNm8OXqD0QhTWVlUEDcFJyUnJ+EfhWkmAwbnGTA4z9ZmmwOqQGOLF/XNXtQ1R8ydXjS0hMo8qHNq2+qbvfCrHT/zZnf7YXf7cSjhKbaqNve1GGVkBEN4hsWIDLMhuKytpwfXI4O6Xt+sLVuMMv9opl5LlmTtUm9jGgpR2GXH9QV84SDub4kL84mW3X433AG3Nve7tbKI9dCyy+/q8gfZ+VU/nGrwbH0PkiCFg7kcDvGRwd2kmGCSTTAqRphkk7beSplRNurbWiuL2ieiTBYchCQi6s8YuonaIJtMsI4fB+v4cVHlAYdDu/R8756oeaC2Nv4gPh+8+/fDu39//PEzM7UwPnAgjAMHwjhwQHi5pASSse17LhVZQl66GXnpZozswPcRQsDu9gcDuAf1zT40uYJTixdNLh8aXT40tkSUu3xobPGiE1kdAOD2qXD7tDPyx0uWAJvJgDSzApvZoC2bFKSbDUgzG2AzKUgzGZBuVvR1m9mANJMBtph9bGatzKQwyFPvZlSMyFK095h3NSEEfKovLqiHQrrH74Er4AqHdb87er2VIO8OuOENeOH2B+cBN3yqr8vbH/VdoN0q4A507gqf7iBBglE2wqyYtWAeGcpDwb2dsG+UjeFJ0eYG2RBVrq8rCcpa208xwiAZot53T0REXYuhm+g4KBkZSJsyGWlTJkeVB+x2eMvL4T1wAJ4DB+A9oC17Dx6E8MSHT9Vuh/uLL+D+4ov4D5FlGIuLg2E8JpCXlsJQUABJ6dwfSZIkaZeJW40Ymt/2WfSodqoCTq8fTS0+NDR7cLCyBpLZhia3PxjYfbC7/XB6/HC4fXC4/XC6I5a9/g5dBh/3uQJwePxwePwAuuap1AZZ0oO7xaQgzaTAalRgMWrzNJMCqyl63WLUyqLWI8oi5xaDApn3wlMvJUmSHva6I9RHUoUKb8ALT8ATnvweeNTgPODRA3pcvUC4TmiKq9vKsTwBD1RxfK9+PF4CAl7VC6/qBbp3rOG4yZLcZoCPC/iKAUYpGNojtrW3X+SgQuyAQtTgQ6huTD2+lo+IeiOGbqIupGRmxt0rDgBCVeGvrIQnFMIPHIC3vBye8gPwH42/VB0AoKrwHT0K39GjwKZNCT5MgaGwUAvmJcUwlJTAWFyiLQfnSm5ul5zVlWUJmRYjMi1GDMi2oNDo6dTTJFVVoNnrh8MdmnxamA4uOyPL3f7gNh9avAE0e/xo9gTQ7PWj2ePv9Bn3WH5V6JfNdxeLUdZCuFGJC/ZmgwKzUYYlODcb5GB5eG42KLAYo+eR+8TOTYrMoE+9jizJ+kPbeppf9UeFdV/Ap4XigFefx5b5Ar6o7YnKfGpwPXJ7cFuLpwWqrEaVheqnwhPzVaHqP5NUZpAMUWHcrJgTXvofeeWAWTHH3QIQur0gKuxHHkNOUBZzXF4hQEQdxdBN1AMkWYZxwAAYBwwAZp4RtU31eOA7chS+I4fhO3wY3sOH4Tt8RF9Wm5oSHzQQgL+yEv7KSri2tfK5JhMMxcUwFhfDUFwEQ0FBgqkQSnrHz3ofD1mWgvdrn9grioQQ8PhVNHv8aPEG4PT40eLVQnmL1w9ncN7sCYZ1rx8tngCcXj9aPH40B0N8aF+3N4AWXwCBE03yMbRL61U09OApLZNBjgnurYd5k0GGUdHmJoMMc8SySZFhMihR62ZDzD7BudkQvW4yyDDIEi/fp5RnkA0wyAbYjN37uy+kvVffBNRAVBBPFPhD5T7VB7/q19bVcLlf+MP1Yuu0sp++3sZ+scdNNr/ww+/3w+Xv4sf5H6fYKwRaO8tvkA1Rl/23djtA5CX/icrbum2grasNDLIBiqRAkRT+jiZKAoZuoiSTzWaYhw2FedjQhNsDDkdcGPdVVsJXVQl/ZRUCDQ2tHlt4vfBVVMBXUdFmG6S0NBgK8lsJ5cEpLw9KdjaQxP9ZS5IES/CMcV4XHtcXUOHyBeDyBiefNrm9AbRErgfrtHiDy76I+hH1XMHtoX3dvkCXvJ+9LV6/Cq9fhaMbz+B3hCQhKpTHBvX2QrtR0dYNigSjIgcnbdmgyDC1smxUJBhkCY6mZtT67TAZlah9Y5f5OjxKJYqswCprT81PZUIILfTGhPWEgwAJAn6iQYXQ2f5QvdCVApHrennEAESiqxKScZa+t1whECkUyg2SFsYlIcFkMOnhXN8eDOqR5aFtceWSIa5eXFlH6rRTV5GVqIEEXm1AvQVDN1GKUzIyoIwZA8uYMQm3q243/FVV8FVVwVdZBX9VJXyVVXoo91VVQXU42vwM0dIC38EK+A62Hc4hSVCysyGyMuEuKIQhLw+G3Fwoubkw5OVCyc2DITcHSrBczsyE1MFL0JMpFMQyT/BMfFt8ARVuXwAef3ju8alw+wNRc0/E3N3KPPIY4Xn0vqFjdnfYjyUEtO/mV9F2r0suWUIwuIcDfqthX5b0AQGDLMEYHCgILRtlCYbQvrIW6I2KBEWWg3Ntu1GWgtvCdQyyDCVmP0Pw2IbgQIJB3yaHy0J1gsfkmSvqCZIkafdxy0ZYkXoDBEIILdyHrhIIeONuCUhYnuAWAf3Wg5iAH5pirwBo7cqA0IBDKvGr/vg2eZPTlq4gQQqH8WBYV2RFHxhQJCXheiiwR60n2GaQDZAluVPHO556CdvTXj1eudBrMHQT9XKyxaI9AX3IkFbrBJxO+Kur4a+ugb8mwVRdDX9NDVRnO6/0EUI7s97QAFf5wfYbpyhQcnNgyM0Lz7OzoWRlafOciOVguZyR0SuCemeFglxGD39uQBURYVwL6d6AdlbcEzw7HlrXlgPw+QU8kWXBcm9EfU/ENl8g+hieuGOGlzvzyrrupIrw1QF9QTiky1FhPS6kh+rE1Y8J+jH1lWDAlyOCviJJUIL1ZSlYrshQpIg6wSl+XYYsQz924jrB4yrhzwsNUiiSFNUmIiA4KKBol1X31K0DHREaDIi7tD9BOI+93L+t+u1dbRC5b2gKiIBeN1Tm8XsAKRzG/cKfkoMFrREQiQcS+olQ+O5IcA8NHkQOCCiyAlmSYZCC2yMCfdT21vaXord3tF6rnxNshwwZTY1N8Fq9GJQ1KNk/5hPG0E3UDyjp6VDS02EeNqzNeqrLFRPGg/O6OgTq6+Gvr0egrg6+ujrA3YHX8AQCCNTUIlCT4FVqrZFlKJmZUUFcW44J51lZWr2MDMiZmVDS0yGZTB3/nH5CkSWkmQxIS5EfjaoKLYTHhfrw3B8QepAPLWuTtuwPqPAmWPb5VTQ5nTCYLAioQq8fuW9oObSfv5Xtoc8/nqfuJ4NfFcEBjb4xiNBZBllKMDAgQ4kJ9q2FeSU4wKAPHiTYV5YkKDIilsPzqO2S1gZ9LkuQAbQ0O5GV2QJD8MGHkfUUOXIZUccP10WCupFtQBvtijhGXBkHLbpb5GBAqmnreQNCCKhC1W8piAzrofAeOflUX+Jy4UNAjSkX/rbXO1MWbE9k2wJqAH7hR0ANJF5PgYcXdhX9+/TRX/8LhizAA2c9kOxmnDCGbiLSyVYrTIMGwTSo9RHF0P+g89PToTY2IVBfFwzj9fDX1yFQ36CV1dVHBXXh6+ADeFQVgcZGBBobO91+yWoNhvAMKBmZ+lzJzICszzOgZGbqc4b2niXLEiyydl9+V2vvYVXHQwvvajDgxw8GeIMhPaCG5kKvqwVhVR8ACNXxB1Q9JMfV17eF99PLIo6pLasJyjpWv68J/Tx7z121qSUylMtSzIBB7GBCzICBLLcy2BA1QAD9NojQ5+jLEvTBhYTL+hQ+RtxycB8ptCwFl+XwvnLkshQeeJCkyMEKtNLGBMdIcLzQwEhkG2PbEtnG0D4Jl4NtSSbt+ytQoMCsmJPalq4mhNDDamwg72hwb7VesCy2Tlv7dLheB9uT6Bh+1Q9VqHqZQO/4f0FfuWefoZuIjouclgZDejowcEC7dYUQUJubEairQ6CpSQ/VgcbgclPMPLjc3r3ocZ/jcsHvcgHV1cf1nfTQnp6uTbY0KOnpkG2hdRvkdJtWFlq3pYfLbDbI6emQzOak/7FEXUM7W9g9gwTJIoQW9kNBNTqkRwT0iOAeUEP7qFBV6IMCflVAVaPrhMoDQiAQPLYqgmWBYHkr+2rLKgIqEFAj9g1E7BOxHhCt7NNKm6LKg+0gbXApAAH0nZN/fUJrgwhxAwqdHBiQgwMDErTnW/j9PphN+/U6oYEISd8vYh8JUceLrCO1so+2LfE+siQBre2D8PeI2wcR6xEDMLL+vSL2ifvO4fbKEXUkhPZRIEuGqDqxc5MkwRL8DEmJHyyJPX7k95clRHzn1uvG7tPVf1eEArgq1KggHxo4UIWqB3W/8ENV1bhBhdh6sds6U0//nGB7/KofjmYHTik5pUu/d7IwdBNRt5MkSb/EvTOE34+A3R4f0hsbEXDYododEXMHVLtdn6stLZ1u54mGdp3BANlmgxIM4XGB3RZcT0uDnGaFnJYGyWqFnGbT12VrqFyr0xfvc6fkkKTgvdp9ZxzhuIUGICIHAhINDPgCKlQhEFARnIuYOaLKAkLbL6APEqhoaGyCLSMDQkjh7RH1AgJ6WUCN3q4KxNQVEXURfayY9iVqc3RbEdXW0PGFQFzd6P0R15becjtGbyEE4BcC6JEzks098Bl0omIHOWIHGpBgOxA/8AFAH2iIHTCJHLSIHTSRED+4EhqskCQZsqREDBqEB3YiByJiB0Wi2hzxPSQAHrcbzWklwIjk/Ly7EkM3EaUsyWCAITcXhtzcTu8rfD4EnE6oDgcCdgdUhz1qHhfa7fZwneZmqM3NOO6/IP1+qE1Nrb9j/ThIFkt0GE+zQramRZXJaVZIaWnR5bbYAB8M+hYLJKsVksnEs/LUb+kDEN38OdqtD0qX3vqQikTUoAPiBxCCwTwU3iOXVRG65FfbVw3WD4V7kWhZP2Z4cCFyWZ+CbUl4DDXcrsg2hsrV4KCEKsIDC5GDDHHtj/0uwTpqwuOF7ptu5xgxP4/w8eJ/Hgm/lxr+WYTazIs8eqdQP9UuSukf/xENJjMunT442c04YQzdRNQnSUYjDDk5QE7Oce0vVBVqiwtqs1ML4U4nVKcTgeZmqM7genBbwOmMKGsO1nNCbW6B6nRCdOShc+21x+1GwO3u+qs/JUkL9BYLJKsFsiUYyPUya3ib2QLZaoFksQbnlnBda8x+sXXMZp6tJ+rjemoQg7qGCN4CUlVdjfz8AkCSIkK5dseviAn8IlgeOYAQt0/U4AD0sC8QLhcicgBC2xY5IKDXUaF/XtRxQ21JtI+AXt6ZffR2JtgnPGCR+LhA5OBIxD5A3PeJ2ifB52jfObKs9faHBmlCP3ft54yI9kZ/p/BxY9qoHyNR3Yhj4fjPRxwvuY+cGODvRSKiBCRZhpJug5J+4q+dET6ffvY84GzWwrrTqYV6lwtqSzOEywW1pUUra2kJlrdAuFq08O6KLXd1wbcEIASEy4WAywU0dM0hWxMV0s1mLYhHLZshm7WALlvMkMyWYJm2LFvMkEzBehYLJFOwXnAZJqM24GEyAVYrgz4RURskKfy0f5NB7tNXYVDXEiJ+UACICfsID9pEDgSImPVweXRdNaCitq4Og0oLk/hNuw5DNxFRN5OMRv11Z131whihquGgHhfYWyBC5VGBPVje4oLqcUO43FDdbgi3C6rLHS5zuYBA1z9RKXS2vrtFXtQvGY1aKLeYIZtaCfkWC2SzqZWQb9KCvskEyWyCZDJp2yPK5WC5FFEum4yA0chL94mIqM/RnmwPaHdedw9VVZGBFhRmWbvtM3oSQzcRUS8kyTIkmw2y7cTPxCcifD6oHo8W7N1aEBfuUEh3Q3WFw7rwaOuq2xUR5LW5XhYZ8l0uqF4vhNsN4enelzwJn097XZ3D0fMPZpakqDAum2LCuTlYZowu0+qao/cNBfuODgCEJqNRmyt8ahoREVGyMHQTEVEcyWiEYjQCnXzifGcJISCCAVz1eCA8nuCyVwvzbjeEvhzcHrGsetwQbg+E14OAyw23vQkmSOHj6McOHidYho6+N/7EvlywvdrAQlLfxiTL4QBuNIanyPWOLHdlPZNRG3AwGSEZDLwVgIiI+iyGbiIiShpJkiCZzYDZjBM9F6s9Ibq6Q0+IFn6/FtpDodztDof/YIjXA7/XC+H1QHi92sCA1xtR7oXqjSkL1lG9seseCK8vHMR78mk0qho1AJCSDIaYcJ4gwBuC6wZDeFtwGUZDdJ2oeuH6MMTWS3CsyOPHHiuyDm8fICKiDmDoJiKifkcyGCAF36eeDEIIwO9PGM7D4d6nh319gCDhIIAvYt+IesFL64XPp21rZ7k77uPvFL9fGwxJbis6Jy7cJwjnRgMkgxE+IeBNS9MGEKJCvSHxYIDBoG0zGLRthnAZFCVYz6D35XAdJXo9rk7MOgcOiIi6HUM3ERFRD5MkSQ9syQr+sUQg0KmQnnDZGwr63vDy8R4rdrknbgnorFDbOljd362NOU6RQVxRtH4ZWWY0AAYjJEUJh3VjxCCAPhAQu09kHSVmPaKOEju4ED1YkHiAIbiuKNHroe9ARJRiGLqJiIhICzCKAlgsyW5KQvrVAT6fdkY8cu71QfiDwTx2m88H4QuVxdSJ2hZRFqqXcFvMsRIdI6I+/CkZtcN64xUGbZGkxGf1YwYKYFDC64qirSuhgQdDeDlUL6aOPtAQU0cyKMEBiOCAQGhZUcJtUmLrKBHtaL+O/v34HASiXoOhm4iIiFJe5NUBvYkQAqrHg+rKSuRnZ0MKBCJCeTjAIzbU+3wQ/oC2LRSMfcG5X7sdIGo9cntAm0PfHlkvEL0eUwf+2H38Wtv8/p59DsHxEiI8GJLstnQ3SQpeLRAT2iODedRAQII6igJvIABfWlr0FQ+hAYbQAEVwAEC/8iC03NE6xpiBBEXRrlAILXekDm+HoF6MoZuIiIiom0ihV8dZrVCystp9yF8qE6oaFcJFxGAA/D7tFoXI9cjBgoA/8eCB3x8cXEgweNDaQEHMAEP8PqHPb3+AoVcTotO3OLQmBW/eSExRogcOOnplQsQgRMIrE/SrFyIGD0K3P0QNMITrQ5HDZbISVS/RNn1AQZGjBxaU2G0RgyjBbaFlyDIHHnophm4iIiIiapcky5BMJsBkSnZTuoQQQnuyfyAQHhAILQcHEMLLASAQCuzB5dDVBgG/duVBTB29PKZOeAAitBwIDyAkrNNK+wKB8KBETB39uDHfI+kPTDxRwe8Nr7fvX8nQmlYCedRAgyxHDzLIEYMVshx9O0XkvrJy/AMLUQMd4YEFyHJ0fUUOH0ufy9rASHAuyTKELCPQ0Ai/osBUUJDsn/oJY+gmIiIion5HkqRw4OgjAwnt0Z+NEAz3qs+LmmPHtFsfggMQCQcbgoMH+nLoaoGowYaIwQN/xKBA1EBCK3VCy63W8UcNNrQ7SBJRp1fcFtEZoe8F9IuBB3HeeRi47E/JbsYJY+gmIiIiIuoHYp+NIKlWyB4PDIWFvfrWh7YIVY0K5vpgQezVAfogQvSAQtStEYGAdnWEPhihRl/poEbsE/qMuG0x+/oDEGogeuAhapsaPZgQaOUYEdv07xaxL1Q12f8pjktfeSMBQzcREREREfVJkiwDJhP6+53QQojowYbQMxoCAYiAGnUVQ+zAQnhbzMBC5DESDTqEyvTBich5uL6+nxrQ64hAAG5nM6yTJiX7R9clGLqJiIiIiIj6MCn0tHuDATCbk92cdqmqiurqamQXFia7KV2ib15HQkRERERERJQCGLqJiIiIiIiIuglDNxEREREREVE3YegmIiIiIiIi6iYM3URERERERETdhKGbiIiIiIiIqJswdBMRERERERF1E4ZuIiIiIiIiom6S9NC9cuVKDB06FBaLBVOnTsVHH33UZv1169Zh6tSpsFgsGDZsGB577LEeaikRERERERFR5yQ1dL/00ktYtGgR7rjjDmzbtg2zZs3C/PnzUVFRkbD+gQMHsGDBAsyaNQvbtm3Dr3/9a/zyl7/Eq6++2sMtJyIiIiIiImpfUkP38uXL8eMf/xg/+clPMGbMGKxYsQJlZWV49NFHE9Z/7LHHMGjQIKxYsQJjxozBT37yE1xzzTX405/+1MMtJyIiIiIiImqfIVkf7PV6sWXLFtx2221R5fPmzcP69esT7rNhwwbMmzcvquycc87BU089BZ/PB6PRGLePx+OBx+PR1+12OwBAVVWoqnqiX6NbqKoKIUTKto/6N/ZPSlXsm5Sq2DcpVbFvUqrqLX2zo+1LWuiura1FIBBAUVFRVHlRURGqqqoS7lNVVZWwvt/vR21tLUpKSuL2Wbp0Ke6+++648pqaGrjd7hP4Bt1HVVU0NTVBCAFZTvpt90RR2D8pVbFvUqpi36RUxb5Jqaq39E2Hw9GhekkL3SGSJEWtCyHiytqrn6g85Pbbb8fixYv1dbvdjrKyMhQUFCAzM/N4m92tVFWFJEkoKChI6U5G/RP7J6Uq9k1KVeyblKrYNylV9Za+abFYOlQvaaE7Pz8fiqLEndWurq6OO5sdUlxcnLC+wWBAXl5ewn3MZjPMZnNcuSzLKf0fUJKklG8j9V/sn5Sq2DcpVbFvUqpi36RU1Rv6ZkfblrTQbTKZMHXqVKxevRrf/va39fLVq1fjoosuSrjP6aefjjfffDOq7L333sO0adMS3s+dSOjMeOje7lSkqiocDgcsFktKdzLqn9g/KVWxb1KqYt+kVMW+Samqt/TNUKYMZcxWiSR68cUXhdFoFE899ZTYuXOnWLRokbDZbKK8vFwIIcRtt90mrrjiCr3+/v37RVpamrjpppvEzp07xVNPPSWMRqN45ZVXOvyZhw4dEgA4ceLEiRMnTpw4ceLEiROnE54OHTrUZgZN6j3dl1xyCerq6nDPPfegsrIS48ePx9tvv43BgwcDACorK6Pe2T106FC8/fbbuOmmm/CXv/wFpaWl+POf/4zvfOc7Hf7M0tJSHDp0CBkZGW3eO55MofvODx06lLL3nVP/xf5JqYp9k1IV+yalKvZNSlW9pW8KIeBwOFBaWtpmPUmI9s6FU0+z2+3IyspCU1NTSncy6p/YPylVsW9SqmLfpFTFvkmpqq/1zdS9QJ6IiIiIiIiol2PoJiIiIiIiIuomDN0pyGw246677kr4qjOiZGP/pFTFvkmpin2TUhX7JqWqvtY3eU83ERERERERUTfhmW4iIiIiIiKibsLQTURERERERNRNGLqJiIiIiIiIuglDdwpauXIlhg4dCovFgqlTp+Kjjz5KdpOoD1u6dClOOeUUZGRkoLCwEAsXLsQ333wTVUcIgSVLlqC0tBRWqxWzZ8/GV199FVXH4/HghhtuQH5+Pmw2Gy688EIcPny4J78K9XFLly6FJElYtGiRXsa+Scl05MgRXH755cjLy0NaWhomTZqELVu26NvZPykZ/H4/fvOb32Do0KGwWq0YNmwY7rnnHqiqqtdh36Se8OGHH+KCCy5AaWkpJEnC66+/HrW9q/phQ0MDrrjiCmRlZSErKwtXXHEFGhsbu/nbdQ5Dd4p56aWXsGjRItxxxx3Ytm0bZs2ahfnz56OioiLZTaM+at26dbjuuuvw6aefYvXq1fD7/Zg3bx6am5v1On/4wx+wfPlyPPLII9i8eTOKi4sxd+5cOBwOvc6iRYvw2muv4cUXX8THH38Mp9OJ888/H4FAIBlfi/qYzZs344knnsDEiROjytk3KVkaGhpwxhlnwGg04p133sHOnTuxbNkyZGdn63XYPykZHnjgATz22GN45JFHsGvXLvzhD3/AH//4Rzz88MN6HfZN6gnNzc04+eST8cgjjyTc3lX98Ic//CG2b9+Od999F++++y62b9+OK664otu/X6cISimnnnqquPbaa6PKRo8eLW677bYktYj6m+rqagFArFu3TgghhKqqori4WNx///16HbfbLbKyssRjjz0mhBCisbFRGI1G8eKLL+p1jhw5ImRZFu+++27PfgHqcxwOhxg5cqRYvXq1OOuss8SNN94ohGDfpOT61a9+JWbOnNnqdvZPSpbzzjtPXHPNNVFlF198sbj88suFEOyblBwAxGuvvaavd1U/3LlzpwAgPv30U73Ohg0bBADx9ddfd/O36jie6U4hXq8XW7Zswbx586LK582bh/Xr1yepVdTfNDU1AQByc3MBAAcOHEBVVVVUvzSbzTjrrLP0frllyxb4fL6oOqWlpRg/fjz7Lp2w6667Dueddx7mzJkTVc6+Scn0xhtvYNq0afje976HwsJCTJ48GU8++aS+nf2TkmXmzJn44IMPsHv3bgDA559/jo8//hgLFiwAwL5JqaGr+uGGDRuQlZWF6dOn63VOO+00ZGVlpVRfNSS7ARRWW1uLQCCAoqKiqPKioiJUVVUlqVXUnwghsHjxYsycORPjx48HAL3vJeqXBw8e1OuYTCbk5OTE1WHfpRPx4osvYuvWrdi8eXPcNvZNSqb9+/fj0UcfxeLFi/HrX/8amzZtwi9/+UuYzWZceeWV7J+UNL/61a/Q1NSE0aNHQ1EUBAIB3Hvvvbj00ksB8HcnpYau6odVVVUoLCyMO35hYWFK9VWG7hQkSVLUuhAiroyoO1x//fX44osv8PHHH8dtO55+yb5LJ+LQoUO48cYb8d5778FisbRaj32TkkFVVUybNg333XcfAGDy5Mn46quv8Oijj+LKK6/U67F/Uk976aWX8Nxzz+H555/HuHHjsH37dixatAilpaW46qqr9Hrsm5QKuqIfJqqfan2Vl5enkPz8fCiKEjcqU11dHTcKRNTVbrjhBrzxxhtYs2YNBg4cqJcXFxcDQJv9sri4GF6vFw0NDa3WIeqsLVu2oLq6GlOnToXBYIDBYMC6devw5z//GQaDQe9b7JuUDCUlJRg7dmxU2ZgxY/QHn/J3JyXL//7v/+K2227DD37wA0yYMAFXXHEFbrrpJixduhQA+yalhq7qh8XFxTh27Fjc8WtqalKqrzJ0pxCTyYSpU6di9erVUeWrV6/GjBkzktQq6uuEELj++uuxatUq/Pe//8XQoUOjtg8dOhTFxcVR/dLr9WLdunV6v5w6dSqMRmNUncrKSnz55Zfsu3Tczj77bOzYsQPbt2/Xp2nTpuGyyy7D9u3bMWzYMPZNSpozzjgj7vWKu3fvxuDBgwHwdyclT0tLC2Q5+k98RVH0V4axb1Iq6Kp+ePrpp6OpqQmbNm3S62zcuBFNTU2p1VeT8fQ2at2LL74ojEajeOqpp8TOnTvFokWLhM1mE+Xl5cluGvVRP//5z0VWVpZYu3atqKys1KeWlha9zv333y+ysrLEqlWrxI4dO8Sll14qSkpKhN1u1+tce+21YuDAgeL9998XW7duFd/61rfEySefLPx+fzK+FvVRkU8vF4J9k5Jn06ZNwmAwiHvvvVfs2bNH/POf/xRpaWniueee0+uwf1IyXHXVVWLAgAHirbfeEgcOHBCrVq0S+fn54tZbb9XrsG9ST3A4HGLbtm1i27ZtAoBYvny52LZtmzh48KAQouv64bnnnismTpwoNmzYIDZs2CAmTJggzj///B7/vm1h6E5Bf/nLX8TgwYOFyWQSU6ZM0V/dRNQdACScnn76ab2OqqrirrvuEsXFxcJsNoszzzxT7NixI+o4LpdLXH/99SI3N1dYrVZx/vnni4qKih7+NtTXxYZu9k1KpjfffFOMHz9emM1mMXr0aPHEE09EbWf/pGSw2+3ixhtvFIMGDRIWi0UMGzZM3HHHHcLj8eh12DepJ6xZsybh35hXXXWVEKLr+mFdXZ247LLLREZGhsjIyBCXXXaZaGho6KFv2TGSEEIk5xw7ERERERERUd/Ge7qJiIiIiIiIuglDNxEREREREVE3YegmIiIiIiIi6iYM3URERERERETdhKGbiIiIiIiIqJswdBMRERERERF1E4ZuIiIiIiIiom7C0E1ERERERETUTRi6iYiIqMOGDBmCFStWJLsZREREvQZDNxERUYq6+uqrsXDhQgDA7NmzsWjRoh777L///e/Izs6OK9+8eTN+9rOf9Vg7iIiIejtDshtAREREPcfr9cJkMh33/gUFBV3YGiIior6PZ7qJiIhS3NVXX41169bhoYcegiRJkCQJ5eXlAICdO3diwYIFSE9PR1FREa644grU1tbq+86ePRvXX389Fi9ejPz8fMydOxcAsHz5ckyYMAE2mw1lZWX4xS9+AafTCQBYu3YtfvSjH6GpqUn/vCVLlgCIv7y8oqICF110EdLT05GZmYnvf//7OHbsmL59yZIlmDRpEv7xj39gyJAhyMrKwg9+8AM4HA69ziuvvIIJEybAarUiLy8Pc+bMQXNzczf9NImIiHoWQzcREVGKe+ihh3D66afjpz/9KSorK1FZWYmysjJUVlbirLPOwqRJk/DZZ5/h3XffxbFjx/D9738/av9nnnkGBoMBn3zyCR5//HEAgCzL+POf/4wvv/wSzzzzDP773//i1ltvBQDMmDEDK1asQGZmpv55t9xyS1y7hBBYuHAh6uvrsW7dOqxevRr79u3DJZdcElVv3759eP311/HWW2/hrbfewrp163D//fcDACorK3HppZfimmuuwa5du7B27VpcfPHFEEJ0x4+SiIiox/HyciIiohSXlZUFk8mEtLQ0FBcX6+WPPvoopkyZgvvuu08v+9vf/oaysjLs3r0bo0aNAgCMGDECf/jDH6KOGXl/+NChQ/G73/0OP//5z7Fy5UqYTCZkZWVBkqSoz4v1/vvv44svvsCBAwdQVlYGAPjHP/6BcePGYfPmzTjllFMAAKqq4u9//zsyMjIAAFdccQU++OAD3HvvvaisrITf78fFF1+MwYMHAwAmTJhwAj8tIiKi1MIz3URERL3Uli1bsGbNGqSnp+vT6NGjAWhnl0OmTZsWt++aNWswd+5cDBgwABkZGbjyyitRV1fXqcu6d+3ahbKyMj1wA8DYsWORnZ2NXbt26WVDhgzRAzcAlJSUoLq6GgBw8skn4+yzz8aECRPwve99D08++SQaGho6/kMgIiJKcQzdREREvZSqqrjggguwffv2qGnPnj0488wz9Xo2my1qv4MHD2LBggUYP348Xn31VWzZsgV/+ctfAAA+n6/Dny+EgCRJ7ZYbjcao7ZIkQVVVAICiKFi9ejXeeecdjB07Fg8//DBOOukkHDhwoMPtICIiSmUM3URERL2AyWRCIBCIKpsyZQq++uorDBkyBCNGjIiaYoN2pM8++wx+vx/Lli3DaaedhlGjRuHo0aPtfl6ssWPHoqKiAocOHdLLdu7ciaamJowZM6bD302SJJxxxhm4++67sW3bNphMJrz22msd3p+IiCiVMXQTERH1AkOGDMHGjRtRXl6O2tpaqKqK6667DvX19bj00kuxadMm7N+/H++99x6uueaaNgPz8OHD4ff78fDDD2P//v34xz/+gcceeyzu85xOJz744APU1taipaUl7jhz5szBxIkTcdlll2Hr1q3YtGkTrrzySpx11lkJL2lPZOPGjbjvvvvw2WefoaKiAqtWrUJNTU2nQjsREVEqY+gmIiLqBW655RYoioKxY8eioKAAFRUVKC0txSeffIJAIIBzzjkH48ePx4033oisrCzIcuv/i580aRKWL1+OBx54AOPHj8c///lPLF26NKrOjBkzcO211+KSSy5BQUFB3IPYAO0M9euvv46cnByceeaZmDNnDoYNG4aXXnqpw98rMzMTH374IRYsWIBRo0bhN7/5DZYtW4b58+d3/IdDRESUwiTBd3IQERERERERdQue6SYiIiIiIiLqJgzdRERERERERN2EoZuIiIiIiIiomzB0ExEREREREXUThm4iIiIiIiKibsLQTURERERERNRNGLqJiIiIiIiIuglDNxEREREREVE3YegmIiIiIiIi6iYM3URERERERETdhKGbiIiIiIiIqJswdBMRERERERF1k/8P4ZrSCPPfZzsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure both models have loss_history of same length (or plot separately)\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot up to 1000 iterations\n",
    "iterations = range(1, len(lr_model.loss_history) + 1)\n",
    "plt.plot(iterations, lr_model.loss_history, label='Logistic Regression', linewidth=2)\n",
    "plt.plot(iterations, mlp_model.loss_history, label='MLP+0.01 LR', linewidth=2)\n",
    "plt.plot(iterations,mlp_model1.loss_history,label='MLP+0.1 LR',linewidth=2)\n",
    "plt.plot(iterations,mlp_model2.loss_history,label='MLP+0.1 LR+3Perceptron in Hidden',linewidth=2)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Binary Cross-Entropy Loss')\n",
    "plt.title('Training Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_curves.png', dpi=150)  # Optional: save for report\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5e8a01d5-05ca-4ff1-ac85-d69a9c9ff041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming you have:\n",
    "# lr_metrics = {'Accuracy': ..., 'Precision': ..., 'Recall': ..., 'F1': ...}\n",
    "# mlp_metrics = {...}\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "lr_values = [lr_metrics[m] for m in metrics]\n",
    "mlp_values = [mlp_metrics[m] for m in metrics]\n",
    "\n",
    "x = np.arange(len(metrics))  # label locations\n",
    "width = 0.35  # bar width\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars1 = plt.bar(x - width/2, lr_values, width, label='Logistic Regression', color='#1f77b4')\n",
    "bars2 = plt.bar(x + width/2, mlp_values, width, label='MLP', color='#ff7f0e')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1 + bars2:\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{bar.get_height():.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab34e99-6c25-4324-9107-43bb9d728ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
